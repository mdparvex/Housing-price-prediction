{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "smooth-retreat",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "operational-april",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>MSSubClass</th>\n",
       "      <th>MSZoning</th>\n",
       "      <th>LotFrontage</th>\n",
       "      <th>LotArea</th>\n",
       "      <th>Street</th>\n",
       "      <th>Alley</th>\n",
       "      <th>LotShape</th>\n",
       "      <th>LandContour</th>\n",
       "      <th>Utilities</th>\n",
       "      <th>...</th>\n",
       "      <th>PoolArea</th>\n",
       "      <th>PoolQC</th>\n",
       "      <th>Fence</th>\n",
       "      <th>MiscFeature</th>\n",
       "      <th>MiscVal</th>\n",
       "      <th>MoSold</th>\n",
       "      <th>YrSold</th>\n",
       "      <th>SaleType</th>\n",
       "      <th>SaleCondition</th>\n",
       "      <th>SalePrice</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>60</td>\n",
       "      <td>RL</td>\n",
       "      <td>65.0</td>\n",
       "      <td>8450</td>\n",
       "      <td>Pave</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Reg</td>\n",
       "      <td>Lvl</td>\n",
       "      <td>AllPub</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2008</td>\n",
       "      <td>WD</td>\n",
       "      <td>Normal</td>\n",
       "      <td>208500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>20</td>\n",
       "      <td>RL</td>\n",
       "      <td>80.0</td>\n",
       "      <td>9600</td>\n",
       "      <td>Pave</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Reg</td>\n",
       "      <td>Lvl</td>\n",
       "      <td>AllPub</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>2007</td>\n",
       "      <td>WD</td>\n",
       "      <td>Normal</td>\n",
       "      <td>181500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>60</td>\n",
       "      <td>RL</td>\n",
       "      <td>68.0</td>\n",
       "      <td>11250</td>\n",
       "      <td>Pave</td>\n",
       "      <td>NaN</td>\n",
       "      <td>IR1</td>\n",
       "      <td>Lvl</td>\n",
       "      <td>AllPub</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "      <td>2008</td>\n",
       "      <td>WD</td>\n",
       "      <td>Normal</td>\n",
       "      <td>223500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>70</td>\n",
       "      <td>RL</td>\n",
       "      <td>60.0</td>\n",
       "      <td>9550</td>\n",
       "      <td>Pave</td>\n",
       "      <td>NaN</td>\n",
       "      <td>IR1</td>\n",
       "      <td>Lvl</td>\n",
       "      <td>AllPub</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2006</td>\n",
       "      <td>WD</td>\n",
       "      <td>Abnorml</td>\n",
       "      <td>140000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>60</td>\n",
       "      <td>RL</td>\n",
       "      <td>84.0</td>\n",
       "      <td>14260</td>\n",
       "      <td>Pave</td>\n",
       "      <td>NaN</td>\n",
       "      <td>IR1</td>\n",
       "      <td>Lvl</td>\n",
       "      <td>AllPub</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>12</td>\n",
       "      <td>2008</td>\n",
       "      <td>WD</td>\n",
       "      <td>Normal</td>\n",
       "      <td>250000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 81 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Id  MSSubClass MSZoning  LotFrontage  LotArea Street Alley LotShape  \\\n",
       "0   1          60       RL         65.0     8450   Pave   NaN      Reg   \n",
       "1   2          20       RL         80.0     9600   Pave   NaN      Reg   \n",
       "2   3          60       RL         68.0    11250   Pave   NaN      IR1   \n",
       "3   4          70       RL         60.0     9550   Pave   NaN      IR1   \n",
       "4   5          60       RL         84.0    14260   Pave   NaN      IR1   \n",
       "\n",
       "  LandContour Utilities  ... PoolArea PoolQC Fence MiscFeature MiscVal MoSold  \\\n",
       "0         Lvl    AllPub  ...        0    NaN   NaN         NaN       0      2   \n",
       "1         Lvl    AllPub  ...        0    NaN   NaN         NaN       0      5   \n",
       "2         Lvl    AllPub  ...        0    NaN   NaN         NaN       0      9   \n",
       "3         Lvl    AllPub  ...        0    NaN   NaN         NaN       0      2   \n",
       "4         Lvl    AllPub  ...        0    NaN   NaN         NaN       0     12   \n",
       "\n",
       "  YrSold  SaleType  SaleCondition  SalePrice  \n",
       "0   2008        WD         Normal     208500  \n",
       "1   2007        WD         Normal     181500  \n",
       "2   2008        WD         Normal     223500  \n",
       "3   2006        WD        Abnorml     140000  \n",
       "4   2008        WD         Normal     250000  \n",
       "\n",
       "[5 rows x 81 columns]"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data=pd.read_csv('C:/Users/Laptop Zone/jupyterNoteBook/HousingPricePrediction/data/train.csv')\n",
    "train_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "aware-building",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>MSSubClass</th>\n",
       "      <th>LotFrontage</th>\n",
       "      <th>LotArea</th>\n",
       "      <th>OverallQual</th>\n",
       "      <th>OverallCond</th>\n",
       "      <th>YearBuilt</th>\n",
       "      <th>YearRemodAdd</th>\n",
       "      <th>MasVnrArea</th>\n",
       "      <th>BsmtFinSF1</th>\n",
       "      <th>...</th>\n",
       "      <th>WoodDeckSF</th>\n",
       "      <th>OpenPorchSF</th>\n",
       "      <th>EnclosedPorch</th>\n",
       "      <th>3SsnPorch</th>\n",
       "      <th>ScreenPorch</th>\n",
       "      <th>PoolArea</th>\n",
       "      <th>MiscVal</th>\n",
       "      <th>MoSold</th>\n",
       "      <th>YrSold</th>\n",
       "      <th>SalePrice</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>1460.000000</td>\n",
       "      <td>1460.000000</td>\n",
       "      <td>1201.000000</td>\n",
       "      <td>1460.000000</td>\n",
       "      <td>1460.000000</td>\n",
       "      <td>1460.000000</td>\n",
       "      <td>1460.000000</td>\n",
       "      <td>1460.000000</td>\n",
       "      <td>1452.000000</td>\n",
       "      <td>1460.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>1460.000000</td>\n",
       "      <td>1460.000000</td>\n",
       "      <td>1460.000000</td>\n",
       "      <td>1460.000000</td>\n",
       "      <td>1460.000000</td>\n",
       "      <td>1460.000000</td>\n",
       "      <td>1460.000000</td>\n",
       "      <td>1460.000000</td>\n",
       "      <td>1460.000000</td>\n",
       "      <td>1460.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>730.500000</td>\n",
       "      <td>56.897260</td>\n",
       "      <td>70.049958</td>\n",
       "      <td>10516.828082</td>\n",
       "      <td>6.099315</td>\n",
       "      <td>5.575342</td>\n",
       "      <td>1971.267808</td>\n",
       "      <td>1984.865753</td>\n",
       "      <td>103.685262</td>\n",
       "      <td>443.639726</td>\n",
       "      <td>...</td>\n",
       "      <td>94.244521</td>\n",
       "      <td>46.660274</td>\n",
       "      <td>21.954110</td>\n",
       "      <td>3.409589</td>\n",
       "      <td>15.060959</td>\n",
       "      <td>2.758904</td>\n",
       "      <td>43.489041</td>\n",
       "      <td>6.321918</td>\n",
       "      <td>2007.815753</td>\n",
       "      <td>180921.195890</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>421.610009</td>\n",
       "      <td>42.300571</td>\n",
       "      <td>24.284752</td>\n",
       "      <td>9981.264932</td>\n",
       "      <td>1.382997</td>\n",
       "      <td>1.112799</td>\n",
       "      <td>30.202904</td>\n",
       "      <td>20.645407</td>\n",
       "      <td>181.066207</td>\n",
       "      <td>456.098091</td>\n",
       "      <td>...</td>\n",
       "      <td>125.338794</td>\n",
       "      <td>66.256028</td>\n",
       "      <td>61.119149</td>\n",
       "      <td>29.317331</td>\n",
       "      <td>55.757415</td>\n",
       "      <td>40.177307</td>\n",
       "      <td>496.123024</td>\n",
       "      <td>2.703626</td>\n",
       "      <td>1.328095</td>\n",
       "      <td>79442.502883</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>20.000000</td>\n",
       "      <td>21.000000</td>\n",
       "      <td>1300.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1872.000000</td>\n",
       "      <td>1950.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>2006.000000</td>\n",
       "      <td>34900.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>365.750000</td>\n",
       "      <td>20.000000</td>\n",
       "      <td>59.000000</td>\n",
       "      <td>7553.500000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>1954.000000</td>\n",
       "      <td>1967.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>2007.000000</td>\n",
       "      <td>129975.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>730.500000</td>\n",
       "      <td>50.000000</td>\n",
       "      <td>69.000000</td>\n",
       "      <td>9478.500000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>1973.000000</td>\n",
       "      <td>1994.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>383.500000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>25.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>2008.000000</td>\n",
       "      <td>163000.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>1095.250000</td>\n",
       "      <td>70.000000</td>\n",
       "      <td>80.000000</td>\n",
       "      <td>11601.500000</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>2000.000000</td>\n",
       "      <td>2004.000000</td>\n",
       "      <td>166.000000</td>\n",
       "      <td>712.250000</td>\n",
       "      <td>...</td>\n",
       "      <td>168.000000</td>\n",
       "      <td>68.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>2009.000000</td>\n",
       "      <td>214000.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1460.000000</td>\n",
       "      <td>190.000000</td>\n",
       "      <td>313.000000</td>\n",
       "      <td>215245.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>9.000000</td>\n",
       "      <td>2010.000000</td>\n",
       "      <td>2010.000000</td>\n",
       "      <td>1600.000000</td>\n",
       "      <td>5644.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>857.000000</td>\n",
       "      <td>547.000000</td>\n",
       "      <td>552.000000</td>\n",
       "      <td>508.000000</td>\n",
       "      <td>480.000000</td>\n",
       "      <td>738.000000</td>\n",
       "      <td>15500.000000</td>\n",
       "      <td>12.000000</td>\n",
       "      <td>2010.000000</td>\n",
       "      <td>755000.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows × 38 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                Id   MSSubClass  LotFrontage        LotArea  OverallQual  \\\n",
       "count  1460.000000  1460.000000  1201.000000    1460.000000  1460.000000   \n",
       "mean    730.500000    56.897260    70.049958   10516.828082     6.099315   \n",
       "std     421.610009    42.300571    24.284752    9981.264932     1.382997   \n",
       "min       1.000000    20.000000    21.000000    1300.000000     1.000000   \n",
       "25%     365.750000    20.000000    59.000000    7553.500000     5.000000   \n",
       "50%     730.500000    50.000000    69.000000    9478.500000     6.000000   \n",
       "75%    1095.250000    70.000000    80.000000   11601.500000     7.000000   \n",
       "max    1460.000000   190.000000   313.000000  215245.000000    10.000000   \n",
       "\n",
       "       OverallCond    YearBuilt  YearRemodAdd   MasVnrArea   BsmtFinSF1  ...  \\\n",
       "count  1460.000000  1460.000000   1460.000000  1452.000000  1460.000000  ...   \n",
       "mean      5.575342  1971.267808   1984.865753   103.685262   443.639726  ...   \n",
       "std       1.112799    30.202904     20.645407   181.066207   456.098091  ...   \n",
       "min       1.000000  1872.000000   1950.000000     0.000000     0.000000  ...   \n",
       "25%       5.000000  1954.000000   1967.000000     0.000000     0.000000  ...   \n",
       "50%       5.000000  1973.000000   1994.000000     0.000000   383.500000  ...   \n",
       "75%       6.000000  2000.000000   2004.000000   166.000000   712.250000  ...   \n",
       "max       9.000000  2010.000000   2010.000000  1600.000000  5644.000000  ...   \n",
       "\n",
       "        WoodDeckSF  OpenPorchSF  EnclosedPorch    3SsnPorch  ScreenPorch  \\\n",
       "count  1460.000000  1460.000000    1460.000000  1460.000000  1460.000000   \n",
       "mean     94.244521    46.660274      21.954110     3.409589    15.060959   \n",
       "std     125.338794    66.256028      61.119149    29.317331    55.757415   \n",
       "min       0.000000     0.000000       0.000000     0.000000     0.000000   \n",
       "25%       0.000000     0.000000       0.000000     0.000000     0.000000   \n",
       "50%       0.000000    25.000000       0.000000     0.000000     0.000000   \n",
       "75%     168.000000    68.000000       0.000000     0.000000     0.000000   \n",
       "max     857.000000   547.000000     552.000000   508.000000   480.000000   \n",
       "\n",
       "          PoolArea       MiscVal       MoSold       YrSold      SalePrice  \n",
       "count  1460.000000   1460.000000  1460.000000  1460.000000    1460.000000  \n",
       "mean      2.758904     43.489041     6.321918  2007.815753  180921.195890  \n",
       "std      40.177307    496.123024     2.703626     1.328095   79442.502883  \n",
       "min       0.000000      0.000000     1.000000  2006.000000   34900.000000  \n",
       "25%       0.000000      0.000000     5.000000  2007.000000  129975.000000  \n",
       "50%       0.000000      0.000000     6.000000  2008.000000  163000.000000  \n",
       "75%       0.000000      0.000000     8.000000  2009.000000  214000.000000  \n",
       "max     738.000000  15500.000000    12.000000  2010.000000  755000.000000  \n",
       "\n",
       "[8 rows x 38 columns]"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "compound-industry",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count      1460.000000\n",
       "mean     180921.195890\n",
       "std       79442.502883\n",
       "min       34900.000000\n",
       "25%      129975.000000\n",
       "50%      163000.000000\n",
       "75%      214000.000000\n",
       "max      755000.000000\n",
       "Name: SalePrice, dtype: float64"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data['SalePrice'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "subtle-maple",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>MSSubClass</th>\n",
       "      <th>MSZoning</th>\n",
       "      <th>LotFrontage</th>\n",
       "      <th>LotArea</th>\n",
       "      <th>Street</th>\n",
       "      <th>Alley</th>\n",
       "      <th>LotShape</th>\n",
       "      <th>LandContour</th>\n",
       "      <th>Utilities</th>\n",
       "      <th>...</th>\n",
       "      <th>ScreenPorch</th>\n",
       "      <th>PoolArea</th>\n",
       "      <th>PoolQC</th>\n",
       "      <th>Fence</th>\n",
       "      <th>MiscFeature</th>\n",
       "      <th>MiscVal</th>\n",
       "      <th>MoSold</th>\n",
       "      <th>YrSold</th>\n",
       "      <th>SaleType</th>\n",
       "      <th>SaleCondition</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1461</td>\n",
       "      <td>20</td>\n",
       "      <td>RH</td>\n",
       "      <td>80.0</td>\n",
       "      <td>11622</td>\n",
       "      <td>Pave</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Reg</td>\n",
       "      <td>Lvl</td>\n",
       "      <td>AllPub</td>\n",
       "      <td>...</td>\n",
       "      <td>120</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>MnPrv</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>2010</td>\n",
       "      <td>WD</td>\n",
       "      <td>Normal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1462</td>\n",
       "      <td>20</td>\n",
       "      <td>RL</td>\n",
       "      <td>81.0</td>\n",
       "      <td>14267</td>\n",
       "      <td>Pave</td>\n",
       "      <td>NaN</td>\n",
       "      <td>IR1</td>\n",
       "      <td>Lvl</td>\n",
       "      <td>AllPub</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Gar2</td>\n",
       "      <td>12500</td>\n",
       "      <td>6</td>\n",
       "      <td>2010</td>\n",
       "      <td>WD</td>\n",
       "      <td>Normal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1463</td>\n",
       "      <td>60</td>\n",
       "      <td>RL</td>\n",
       "      <td>74.0</td>\n",
       "      <td>13830</td>\n",
       "      <td>Pave</td>\n",
       "      <td>NaN</td>\n",
       "      <td>IR1</td>\n",
       "      <td>Lvl</td>\n",
       "      <td>AllPub</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>MnPrv</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>2010</td>\n",
       "      <td>WD</td>\n",
       "      <td>Normal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1464</td>\n",
       "      <td>60</td>\n",
       "      <td>RL</td>\n",
       "      <td>78.0</td>\n",
       "      <td>9978</td>\n",
       "      <td>Pave</td>\n",
       "      <td>NaN</td>\n",
       "      <td>IR1</td>\n",
       "      <td>Lvl</td>\n",
       "      <td>AllPub</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>2010</td>\n",
       "      <td>WD</td>\n",
       "      <td>Normal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1465</td>\n",
       "      <td>120</td>\n",
       "      <td>RL</td>\n",
       "      <td>43.0</td>\n",
       "      <td>5005</td>\n",
       "      <td>Pave</td>\n",
       "      <td>NaN</td>\n",
       "      <td>IR1</td>\n",
       "      <td>HLS</td>\n",
       "      <td>AllPub</td>\n",
       "      <td>...</td>\n",
       "      <td>144</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2010</td>\n",
       "      <td>WD</td>\n",
       "      <td>Normal</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 80 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Id  MSSubClass MSZoning  LotFrontage  LotArea Street Alley LotShape  \\\n",
       "0  1461          20       RH         80.0    11622   Pave   NaN      Reg   \n",
       "1  1462          20       RL         81.0    14267   Pave   NaN      IR1   \n",
       "2  1463          60       RL         74.0    13830   Pave   NaN      IR1   \n",
       "3  1464          60       RL         78.0     9978   Pave   NaN      IR1   \n",
       "4  1465         120       RL         43.0     5005   Pave   NaN      IR1   \n",
       "\n",
       "  LandContour Utilities  ... ScreenPorch PoolArea PoolQC  Fence MiscFeature  \\\n",
       "0         Lvl    AllPub  ...         120        0    NaN  MnPrv         NaN   \n",
       "1         Lvl    AllPub  ...           0        0    NaN    NaN        Gar2   \n",
       "2         Lvl    AllPub  ...           0        0    NaN  MnPrv         NaN   \n",
       "3         Lvl    AllPub  ...           0        0    NaN    NaN         NaN   \n",
       "4         HLS    AllPub  ...         144        0    NaN    NaN         NaN   \n",
       "\n",
       "  MiscVal MoSold  YrSold  SaleType  SaleCondition  \n",
       "0       0      6    2010        WD         Normal  \n",
       "1   12500      6    2010        WD         Normal  \n",
       "2       0      3    2010        WD         Normal  \n",
       "3       0      6    2010        WD         Normal  \n",
       "4       0      1    2010        WD         Normal  \n",
       "\n",
       "[5 rows x 80 columns]"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data=pd.read_csv('C:/Users/Laptop Zone/jupyterNoteBook/HousingPricePrediction/data/test.csv')\n",
    "test_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "weekly-driver",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>MSSubClass</th>\n",
       "      <th>LotFrontage</th>\n",
       "      <th>LotArea</th>\n",
       "      <th>OverallQual</th>\n",
       "      <th>OverallCond</th>\n",
       "      <th>YearBuilt</th>\n",
       "      <th>YearRemodAdd</th>\n",
       "      <th>MasVnrArea</th>\n",
       "      <th>BsmtFinSF1</th>\n",
       "      <th>...</th>\n",
       "      <th>GarageArea</th>\n",
       "      <th>WoodDeckSF</th>\n",
       "      <th>OpenPorchSF</th>\n",
       "      <th>EnclosedPorch</th>\n",
       "      <th>3SsnPorch</th>\n",
       "      <th>ScreenPorch</th>\n",
       "      <th>PoolArea</th>\n",
       "      <th>MiscVal</th>\n",
       "      <th>MoSold</th>\n",
       "      <th>YrSold</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>1459.000000</td>\n",
       "      <td>1459.000000</td>\n",
       "      <td>1232.000000</td>\n",
       "      <td>1459.000000</td>\n",
       "      <td>1459.000000</td>\n",
       "      <td>1459.000000</td>\n",
       "      <td>1459.000000</td>\n",
       "      <td>1459.000000</td>\n",
       "      <td>1444.000000</td>\n",
       "      <td>1458.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>1458.000000</td>\n",
       "      <td>1459.000000</td>\n",
       "      <td>1459.000000</td>\n",
       "      <td>1459.000000</td>\n",
       "      <td>1459.000000</td>\n",
       "      <td>1459.000000</td>\n",
       "      <td>1459.000000</td>\n",
       "      <td>1459.000000</td>\n",
       "      <td>1459.000000</td>\n",
       "      <td>1459.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>2190.000000</td>\n",
       "      <td>57.378341</td>\n",
       "      <td>68.580357</td>\n",
       "      <td>9819.161069</td>\n",
       "      <td>6.078821</td>\n",
       "      <td>5.553804</td>\n",
       "      <td>1971.357779</td>\n",
       "      <td>1983.662783</td>\n",
       "      <td>100.709141</td>\n",
       "      <td>439.203704</td>\n",
       "      <td>...</td>\n",
       "      <td>472.768861</td>\n",
       "      <td>93.174777</td>\n",
       "      <td>48.313914</td>\n",
       "      <td>24.243317</td>\n",
       "      <td>1.794380</td>\n",
       "      <td>17.064428</td>\n",
       "      <td>1.744345</td>\n",
       "      <td>58.167923</td>\n",
       "      <td>6.104181</td>\n",
       "      <td>2007.769705</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>421.321334</td>\n",
       "      <td>42.746880</td>\n",
       "      <td>22.376841</td>\n",
       "      <td>4955.517327</td>\n",
       "      <td>1.436812</td>\n",
       "      <td>1.113740</td>\n",
       "      <td>30.390071</td>\n",
       "      <td>21.130467</td>\n",
       "      <td>177.625900</td>\n",
       "      <td>455.268042</td>\n",
       "      <td>...</td>\n",
       "      <td>217.048611</td>\n",
       "      <td>127.744882</td>\n",
       "      <td>68.883364</td>\n",
       "      <td>67.227765</td>\n",
       "      <td>20.207842</td>\n",
       "      <td>56.609763</td>\n",
       "      <td>30.491646</td>\n",
       "      <td>630.806978</td>\n",
       "      <td>2.722432</td>\n",
       "      <td>1.301740</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>1461.000000</td>\n",
       "      <td>20.000000</td>\n",
       "      <td>21.000000</td>\n",
       "      <td>1470.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1879.000000</td>\n",
       "      <td>1950.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>2006.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>1825.500000</td>\n",
       "      <td>20.000000</td>\n",
       "      <td>58.000000</td>\n",
       "      <td>7391.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>1953.000000</td>\n",
       "      <td>1963.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>318.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>2007.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>2190.000000</td>\n",
       "      <td>50.000000</td>\n",
       "      <td>67.000000</td>\n",
       "      <td>9399.000000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>1973.000000</td>\n",
       "      <td>1992.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>350.500000</td>\n",
       "      <td>...</td>\n",
       "      <td>480.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>28.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>2008.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>2554.500000</td>\n",
       "      <td>70.000000</td>\n",
       "      <td>80.000000</td>\n",
       "      <td>11517.500000</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>2001.000000</td>\n",
       "      <td>2004.000000</td>\n",
       "      <td>164.000000</td>\n",
       "      <td>753.500000</td>\n",
       "      <td>...</td>\n",
       "      <td>576.000000</td>\n",
       "      <td>168.000000</td>\n",
       "      <td>72.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>2009.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>2919.000000</td>\n",
       "      <td>190.000000</td>\n",
       "      <td>200.000000</td>\n",
       "      <td>56600.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>9.000000</td>\n",
       "      <td>2010.000000</td>\n",
       "      <td>2010.000000</td>\n",
       "      <td>1290.000000</td>\n",
       "      <td>4010.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>1488.000000</td>\n",
       "      <td>1424.000000</td>\n",
       "      <td>742.000000</td>\n",
       "      <td>1012.000000</td>\n",
       "      <td>360.000000</td>\n",
       "      <td>576.000000</td>\n",
       "      <td>800.000000</td>\n",
       "      <td>17000.000000</td>\n",
       "      <td>12.000000</td>\n",
       "      <td>2010.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows × 37 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                Id   MSSubClass  LotFrontage       LotArea  OverallQual  \\\n",
       "count  1459.000000  1459.000000  1232.000000   1459.000000  1459.000000   \n",
       "mean   2190.000000    57.378341    68.580357   9819.161069     6.078821   \n",
       "std     421.321334    42.746880    22.376841   4955.517327     1.436812   \n",
       "min    1461.000000    20.000000    21.000000   1470.000000     1.000000   \n",
       "25%    1825.500000    20.000000    58.000000   7391.000000     5.000000   \n",
       "50%    2190.000000    50.000000    67.000000   9399.000000     6.000000   \n",
       "75%    2554.500000    70.000000    80.000000  11517.500000     7.000000   \n",
       "max    2919.000000   190.000000   200.000000  56600.000000    10.000000   \n",
       "\n",
       "       OverallCond    YearBuilt  YearRemodAdd   MasVnrArea   BsmtFinSF1  ...  \\\n",
       "count  1459.000000  1459.000000   1459.000000  1444.000000  1458.000000  ...   \n",
       "mean      5.553804  1971.357779   1983.662783   100.709141   439.203704  ...   \n",
       "std       1.113740    30.390071     21.130467   177.625900   455.268042  ...   \n",
       "min       1.000000  1879.000000   1950.000000     0.000000     0.000000  ...   \n",
       "25%       5.000000  1953.000000   1963.000000     0.000000     0.000000  ...   \n",
       "50%       5.000000  1973.000000   1992.000000     0.000000   350.500000  ...   \n",
       "75%       6.000000  2001.000000   2004.000000   164.000000   753.500000  ...   \n",
       "max       9.000000  2010.000000   2010.000000  1290.000000  4010.000000  ...   \n",
       "\n",
       "        GarageArea   WoodDeckSF  OpenPorchSF  EnclosedPorch    3SsnPorch  \\\n",
       "count  1458.000000  1459.000000  1459.000000    1459.000000  1459.000000   \n",
       "mean    472.768861    93.174777    48.313914      24.243317     1.794380   \n",
       "std     217.048611   127.744882    68.883364      67.227765    20.207842   \n",
       "min       0.000000     0.000000     0.000000       0.000000     0.000000   \n",
       "25%     318.000000     0.000000     0.000000       0.000000     0.000000   \n",
       "50%     480.000000     0.000000    28.000000       0.000000     0.000000   \n",
       "75%     576.000000   168.000000    72.000000       0.000000     0.000000   \n",
       "max    1488.000000  1424.000000   742.000000    1012.000000   360.000000   \n",
       "\n",
       "       ScreenPorch     PoolArea       MiscVal       MoSold       YrSold  \n",
       "count  1459.000000  1459.000000   1459.000000  1459.000000  1459.000000  \n",
       "mean     17.064428     1.744345     58.167923     6.104181  2007.769705  \n",
       "std      56.609763    30.491646    630.806978     2.722432     1.301740  \n",
       "min       0.000000     0.000000      0.000000     1.000000  2006.000000  \n",
       "25%       0.000000     0.000000      0.000000     4.000000  2007.000000  \n",
       "50%       0.000000     0.000000      0.000000     6.000000  2008.000000  \n",
       "75%       0.000000     0.000000      0.000000     8.000000  2009.000000  \n",
       "max     576.000000   800.000000  17000.000000    12.000000  2010.000000  \n",
       "\n",
       "[8 rows x 37 columns]"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "geological-locking",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Id                 0\n",
       "MSSubClass         0\n",
       "MSZoning           0\n",
       "LotFrontage      259\n",
       "LotArea            0\n",
       "                ... \n",
       "MoSold             0\n",
       "YrSold             0\n",
       "SaleType           0\n",
       "SaleCondition      0\n",
       "SalePrice          0\n",
       "Length: 81, dtype: int64"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "opening-virgin",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Id                 0\n",
       "MSSubClass         0\n",
       "MSZoning           4\n",
       "LotFrontage      227\n",
       "LotArea            0\n",
       "                ... \n",
       "MiscVal            0\n",
       "MoSold             0\n",
       "YrSold             0\n",
       "SaleType           1\n",
       "SaleCondition      0\n",
       "Length: 80, dtype: int64"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "controlled-lithuania",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "adopted-principal",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXcAAAD4CAYAAAAXUaZHAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjMuNCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8QVMy6AAAACXBIWXMAAAsTAAALEwEAmpwYAAASlklEQVR4nO3da4xc533f8e8vpC3Ht5iMVgJB0iUNEG6poJYUgrWhwnCtJqItw9QbARTQgkhVsEDZwkYLBGQNtM0LAkoKFG7RKg3hOGURxwzjxBFho0kINkbTC0yvbPlCSazWkiItyJAbBa6bBFAr5d8X86gcknsZ7s5oh0+/H2BxznnmOXN+y8tvD8/MGaaqkCT15UfWO4Akafwsd0nqkOUuSR2y3CWpQ5a7JHVo43oHALj99ttrx44d6x1Dkm4pTz755B9X1cxij01Fue/YsYPZ2dn1jiFJt5Qkf7jUY16WkaQOWe6S1CHLXZI6ZLlLUocsd0nqkOUuSR2y3CWpQ5a7JHXIcpekDk3FHaq3qh1Hvroux33xsQfX5biSbh2euUtShyx3SerQiuWe5P1Jnhr6+mGSTyfZnORMkufactPQPkeTzCW5kOSByX4LkqTrrVjuVXWhqu6uqruBnwT+HPgycAQ4W1W7gLNtmyS7gQPAXcA+4PEkGyYTX5K0mJu9LHM/8P2q+kNgP3CijZ8AHmrr+4GTVfVqVb0AzAF7x5BVkjSimy33A8AX2/qdVXUJoC3vaONbgZeH9plvY9dIcijJbJLZhYWFm4whSVrOyOWe5K3AJ4HfWGnqImN1w0DV8araU1V7ZmYW/Y9EJEmrdDNn7h8DvllVl9v25SRbANryShufB7YP7bcNuLjWoJKk0d1MuT/C1UsyAKeBg239IPDE0PiBJLcl2QnsAs6tNagkaXQj3aGa5O3ATwF/b2j4MeBUkkeBl4CHAarqfJJTwNPAa8Dhqnp9rKklScsaqdyr6s+BH79u7BUG755ZbP4x4Nia00mSVsU7VCWpQ5a7JHXIcpekDlnuktQhy12SOmS5S1KHLHdJ6pDlLkkdstwlqUOWuyR1yHKXpA5Z7pLUIctdkjpkuUtShyx3SeqQ5S5JHbLcJalDlrskdchyl6QOWe6S1KGRyj3Je5J8KcmzSZ5J8qEkm5OcSfJcW24amn80yVySC0kemFx8SdJiRj1z/1fA71TVXwY+ADwDHAHOVtUu4GzbJslu4ABwF7APeDzJhnEHlyQtbcVyT/Ju4MPALwNU1f+uqh8A+4ETbdoJ4KG2vh84WVWvVtULwBywd7yxJUnLGeXM/X3AAvArSb6V5HNJ3gHcWVWXANryjjZ/K/Dy0P7zbewaSQ4lmU0yu7CwsKZvQpJ0rVHKfSNwL/CLVXUP8Ge0SzBLyCJjdcNA1fGq2lNVe2ZmZkYKK0kazSjlPg/MV9XX2/aXGJT95SRbANryytD87UP7bwMujieuJGkUK5Z7Vf0R8HKS97eh+4GngdPAwTZ2EHiirZ8GDiS5LclOYBdwbqypJUnL2jjivH8IfCHJW4HngZ9h8IPhVJJHgZeAhwGq6nySUwx+ALwGHK6q18eeXJK0pJHKvaqeAvYs8tD9S8w/BhxbfSxJ0lp4h6okdchyl6QOWe6S1CHLXZI6ZLlLUocsd0nqkOUuSR2y3CWpQ5a7JHXIcpekDlnuktQhy12SOmS5S1KHLHdJ6pDlLkkdstwlqUOWuyR1yHKXpA5Z7pLUoZHKPcmLSb6b5Kkks21sc5IzSZ5ry01D848mmUtyIckDkwovSVrczZy5/42quruq3viPso8AZ6tqF3C2bZNkN3AAuAvYBzyeZMMYM0uSVrCWyzL7gRNt/QTw0ND4yap6tapeAOaAvWs4jiTpJo1a7gX8XpInkxxqY3dW1SWAtryjjW8FXh7ad76NSZLeJBtHnHdfVV1McgdwJsmzy8zNImN1w6TBD4lDAO9973tHjCFJGsVIZ+5VdbEtrwBfZnCZ5XKSLQBteaVNnwe2D+2+Dbi4yHMer6o9VbVnZmZm9d+BJOkGK5Z7knckedcb68BPA98DTgMH27SDwBNt/TRwIMltSXYCu4Bz4w4uSVraKJdl7gS+nOSN+b9WVb+T5BvAqSSPAi8BDwNU1fkkp4CngdeAw1X1+kTSS5IWtWK5V9XzwAcWGX8FuH+JfY4Bx9acTpK0Kt6hKkkdstwlqUOWuyR1yHKXpA5Z7pLUIctdkjpkuUtShyx3SeqQ5S5JHbLcJalDlrskdchyl6QOWe6S1CHLXZI6ZLlLUocsd0nqkOUuSR2y3CWpQ5a7JHXIcpekDo1c7kk2JPlWkq+07c1JziR5ri03Dc09mmQuyYUkD0wiuCRpaTdz5v4p4Jmh7SPA2araBZxt2yTZDRwA7gL2AY8n2TCeuJKkUYxU7km2AQ8Cnxsa3g+caOsngIeGxk9W1atV9QIwB+wdS1pJ0khGPXP/LPCzwF8Mjd1ZVZcA2vKONr4VeHlo3nwbu0aSQ0lmk8wuLCzcbG5J0jJWLPcknwCuVNWTIz5nFhmrGwaqjlfVnqraMzMzM+JTS5JGsXGEOfcBn0zyceBtwLuT/CpwOcmWqrqUZAtwpc2fB7YP7b8NuDjO0JKk5a145l5VR6tqW1XtYPBC6X+qqr8FnAYOtmkHgSfa+mngQJLbkuwEdgHnxp5ckrSkUc7cl/IYcCrJo8BLwMMAVXU+ySngaeA14HBVvb7mpJKkkd1UuVfV14CvtfVXgPuXmHcMOLbGbJKkVfIOVUnqkOUuSR1ayzV3rZMdR766bsd+8bEH1+3Ykkbnmbskdchyl6QOWe6S1CHLXZI6ZLlLUocsd0nqkOUuSR2y3CWpQ5a7JHXIcpekDlnuktQhy12SOmS5S1KHLHdJ6pDlLkkdstwlqUOWuyR1aMVyT/K2JOeSfDvJ+SQ/18Y3JzmT5Lm23DS0z9Ekc0kuJHlgkt+AJOlGo5y5vwp8tKo+ANwN7EvyQeAIcLaqdgFn2zZJdgMHgLuAfcDjSTZMILskaQkrlnsN/GnbfEv7KmA/cKKNnwAeauv7gZNV9WpVvQDMAXvHGVqStLyRrrkn2ZDkKeAKcKaqvg7cWVWXANryjjZ9K/Dy0O7zbez65zyUZDbJ7MLCwhq+BUnS9UYq96p6varuBrYBe5P8xDLTs9hTLPKcx6tqT1XtmZmZGSmsJGk0N/Vumar6AfA1BtfSLyfZAtCWV9q0eWD70G7bgItrDSpJGt0o75aZSfKetv6jwN8EngVOAwfbtIPAE239NHAgyW1JdgK7gHNjzi1JWsbGEeZsAU60d7z8CHCqqr6S5L8Dp5I8CrwEPAxQVeeTnAKeBl4DDlfV65OJL0lazIrlXlXfAe5ZZPwV4P4l9jkGHFtzOknSqniHqiR1yHKXpA5Z7pLUIctdkjpkuUtShyx3SeqQ5S5JHbLcJalDlrskdchyl6QOWe6S1CHLXZI6ZLlLUocsd0nqkOUuSR2y3CWpQ5a7JHXIcpekDlnuktShFcs9yfYkv5/kmSTnk3yqjW9OcibJc225aWifo0nmklxI8sAkvwFJ0o1GOXN/DfjHVfVXgA8Ch5PsBo4AZ6tqF3C2bdMeOwDcBewDHk+yYRLhJUmLW7Hcq+pSVX2zrf8v4BlgK7AfONGmnQAeauv7gZNV9WpVvQDMAXvHnFuStIybuuaeZAdwD/B14M6qugSDHwDAHW3aVuDlod3m25gk6U0ycrkneSfwm8Cnq+qHy01dZKwWeb5DSWaTzC4sLIwaQ5I0gpHKPclbGBT7F6rqt9rw5SRb2uNbgCttfB7YPrT7NuDi9c9ZVcerak9V7ZmZmVltfknSIkZ5t0yAXwaeqap/OfTQaeBgWz8IPDE0fiDJbUl2AruAc+OLLElaycYR5twH/G3gu0meamP/BHgMOJXkUeAl4GGAqjqf5BTwNIN32hyuqtfHHVyStLQVy72q/guLX0cHuH+JfY4Bx9aQS5K0Bt6hKkkdstwlqUOWuyR1yHKXpA5Z7pLUIctdkjpkuUtShyx3SeqQ5S5JHRrl4wem3o4jX13vCJI0VTxzl6QOWe6S1CHLXZI61MU1d7151uv1jRcfe3Bdjivdqjxzl6QOWe6S1CHLXZI6ZLlLUocsd0nqkOUuSR1asdyTfD7JlSTfGxrbnORMkufactPQY0eTzCW5kOSBSQWXJC1tlDP3fw/su27sCHC2qnYBZ9s2SXYDB4C72j6PJ9kwtrSSpJGsWO5V9Z+BP7lueD9woq2fAB4aGj9ZVa9W1QvAHLB3PFElSaNa7TX3O6vqEkBb3tHGtwIvD82bb2M3SHIoyWyS2YWFhVXGkCQtZtwvqGaRsVpsYlUdr6o9VbVnZmZmzDEk6f9vqy33y0m2ALTllTY+D2wfmrcNuLj6eJKk1VhtuZ8GDrb1g8ATQ+MHktyWZCewCzi3toiSpJu14qdCJvki8BHg9iTzwD8DHgNOJXkUeAl4GKCqzic5BTwNvAYcrqrXJ5RdkrSEFcu9qh5Z4qH7l5h/DDi2llCSpLXxDlVJ6pDlLkkdstwlqUOWuyR1yHKXpA5Z7pLUIctdkjpkuUtSh1a8iUmaBjuOfHXdjv3iYw+u27Gl1fLMXZI6ZLlLUocsd0nqkOUuSR2y3CWpQ5a7JHXIcpekDlnuktQhb2KSVrBeN1B585TWwjN3SeqQ5S5JHZpYuSfZl+RCkrkkRyZ1HEnSjSZS7kk2AP8W+BiwG3gkye5JHEuSdKNJvaC6F5irqucBkpwE9gNPT+h4UnfW85Mw9eaZ1Avnkyr3rcDLQ9vzwF8bnpDkEHCobf5pkleAP55QnnG6HXOO262S1ZzjdavkhAlmzc+vafe/tNQDkyr3LDJW12xUHQeO/78dktmq2jOhPGNjzvG7VbKac7xulZxwa2V9w6ReUJ0Htg9tbwMuTuhYkqTrTKrcvwHsSrIzyVuBA8DpCR1LknSdiVyWqarXkvwD4HeBDcDnq+r8CrsdX+HxaWHO8btVsppzvG6VnHBrZQUgVbXyLEnSLcU7VCWpQ5a7JPWoqtb1C9gHXADmgCMTPM7ngSvA94bGNgNngOfactPQY0dbpgvAA0PjPwl8tz32r7l6aes24Nfb+NeBHUP7HGzHeA44uELO7cDvA88A54FPTWNW4G3AOeDbLefPTWPOofkbgG8BX5nynC+2YzwFzE5rVuA9wJeAZxn8Wf3QtOUE3t9+Hd/4+iHw6WnLOamvN/VgS/yF+z7wPuCtDIpi94SO9WHgXq4t91+g/UABjgA/39Z3tyy3ATtbxg3tsXPtD3KA/wh8rI3/feDftfUDwK8P/cV8vi03tfVNy+TcAtzb1t8F/I+WZ6qytud8Z1t/S/uD/cFpyzmU9x8Bv8bVcp/WnC8Ct183NnVZgRPA323rb2VQ9lOX87qu+SMGN/1Mbc6xdt6bebBFfsE/BPzu0PZR4OgEj7eDa8v9ArClrW8BLiyWg8G7fj7U5jw7NP4I8EvDc9r6RgZ3s2V4Tnvsl4BHbiLzE8BPTXNW4O3ANxnchTx1ORncZ3EW+ChXy33qcrY5L3JjuU9VVuDdwAu0s9dpzXldtp8G/uu05xzn13pfc1/sYwq2vonHv7OqLgG05R0r5Nra1q8fv2afqnoN+J/Ajy/zXCtKsgO4h8FZ8dRlTbIhyVMMLnedqaqpzAl8FvhZ4C+GxqYxJwzu5P69JE+2j+iYxqzvAxaAX0nyrSSfS/KOKcw57ADwxbY+zTnHZr3LfcWPKVgnS+VaLu9q9lk6QPJO4DeBT1fVD5ebuorjjiVrVb1eVXczODPem+Qnpi1nkk8AV6rqyWWyXbPLKo45zt/7+6rqXgafqHo4yYeXmbteWTcyuMT5i1V1D/BnDC5vTFvOwRMNbqT8JPAby81b5THH+vd+nNa73Nf7YwouJ9kC0JZXVsg139avH79mnyQbgR8D/mSZ51pSkrcwKPYvVNVvTXNWgKr6AfA1Bi+OT1vO+4BPJnkROAl8NMmvTmFOAKrqYlteAb7M4BNWpy3rPDDf/qUGgxdW753CnG/4GPDNqrrctqc153i9mdeAFrkOtpHBCw07ufqC6l0TPN4Orr3m/i+49oWVX2jrd3HtCyvPc/WFlW8weOHwjRdWPt7GD3PtCyun2vpmBtcnN7WvF4DNy2QM8B+Az143PlVZgRngPW39R4E/AD4xbTmvy/wRrl5zn7qcwDuAdw2t/zcGPzCnMesfAO9v6/+8ZZy6nG2fk8DPTOvfpYn13Zt5sCV+4T/O4B0h3wc+M8HjfBG4BPwfBj9VH2Vwbewsg7cqnR3+xQc+0zJdoL0y3sb3AN9rj/0brr4l6m0M/tk3x+CV9fcN7fN32vjc8B+yJXL+dQb/fPsOV9/C9fFpywr8VQZvLfxOO8Y/beNTlfO6zB/harlPXU4G17K/zdW3l35mirPeDcy23//fZlBg05jz7cArwI8NjU1dzkl8+fEDktSh9b7mLkmaAMtdkjpkuUtShyx3SeqQ5S5JHbLcJalDlrskdej/At6gLeo6KRV0AAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.hist(train_data['SalePrice'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "ordered-memorial",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skew of SalePrice: 1.8828757597682129\n"
     ]
    }
   ],
   "source": [
    "print (\"Skew of SalePrice:\", train_data['SalePrice'].skew())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "informed-sharing",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_data['LT_SalePrice'] = np.log(train_data['SalePrice']+1)\n",
    "plt.hist(train_data['LT_SalePrice'], color='red')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "renewable-driving",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skew of Log Transformed SalePrice: 0.12134661989685333\n"
     ]
    }
   ],
   "source": [
    "print (\"Skew of Log Transformed SalePrice:\", train_data['LT_SalePrice'].skew())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "modern-newman",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Id', 'MSSubClass', 'LotFrontage', 'LotArea', 'OverallQual',\n",
       "       'OverallCond', 'YearBuilt', 'YearRemodAdd', 'MasVnrArea',\n",
       "       'BsmtFinSF1', 'BsmtFinSF2', 'BsmtUnfSF', 'TotalBsmtSF', '1stFlrSF',\n",
       "       '2ndFlrSF', 'LowQualFinSF', 'GrLivArea', 'BsmtFullBath',\n",
       "       'BsmtHalfBath', 'FullBath', 'HalfBath', 'BedroomAbvGr',\n",
       "       'KitchenAbvGr', 'TotRmsAbvGrd', 'Fireplaces', 'GarageYrBlt',\n",
       "       'GarageCars', 'GarageArea', 'WoodDeckSF', 'OpenPorchSF',\n",
       "       'EnclosedPorch', '3SsnPorch', 'ScreenPorch', 'PoolArea', 'MiscVal',\n",
       "       'MoSold', 'YrSold', 'SalePrice', 'LT_SalePrice'], dtype=object)"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.select_dtypes(include=[np.number]).columns.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "ongoing-class",
   "metadata": {},
   "outputs": [],
   "source": [
    "zero_feat=[  'MSSubClass', 'LotFrontage', 'LotArea', 'OverallQual',\n",
    "       'OverallCond', 'YearBuilt', 'YearRemodAdd', 'MasVnrArea',\n",
    "       'BsmtFinSF1', 'BsmtFinSF2', 'BsmtUnfSF', 'TotalBsmtSF', '1stFlrSF',\n",
    "       '2ndFlrSF', 'LowQualFinSF', 'GrLivArea', 'BsmtFullBath',\n",
    "       'BsmtHalfBath', 'FullBath', 'HalfBath', 'BedroomAbvGr',\n",
    "       'KitchenAbvGr', 'TotRmsAbvGrd', 'Fireplaces', 'GarageYrBlt',\n",
    "       'GarageCars', 'GarageArea', 'WoodDeckSF', 'OpenPorchSF',\n",
    "       'EnclosedPorch', '3SsnPorch', 'ScreenPorch', 'PoolArea', 'MiscVal',\n",
    "       'MoSold', 'YrSold','LT_SalePrice']\n",
    "out_feat=['SalePrice']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "exclusive-delight",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x720 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "def correlation_heatmap(train_data):\n",
    "    correlations = train_data.corr()\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(10,10))\n",
    "    sns.heatmap(correlations, vmax=1.0, center=0, fmt='.2f',\n",
    "                square=True, linewidths=.5,  cbar_kws={\"shrink\": .70})\n",
    "    plt.show();\n",
    "    \n",
    "correlation_heatmap(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "least-virgin",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAnQAAAIfCAYAAAD5WkFWAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjMuNCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8QVMy6AAAACXBIWXMAAAsTAAALEwEAmpwYAADZDUlEQVR4nOzdd5xcVf3/8dc7QQUEQUX5giCbHtJJQughQECalB8gTSS2iF+wi2L50pSiKH4FFAiKAaRJjyAlCQkhQEjPppdtSvmCFBGkKMnn98c5Q26G2d1zd2cnO+znyWMembnzufeemZ3ZPZx773nLzHDOOeecc9Wr28ZugHPOOeecax/v0DnnnHPOVTnv0DnnnHPOVTnv0DnnnHPOVTnv0DnnnHPOVTnv0DnnnHPOVblNNnYDuhBj6R1plQOPAaDHTjsllTc0NbWpvv9ONUn1K5oaAejZo19SPUB9w8rk9hTa1KNH3/T6hlX07LFzjvYsp2anHsn1jU0N1NT0Tq9vXEPPHK+3vqmJmhz1jU1N1NT0zNGeegBqEn/GjfFnnNqmxvgZSn1PG5sa2lif9pobm8Lrzf2dSfzMNTSsCvU1fZLqARoaV+f/GSf+vEJ9Y4d/hvK+3o6uB5K/Z/XvfEbzfaZ7JdbXtfE7kHf7/RI/Eyvb+B3es2f67/XH61dy319XJdcf9sm+Si527eYjdM4555xzVc47dDlIeq2Z5RMlHVvp9jjnnHPOgXfonHPOOeeqnp9D1waSBFwO7A80AH6egHPOOec2Gh+ha5ujgX7AYODLwJ4btznOOeec68q8Q9c2o4GbzWytmT0DPFyqSNJ4SXMlzZ0wYUJlW+icc865LsMPubadtVpgNgEo9OTSpy1xzjnnnMvBR+jaZgZwgqTukrYD9tvYDXLOOedc1+UjdG1zF+GCiMXAKuCRjdsc55xzznVl3qHLwcy2iP8acMZGbo5zzjnnHAAKfRNXAf5GO+ec60p8Sq8K8hG6CsqbM5k3+/XgU6YmlT9wwwGh/uDL0+of+BoAvXPkRq5pasqd/do3R47lqqbG5IxDCDmHeXMyOzpbtk+fXZLrV69ewC5DD0uuX7DoPgB2H3VKUv2s2TcA8K1B+yTV/2rJo0D+z/TAxNzLpTH3sqNzO/fu2T+pfmb9CgDG9h6YVA8wZc3S3HnGJ+w8Mrn+luVzc39n8n6mh9X0Sq5f2FiXO5u1T++hyfWr1ywCOl9+cN761N+L9Q0r4/ZrErffGOoTM6gbG9cA6d9hCJ/RvPnBrnL8ogjnnHPOuSrX4R06SSbphszjTST9XdK98fG2ku6VtEjSMkl/icu7SbpM0hJJiyXNkdTi/2q1lKkqaZSkGZJWSloh6XeSNpc0TtIV5XzNzjnnnHOVVIlDrv8CBknazMzeAA4Ens48fz4w2cx+DSBpSFx+PLA9MMTM1knaIW4rN0nbArcBJ5jZEzG66xhgyza9Iuecc865TqRSh1zvBwonAJ0I3Jx5bjvgqcIDM6vNLH/WzNbF5U+Z2csAkl4r1Es6VtLEzPbGSnpU0ipJh8dlpwPXmdkTcVtmZreb2XPZRkr6tKQnJS2QNCV2BJG0r6SF8bZA0paStosjfgvjKGLayUfOOeecc2VWqQ7dLYSJeDcFhgBPZp77DfB7SdMk/UjS9nH5n4BPxw7TLyWlnkFeA+xL6EBeFfc5CJiXsO5MYHcz2yW2+Xtx+XeB081sGLAP8AZwEvBgXDYUWJjYPuecc865sqpIhy6OutUQRuf+UvTcg0BP4BqgP7BA0sfM7CmgH/ADYB0wVdIBCbv7k5mtM7PVQH3cZqodgAclLQbOBAqXtD0GXCrp68DWZvY2MAf4vKRzgcFm9mrxxjzL1TnnnHOVUMmrXCcBv2DDw60AmNlLZnaTmZ1C6CiNjsvfMrP7zexM4ELgqMIqmdU3Ld5cicdLgREJbbwcuMLMBgNfKWzbzC4GvgRsBsyS1N/MZsR2Pg3cIOlzJV7XBDMbaWYjx48fn7B755xzzrn8KtmhuxY438wWZxdK2l/S5vH+lkAv4K+ShhcOv0rqRjhUW5jU5jlJO8flRxft57h4hWwvwsjfSuAK4FRJu2X2+1lJ/1W07lasv2Dj1ExtLzNbbGY/A+YC/SXtBDxvZtcAvweGt+VNcc4555xrr4pNLBwPof66xFMjgCskvU3oYP7OzOZIOhi4RtIHYt1sQscM4CzgXuBvwBJgi8z2VhKyVbcFTjOzN4E3JZ0A/ELSxwmHcGcAdxa15VzgNklPA7OAwjQp35S0H7AWWEa4yOME4ExJ/wFeA941Quecc845Vwkd3qEr5J8WLZsOTI/3LwEuKVHzAPBAM9u8Hbi9xPJxLbTjCcIFDcUmxhtmdg9wT4l1v1ZivevizTnnnHNuo/Is18rxN9o551xX4lmuFeTRX84555xzVa5i59C5/EHmB58yNan+gRvibC5L70hryMBjwn5OSAsC73HLXAA+1WdQ2vaBB1cvoVev9Pq6uiWM6pEe7D27YXXu4PCTd941uf7G5XM4rG96cPh9qxbRL0dQ+sqmxtzt/87g0cn1v1w8A4DbdxmbVH/sgikA1NQkBo031rep/vwh+yfVn137MEDyZ2J2w+rYnnzB5GN67ZxUP71uOQC9cgST1zU15f4M9cyx/fqmJs4Zsl9y/Xm10zhlwKjk+huWzWZI4s8XoLaxnh8MHpNcf9Hi6VwyNO3zCXDmovAZHZ34M5sRf2Zjew9spTKYsmYpQPLPoD7+ns77HRjZI+0zOrchfEZ379E3qX5WwyoABu7UYkLmO5Y2NQDpf5cg/G2qSdw+QGPch6uMiozQZZMdEmrHZSYXRtL0mL9aSGoomdWasz1HSRrQ3u0455xzznUGnXGEbhzhytVnMstONrO5pYoldTeztTn3cRThKtllbWmgc84551xnstHOoZM0TNIsSbWS7pL04Tj6NhK4MY7GbdbMuo2SzpY0kzDv3ImSFsdM1Z9l6l6TdIGkRXFf20raEzgCuCTuo5ekL0uaE+vuyMyL1yuuN0fS+UUZsmfG5bWSzuvQN8s555xzrgUb86KI64Hvm9kQYDFwTpyOZC5hRG6Ymb0RawsdvIWSPhqXvWlmexPmk/sZsD8wDNhV0lGx5oPALDMbGuu+bGaPE1Irzoz7qAPuNLNdY91y4Itx/V8DvzazXcmMGEo6COgDjIr7HCEp/QQn55xzzlUtSddKel7Skmael6TLJK2JAz/DM88dHE8lWyPprHK1aaN06CRtRchEfSQuuo4Y99WMQgdvmJm9GJfdGv/dFZhuZn+PGas3Zrb1b8KhVYB5hDzZUgZJejRmuJ7M+gzXPYDb4v2bMvUHxdsCYD4hL/ZdZ297lqtzzjn3njQROLiF5w8h9Av6AOOBKyGcJgb8Jj4/ADixXOf0d8Zz6FL9K/7b0jw3/7H1E+2tpfnXOxE4yswWSRoHjGll3wIuMrOrWyoyswlAoSdnF11wQSubdc4551xnZ2YzJNW0UHIkcH3sg8yStLWk7QgDS2vMrB5A0i2xtt3n9G+UETozewV4WVIhueEUQlwXwKvAljk29ySwr6RtYs/3xMy2mlO8jy2BZyW9jzBCVzALOCbePyGz/EHgC5K2AJD0iRgp5pxzzjn3CUI8acFTcVlzy9utUiN0m0t6KvP4UuBU4Kp4AUI98Pn43MS4/A3CIc8Wmdmzkn4ATCOMnP0lRni15BZCTuzXgWOB/yF0DJsI5/MVOnvfBP4o6TvAfcArcZ8PSdoZeEIShCzXzwLPt9Ze55xzznWApXeULZFJg479CuFQacGEeNQteRMlllkLy9utIh06M2tuJHD3ErV3ANkZcseUqKkpenwTG57jVli+Reb+O/mvZvYY4dh1wZXxVuxpYHczM0knEC7YKGzv14SLJpxzzjn3HlJ0ylRbPAXsmHm8A+Hiyvc3s7zdPMu1BfGQ8BWEHvU/gC+Y2Zo2bs7faOecc11JZbNcyzhCx8BjWm17PIfuXjN7VyySpMOAM4BDgd2Ay8xslKRNgFXAAYRBoznASWa2tL1NruaLIjqcmT0KpGf3OOecc26jsLV5Mwaa11pvTtLNhCOI28RTys4B3gdgZlcBfyF05tYArxNPKzOztyWdQTgXvztwbTk6c+AjdJVk/ROzPlc0NQJw8MGXJ9U/8MDXgPzZrHmzX4cn5mQCzG9cQ5/e6X3h1WvyZ6H2zVG/qqkxdxZt3vrUny+En3Fq5iKE3MXTBu6ZXH/V0scBuHWXA5Pqj18wGYCaxFzHxkKOZeJrboyf6QuGHJBU/6PakGOcmne7sLEutidf+/fu2T+pfmb9CgAG75Sebbq4qT45RxRClmhNju03NtXn/kx8eWCrpyW/45qlT+TOlh03YLfk+onLnmxTPnHeLNe8eb2pv+fmxzzgvJ+51Hzc2pj9umfPfkn1j9evbFN93izXHjXpmdsNjasrOkJntX8q3zl0Qz5T2dHFMtiYEwu3S3vyYeOyj0n6j6SvlL91zjnnnKuotW+X71aFqrZDl9M4YPuiZccRpiU5sbmV4jQozjnnnOvkbN3bZbtVo/dUhy5nPuyJwHeAHSR9IrON12Ju65PAHpI+K2l2XPfqQidP0pUxBWKpZ7k655xzbmN6T3XoSMyHlbQj8F9mNhv4E3B8ZhsfBJaY2W7Ai/G5vcxsGCFtojDx8I/MbCQwhDCx8ZAKvD7nnHPOlbJ2bfluVeg9c5VrM/mwtzVTfgKhIwdhkuHfEyY7htBpK1wtcAAwApgTJxDejPWTB39G0njCe7gdYV672qI2jSdOTHj11S2mhDnnnHOuHaxKz30rl/dMhy6nE4FtJRVG27aX1MfMVgNvmlmhey7gOjP7QXZlST2A7wK7mtnLkiYCmxbvpDjL9dILLuyAl+Kcc865ar2YoVzeM4dcU/NhJfUDPmhmnzCzmpg6cREbZrUWTAWOLeS0SvqIpJ2ADwH/Al6RtC1wSAe9LOecc865VlXzCF1b82EfBO4q2tYdhEOvP8kuNLNlkn4MPCSpG/Af4HQzmyVpAbA07uexsr4y55xzzuVSrVenlkvVdujamQ9b/HwtMds1m/8aH98K3FpinXE5muucc865jlSlFzOUiydFVI6/0c4557qSiqYtvD3tf8v2d3aT/b5ZdUkRVTtC55xzzjlX4Fe5uorp2SMtY6++IWTs9U7M2FsTMwJTs0cfXL0EyJ9ZmJz9CjDwGHr3Sp+ab01dLauO3jW5vu9dc1hyaHpu5KC/PJmcawoh2/R7g/dNrv/54kdy53CesHNa9i7ALcvnMm//9NzOEQ+HLNdJI9KyU4+YF7JTD+kzOKn+/tWLgfyf6dlj9kqqHzU9nJZ6VL9dkurvXrkAgJrEnMzGmJM5ZbcxSfVjn5wOwBcHvOuMjmb9ftms3PnE/fulf6ZXrHyS84fsn1x/du3DXDp0bHL9txdNyf0dW3b4qOT6AffOZsHY9GzZXaY8AaT/DH6/bBZAct5tIf944IC0fNmly0K2bN4s116J9XVtzEvO+3cjtf1hH03J7cm2qWK6eIeuYle5StpB0j2SVkuqk/RrSe/v4H2+Fv+tkbQks3zvmP6wQtJKSaeXYz/OOeeccxtDRTp0CrPy3gncbWZ9gL7AFsAF7dxu7hFGSf8F3AScZmb9gb2AL0g6uj1tcc4559zGY+vWlu1WjSo1Qrc/YcLePwDEiXu/RehIzZE0sFAoabqkEZI+KOna+PwCSUfG58dJuk3SnwnTiWwhaaqk+ZIWF+pacDow0czmx7a8AHwPODNuf2LMfy20pzDKl3c/zjnnnKsQW/t22W7VqFLn0A0E5mUXmNk/Jf0VuBf4DHCOpO2A7c1snqQLgYfN7AuStgZmS5oSV98DGGJmL8VRuqPj9rYBZkmaZM1fvjuQEAuWNZc4bUkL3sy5H+ecc865iqhUh06UnrZDwHTgSuAcQseukL96EHCEpO/Gx5sCn4z3J5vZS5ltXChpNLAO+ASwLfB/OduS8hry7MezXJ1zzrlKqdKRtXKpVIduKXBMdoGkDwE7AnOAFyUNAY4HvlIoAY4xs5VF6+1GiN0qOBn4GDDCzP4jqZESuapFbRkJTMosG0EYpQN4m3goOp77V7hwI+9+3pXlevFFv2yp3DnnnHNtVK3nvpVLpc6hm0qI6vocgKTuwC8J57K9Tojd+h6wlZktjus8CHwtdqqQ1Nz8BVsBz8dO1n5Aa9dg/wYYJ2lY3O5HCRdnFGK/GgkdPIAjgfe1cT/OOeeccxVRkQ5dPM/saOA4SauBVYRz0n4YS24HTgD+lFntJ4TOVG2ccmSDnNWMG4GRkuYSRtFWtNKWZ4HPAhMkrQSeAS4zs0diyTXAvpJmA9nRwFz7cc4551wFrX27fLcqVLGJhc3sb8Cnm3nuueK2mNkbrD/8ml0+EZiYefwC4SKJUtvdIv7bCAzKLJ8BjAKIc9D9UNIDZvZybEt25sofpO7HOeeccxtHtV6dWi6e5Vo5/kY755zrSiqah/rGrd8u29/ZzY6/1LNcnXPOOecqrouP0HmHroJ6JGbmNcSMvbw5mb16pWW51tWFFLQ+vYcm1a9eswggdzZr3uzXgw++PLn8gQe+lpyJCCEXcef+6Vmoy1c8To+aPsn1DY2r6Zsj43BVG3I7Bw9Ky2UFWLwkZLMOGrhfUv2SpdMAqNmpR1J9Y1NDrM+XY5k/JzMxm7UpZLP2T/wZrIgZk7vtelJS/ZNzbgKgT5+0bFmA1asX5M737dWztekw16urX0a/vun5xytXzUnOuoWQd3vgAc2duvxuk6f+T+7P6ICd906uX7Z8JkDy92xV/Bnn/T03MPE7sDR+B/LW585azZlPnPfvTN4s15456uvjPirFr3J1zjnnnHNVrUM7dJJ2kHSPpNWS6iT9WtL7W1+zXfssRHXVxKtjC8tHSZohaaWkFZJ+J2nzMuzv3Mzkx84555zbGLr4Va4d1qGL88fdCdxtZn2AvsAWhDnf2rPd3IeJJW1LSKD4vpn1A3YGHgC2bE9bnHPOOdc52Nq1ZbtVo44codsfeNPM/gBgZmuBbwFfkDRH0sBCoaTpkkZI+qCka+PzCyQdGZ8fJ+k2SX8GHpK0haSpkuZLWlyoa8HpwHVm9kRsi5nZ7Wb2nKSPSLpbUq2kWTGxojDydm1sW72kr2fa+6M40jcFSDvRzTnnnHOug3TkRREDgXnZBTHY/q/AvYTc1nMkbQdsb2bzJF0IPGxmX5C0NTA7dpogzAE3xMxeiqN0R8ftbQPMkjTJmp+DZRBwXTPPnQcsMLOjJO0PXA8Mi8/1B/YjjOStlHQlMIQwCfIuhPdvfvHrLPAsV+ecc64yuvo8dB3ZoROl514TMB24EjiH0LG7LT53EHBE5py0TYFPxvuTzeylzDYulDQaWAd8AtgW+L82tHNvYs6smT0s6aOStorP3WdmbwFvSXo+7mMf4K4YWYakSaU2Gre3QZbrRRe062izc84555qzrmt36DrykOtSYGR2gaQPATsCc4AX4+HN4wlZrhA6aseY2bB4+6SZLY/P/SuzqZOBjwEjzGwY8Byh89dSW0Y081ypyQMLHdG3MsvWsr4D7JMEO+ecc67T6MgO3VRgc0mfA5DUHfglMDGObt0CfA/YyswWx3UeBL4WL6hAUnOTPm0FPG9m/5G0H9DaxDhXAKdKemfiL0mflfRfwAxCBxFJY4AXzOyfLWxrBnC0pM0kbUkzcWbOOeecqxy/KKKDxPPZjgaOk7QaWAW8CfwwltxOOBftT5nVfgK8D6iNU440N6vljcBISXMJnbEVrbTlubivX8SLGZYTDp3+Ezg3bqsWuBg4tZVtzQduBRYCdwCPtlTvnHPOuQpYu7Z8tyrkWa6V42+0c865rqSieaivXv6Zsv2d3fJrf/IsV+ecc865SvOrXF3F9OjRN6muoWEVkD+zcFSPtOzR2Q2rAeiXuP2Vcfurjk7Pjex715zc2ax5s1/rPpPenl5/msO3Bu2TXP+rJY/mrs+biTi8pndy/fzGNfx+2EHJ9V9c+BAAPx6SluX609qQ5Zqaj1sXMxpT833X1NUCcPqgvZLqf7PkMQCO69/ctUwbum1FmDkobxbtXcPHJtUfPT/MnpTafgivIW92at685N0Tf6cAzGpYxSF9BifX3796Ma9f95Xk+s1PvZrH90n/zuz56KNMGpGe/XrEvJBPnPczcUTfYUn1k1YtBEj+vfXAA18D8ucZ580nzrv93JnhObNZa3JkVjfGvx0VU6WHSsvFs1ydc84556pcp+7QKZgp6ZDMss9IeqCd210raaGkRTFtYs+EdX4naUC83yhpG0lbS/rv9rTFOeecc+3X1a9y7dSHXM3MJJ0G3CZpGtCdkAV7cFu2J6l7jCB7I85fh6RPARcB+7bSli+VWLw18N/Ab9vSHuecc86Vh62rzo5YuXTqEToAM1sC/Bn4PiFZ4o/Aj0rkvdZIejSOuL0z6iZpjKRpkm4CFpfYxYeAlzO19xaekHSFpHHx/nRJI4vWvRjoFUf7LinrC3fOOedcpyXp4DgV2hpJZ5V4/szYP1goaUk8OviR+FxjzKJfGKdga7dOPUKXcR4hM/XfhBzYUnmvzwMHmtmbkvoAN7M+qWIUMMjMGuLjzSQtJKRLbAfs38Z2nRW3O6zUk57l6pxzzlVIBQ+VxrCE3wAHAk8Bc2Km/LJCjZldAlwS6z8NfCsTYQqwn5m9UK42VUWHzsz+JelW4DVC9uunS+S9PgNcIWkYIaYre/nX7ExnDjY85LoHcL2kQR3Q7g2zXC/6Rbl34Zxzzjmo9Llvo4A1ZlYPIOkW4EhgWTP1JxIGmjpMVXToonXxVsh7XZl9UtK5hEzXoYRDyW9mns7mwG7AzJ6QtA0hG/ZtNjwM3VI+rHPOOee6pk8Af8s8fgrYrVShpM0J5/6fkVlswEOSDLg6DgC1S6c/h66E5vJetwKeNbN1wCmECyhaJal/rH0RaAIGSPqApK2A1iZJehXYMv9LcM4551w52dp1ZbtJGi9pbuY2vmh3pZIkmkuq+DTwWNHh1r3MbDhwCHC6pNHtff3VNEJX8BPgfwl5rwIagcMJV5reIek4YBotjMqx/hw6CD+UU+PVr3+T9CegFlgNLGipIWb2oqTHYu7s/WZ2ZptflXPOOefabu26sm2q6JSpUp4Cdsw83oFw6lcpJ1B0uNXMnon/Pi/pLsIh3BltbjCe5VpJ/kY755zrSiqah/rSj/ct29/Zj/z0kRbbLmkTYBXhSN7TwBzgJDNbWlS3FdAA7Ghm/4rLPgh0M7NX4/3JwPlm1q45dqtxhM4555xzbgOVvCjCzN6WdAbhNLDuwLVmtjTOnYuZXRVLjwYeKnTmom2Bu+KZY5sAN7W3Mwc+QldJ1rPHzkmF9Q3LgfxZq8NqeiXVL2ysA/JnxS45tOT5niUN+suTybmgELJB82az5s1+ParfLq3XRXevXMDY3gOT66esWUqPmrQsXYCGxtXs3bN/cv3M+hV8fdDeyfWXLZkJwAk7F0+dWNoty8M0SKmvecqa8D+hefOJz0nMlj0vZsumvkcz61cAkPc7lpolWsgRTX0/IbynefN9+/dL/46tWPkkh/Udmlx/36pFjO6V9v4AzKhbzt/Gpb/eHSfOZeKwA5Prxy2czCVD07J0Ac5cFPJ082a5njJgVFL9DctmA7D7qFOS6mfNvgHIn3+c+nuioTFkbqfmATc2dmz2K4T819456tc0NVV0hO6F7+1Vtg7NNj9/rKJtL4dqvCjCOeecc85ltNqhq0Ce6hJJf46TBFeEpHGSrihatkhSs3PEFKdIFD3XGKc+cc4559xGUM6rXKtRqx06C8dkTwMulbRpPIHvAuD0tuwwzq4McXJfMxsEvNTW7ZWDpJ0J78Xo+Pqcc845V0W8Q5egAnmqTxAm6UNSL0kPSJoXt9U/Lp8o6cq4nXpJ+0q6VtJySRMLG5J0YsxHWyLpZ5nln5e0StIjwF5F+z8JuAF4CDgis87BklZImgn8v8zyj0p6KL72q6nwlTzOOeecc1l5rnItd54q8M6I3QHA7+OiCcBpZrZa0m6E+eUKWasfjvePIHQw9wK+RMhQGxb3/zNgBPAyYRbmo4AnY/tHAK8Q5qnLzjF3PCGPrR9hJuebJW0KXBP3twa4NVN/DjDTzM6XdBgxr7WYZ7k655xzlWHruvZFnskdug7IUy1M7lsDzAMmS9oC2BO4LV7OC/CBzDp/NjOTtBh4zswWA0haGrezEzDdzP4el98IFGZfzi6/tdA2SbsCfzezJklPAddK+nDcVoOZrY51f2R9x200ccTOzO6T9HIz79kGWa4XX/SrUmXOOeecaydb6x26PMqZp/qGmQ2Lk+7dSziHbiLwDzMb1sz+38q0463M8nXxtbzdQtub+0mfCPSX1Bgffwg4Bpjbwjotbc8555xzrqLaOm1J2fJUzewV4OvAd4E3gIYY31W4wjZ9oqVwaHVfSdvEQ7knAo/E5WPiuW/vAwrb7xbvDzGzGjOrAY6M660AekgqTO52YmY/M4CT4zYOIRwKds4559xGYmvLd6tGbe3Q/QR4HyFPdUl8DOF8t1MlzSIc0mwpT/UdZrYAWETIOzsZ+KKkRcBSQgcriZk9C/yAcI7cImC+md0Tl59LuPhiCuFcQAiHTp82s6czm5kBDCB00sYD98WLIpoyNecRroidDxwE/DW1jc4555wrP1trZbtVI0+KqBx/o51zznUlFZ0B4pkvjyzb39ntr5lbdbNXeJarc84556reuuqcPq5svENXQTU79Uiqa2xqiPU1ifWNAJy8c1oW6o3L5wDwqT6DkuofXL0EgFt3Sc9pPH7BZHbuv2dy/fIVj/OtQfsk1/9qyaO5s1nzZr/+ODF3FOCntdOSs3Eh5OMOSPw8ACxrasidkwlwU+I6J8X61DYtK3xGa3on1Tc2rgFgwtC09oxfFNqTmlV636pFoT05cyw7KusWQt5t/xyfiRVNjfTs0S+5vr5hJV8euEdy/TVLn8idB5yagwohC/X8Ifu3XhidXftwm7Jc9+89IKn+4TXLABiTmF87vS7k+/bpk/Z7ZfXqMPNVahZqQ/zMDUz8ji195+9Avs/08MTv5Pz4ncyb5VqzU1q2bGhTfXJtOVTruW/l4h0655xzzlW9rt6ha+tFEckkmaQbMo83kfT35nJRW9nWdEmfKlr2TUm/bcO2NpH0gqSL8q7rnHPOOdeZdHiHjnCl6yBJm8XHBwJPt1DfkpsJV8JmnRCXJ8lkyR4ErAQ+U5h+pYVa55xzznVi69aV71aNKtGhA7gfOCzeP5FMB0zSKEmPx1zUxyX1i8sHSpotaaGk2hgldjtwuKQPxJoaYHtgZsyLnS7p9pi/emNmnrxGSWfH6UeOy7Tj14QpR3bPtGeDWkkHSXoiZtPeFtMsiDVzYmbshOY6hc4555zreD4PXWXcApwQ81GHECb6LVgBjDazXYCzgQvj8tOAX8fUiJHAU2b2IjAbODjWnADcauvnXtkF+CZhHrmehKzXgjfNbG8zuyWOFh5ASKi4mQ0nDX6nljBn3Y+BsWY2nJAe8e1Yc4WZ7Wpmg4DNgMOLX7Sk8ZLmSpo7YcKE4qedc84558qiIh06M6slZK2eCPyl6OmtCNmtS4BfAYXLyJ4Afijp+8BOZvZGXJ497Fp8uHW2mT0VkyoWxn0W3Jq5fzgwzcxeB+4Aji46vFqo3Z3QOXws5s6eSsh4BdhP0pMxV3b/TLuzr3uCmY00s5Hjx48vfto555xzZbJuncp2q0aVGqEDmAT8gnef7/YTQudqEPBpYFMAM7sJOIIQB/agpML18HcDB0gaDmxmZvMz28rmu65lw6t4s6kVJwJjY37rPOCjwH4lagVMNrNh8TbAzL4YRxp/CxxrZoOBawrtds4551zl+Tl0lXMtcL6ZLS5avhXrL5IYV1goqSdQb2aXETqDQwDM7DVgetxe8sUQme1+CNgb+GQmv/V03n3YFWAWsJek3nHdzSX1ZX3n7YV4Tt2xedvhnHPOOVcuFevQxUOhvy7x1M+BiyQ9BmQPex4PLImHOvsD12eeuxkYSjg3L6//BzxsZtnRvHuAIwoXW2Ta/HdCJ/NmSbWEDl5/M/sHYVRuMWHEcE4b2uGcc865MunqF0V4lmvl+BvtnHOuK6noyWgrjhxVtr+z/e+ZXXUn0lXykKtzzjnnnOsAHv1VQXlzL3v06JtU39CwCsife5k3y/V7g/dNqgf4+eJH6FHTJ7m+oXF17izXvLmaebNZ82a/pv68IPzMPnXqtOT6B6/bj1mj03M4d58xE4C7hqdlZR49P+RkDh1ySFL9otr7AZKzR+sbVgIk5+/evTLkZK4+Ni2fuM/t4ayHITVpOZO1jSFjMm/uZWoeM4RM5r59RyTXr1o1j1690r6TAHV1S3LnAef9Dh/XP739t62Yx2kD0/Obr1r6eO72QP783T17pn1GH68Pn9HevYYk1a+pqwXyf+Z2T/w9MSv+Xs+bFZs3u7ZnjizX+qYmeueoXxPbVCnrqvRQabl4h84555xzVa9apxspl4odcpW0NqY+LIqpC+n/K9f8NodJOjTzeFzMiV0Yb9dLOkLSWa1sp5uky2Lqw+KYANEjPtcYlxW2uWdc/oCkf7Qlk9Y555xzrpwqOUL3Rkx9QNKngIuA9PH20oYRUiSykxXfamZnFNVNamU7xxMixIaY2TpJO7DhvHX7mdkLRetcAmwOfCV3q51zzjlXVuYjdBvFh4CXASRtJ2lGHP1aImmfuPw1ST+TNE/SlJj5Ol1SfRx1ez9wPnB8XPf4UjuKo3ZXxPsT40jc43E7hfnjtgOejQkThSlWXm7pBZjZVODVcrwZzjnnnGufrj6xcCVH6DaLc8ptSuhAFZIfTgIeNLMLYvzW5nH5B4HpZvZ9SXcBPwUOJERxXWdmkySdDYwsjMhJGkfo4BXOHv81754uZDvCxML9CSN3twN/AmbGzuRU4I9mtiCzzjRJa4G3zGy31BcsaTwwHuDqq69OXc0555xzLpeNdch1D+B6SYMIk/JeK+l9wN1mtjDW/xt4IN5fTOhM/Sdmp9a0sJ8NDrnGTl7W3XEkbpmkbSGMyEnqR+hk7g9MlXRcHIWD0odcW2VmE4AJhYcXXvjzvJtwzjnnXAK/KGIjMLMngG2Aj5nZDGA0If7rBkmfi2X/sfWzHq8j5rTGzlh7OqLZhIh3fvpm9paZ3W9mZwIXAke1Yx/OOeecq6B161S2WzXaKB06Sf0JMV8vStoJeN7MrgF+DwzPsalXgS3L0J7hkraP97sRcmMrO4GOc84551wbbYxz6CCMjJ1qZmsljQHOlPQf4DXgc6VXL2kacFbc7kXtaNvHgWsyWa6zgStaWkHSo4Tz8LaQ9BTwRTN7sB1tcM4551wbra3SkbVyqViHzsy6N7P8OuC6Esu3yNw/t9RzZvYSUDyV/MSi2omFZWY2rpntPMD68/WK21HTzPL0WAPnnHPOdahqPVRaLlp/mprrYP5GO+ec60oq2sOaNXrvsv2d3X3GzKrrHXr0VwWlZubVx/y7nj3SMvnqG0ImX7+dapLqVzY1AtA/sX5FrK/ZKS2zEKCxqZ6+idsHWNXUSE2OjMDGpqbcWbF525M3mzVv9uvee6TPST3ziasZnOP9X9wUciMHJmZ9Lm1qAPLnWKZmmzbG7eetz5tLOapH2mdidsNqIP93LO93IDW/GUKGc01iLmior8/9GU3NHYWQPZr3O5aaOwohe7RXjvq6+Hsx78+4JvF73xh/z6VmlRZySvNuP/X30KpYn/f3et725P29m7r97D4qZZ1VXR+srLxD55xzzrmqV60TApeLZ7mSP8s17vcJSUsl1TaXUuGcc845Vwme5RrkynKV1Bf4nJmtjtOdzJP0oJn9o42vwznnnHPtsLaLH3L1LNcgV5arma0ys9Xx/jPA88DHyvLOOOeccy63Sk8sLOlgSSslrSl1JFDSGEmvZI7wnZ26blt4lms7s1wljQLeD9QVv2DPcnXOOefee2J/5TeEfslTwBxJk8xsWVHpo2Z2eBvXzcWzXGl7lquk7YAbCJMkv+t0zOIs14svuKCFZjvnnHOurSp8yHUUsMbM6gEk3QIcCaR0ytqzbrM8y3V9m3JluUr6EHAf8GMzm9WO9jjnnHOundaZynZL8Angb5nHT8VlxfaIF4PeL2lgznVz8SxX8me5xvP37gKuN7Pb2rt/55xzznUeksZLmpu5jS8uKbFa8Sle84GdzGwocDlwd451c/Ms1yBvlutnCKOKH80c0h2XOVzsnHPOuQoq5yHXolOmSnkK2DHzeAfgmaJt/DNz/y+Sfitpm5R128KzXMmf5WpmfwT+WKreOeecc5W3trIBm3OAPnHO2qeBEwgXeb5D0n8Bz5mZxQsouwEvAv9obd228CzXyvE32jnnXFdS0asU7h25f9n+zh4+9+FW2x6DDf6XcArZtXG2jtMAzOwqSWcAXwXeBt4Avm1mjze3bnvb7B26yrHUzLzGmBHYp88uSfWrV4cZVobV9EqqX9gYZljJm/N5ws4jk+oBblk+l/79dmu9MFqx8kmG58i9nN+4hr179k+un1m/ggGJrxdgWVMDnzp1WnL9g9ftlzubNW/265VDD0wu/+qiyQB8eeAeSfXXLH0CSM91LHxG836GjumfdorsHSvmA3DoYdck1f/lvi8D+b8zpwwYlVR/w7LZQP4s18P6Dk2uv2/VotzfgbzfybzZsqMTs3QBZtQtz/2dzFsP+fN3U9/T+Y1rAJLzaBsKWa6J22+M2+/Vc0BSfV19uOAx9TPXGPOb836H8+bv5swbrmiHbtKIA8rWoTli3tSqm6XYs1ydc845V/U8KWIjUGVyXc+V9N2imsZ4QmJL2+kf27ZAUi9JP8pkti6UtFusmx5neS7MAH1sS9t1zjnnnOsoG2uErlK5rm1xFHCPmZ0TJ0A+HBhuZm/FzuD7M7Unm9ncdu7POeecc+1U4YsiOp2NleWaVbFc1wJJNZKWS7omjr49JGmzOML3TeBLkqYRYsJeMLPCpMYvxOxW55xzznUia1HZbtVoY3XoNosdrxXA74CfxOWFXNdhwFBgYVxeyHUdQZhMuJDrejRwvpn9GzibEPs1zMxuTWhDH+A3ZjaQcAnxMWb2F+Aq4Fdmth/wELCjpFVx/pjiUcQbM4dcP1q8g+zEhBMmtDSdjXPOOedc23WGQ64dleva3OBrYXlDZvvzSm3HzF6TNALYB9gPuFXSWXFuO2jlkGtxluuFnuXqnHPOdQg/5LqRdWCu64vAh4uWbUkYjYMNM13XNrcdM1trZtPN7BzgDOCYxJfmnHPOuQpZW8ZbNdroHboOzHWdARwhacu4n/8HLDKz5J+VpH6S+mQWDaOFjFfnnHPOuY1hYx1y7fBcVzO7VdIVwExJBjwPfClnO7cALpe0NWGm5zVAcUCvc8455zayah1ZK5eN0qGrVK6rmV0NXF1ie43AoMzjX5TavpnNA0rOkWdmY0otd84551zlVevVqeXi0V+V42+0c865rqSiPazfDzuobH9nv7jwoarrHXr0VwWlZuA1NoZMvl2GHpZUv2DRfQB8Z/DopPpfLp4BwGkD0wI6rlr6OADz9k8P9Bjx8OMMHnRAcv3iJVP5/bCDkuu/uPAhvj5o7+T6y5bMZOKw9CzUcQsnM2t0+vZ3nzGTwTlyPhc31efPZs2Z/Qrwz18emVT+oe/cA8Buu56UVP/knJsAqNmpJqm+sakRIPlnMG5hyKL98ZD9kup/Whtyd3sl5lLWxRzLHwwek1R/0eLpAPTo0TepHqChYRX9E98fgBVNjck5nBCyOFOzeiHk9X5vcPr87T9f/AgXD0n/Dp9VO5UbcnzHTlk4mXtH7p9cf/jch4H8ecAn77xrK5XBjcvnAPmzYvsm/oxXxe9A3vzjvNmsYxLzd6fXhfanfmcgfG/yfkYraW0XH6DyDp1zzjnnql5XP4euw65ylfTRzKS7/yfp6czj9xfVflPS5gnbnC5pZLzfKGlx3N5iSWlDES1vv0bSSZnHm0u6MW5/iaSZkraIz63NvJ6Fkmrau3/nnHPOubbosBE6M3uRMM0Hks4FXstefFDkm8Afgddz7mY/M3tBUj9CqsM9bWrsejWEtIqb4uNvAM+Z2WAI05gA/4nPvTM5snPOOec2Lh+hqyBJB0haEEe8rpX0AUlfB7YHpsX8VCRdGSOzlko6L2HT2TzYD0q6T9KiOKp2fFzeKOlCSU/EbQ+X9KCkOkmnxe1cDOwTR9y+RchyfbqwEzNbWch1dc4551zn0dUnFq7kOXSbAhOBA8xslaTrga+a2f9K+jZxtC3W/sjMXpLUHZgqaYiZ1ZbY5jRJAnoCn4nLDgaeMbPDACRtlan/m5ntIelXsS17xXYtJWS4ngV818wOj+sOAx6SdCwwFbjOzFbHbWXn0msws6OLGydpPHHeuquvftfsKc4555xzZVHJEbruhI7Pqvj4OkLMVymfkTQfWAAMBAY0U7efmQ0CBgNXxPPbFgNjJf1M0j5m9kqmflL8dzHwpJm9amZ/B96MkwdvIGa99gQuAT4CzJFUuIToDTMbFm/v6szF9SeY2UgzGzl+vM9H7JxzznWUtVjZbtWokiN0/0opktQD+C6wq5m9LGkiYRStWWZWJ+k5YICZzZY0AjgUuEjSQ2Z2fiwtHC5dx4ZZrs1mwprZa8CdwJ2S1sXtLk95Lc4555yrjGo9VFoulRyh2xSokdQ7Pj4FeCTez+awfojQ+XtF0rbAIa1tWNLHgR5Ak6TtgdfN7I/AL2hHHqykvSR9ON5/P2Gk0LNcnXPOuU5mrVnZbtWokiN0bwKfB26TtAkwh3DeGsAE4H5Jz5rZfpIWEM5rqwcea2Gb0yStBd4HnGVmz0n6FHBJHE37D/DVHG2sBd6WtIhwjt2LwJXxPL1uwH1AjtldnXPOOec6XkU6dEX5q7uUeP5y4PLM43HNbGdM5n5NMzUPAg+WWF6TuT+R0GErta3iqdGvb2Y/W5Ra7pxzzrnK6+qHXD3LtXL8jXbOOdeVVDQP9fwh+5ft7+zZtQ97lqtrXt7cy91HnZJUP2v2DQDcvsvYpPpjF0wB4NZd0nIXj18QcjUnjUjPdTxi3lQGDUzL4QRYsnRacm4nhOzOE3YemVx/y/K53JQjZ/KkhZO5a3ja+wlw9PwpyRmNEHIa8+Zwpuaywvps1uT815j9Omb0mUnl02dcAuT/TKfmAY94OOQHp75H1yx9AoCeiTmT9TFj8oLErNIf1U4FoFevQUn1AHV1SxhW0yu5fmFjHUMS854Bahvrc2etXpEjP/iMRZOT854hZD7nzXK9f9f0LNdD5oQs16P6vesgT0l3r1wAwLgBuyXVT1z2JAA9e/RLqq9vWAlA78TP3Jr4mRte07uVymB+45o21Y/tPTCpfsqapQD0y5E3vLKpMfk7D+u/964yvEPnnHPOuapXrdONlEtFkyLKISZMPC9pSSt1YyTtmXl8blGe7MVx+Tv5sCW2cXhMtlgkaZmkr7S0Leecc85tHJ4UUX0mAlfQzMUKGWOA14DHM8t+1UKe7AYkfYBw9e0oM3sqPq5py7acc8455zpS1Y3QmdkM4KXsMklfjyNotZJukVQDnAZ8K46g7ZOybUmvSTpf0pPAboQO74txv2+Z2cqyvhjnnHPOlUVXn4eu6jp0zTgL2MXMhgCnmVkjYY67X8Vorkdj3bcyh0k/VWI7HwSWmNluseM4iTBZ8c2STpaUfb9a25ZzzjnnKqSrR3+9Vzp0tcCNkj4LvN1CXaGDNyzOV1dsLZmJg83sS4R56WYT4siuzbEtJI2XNFfS3AkTJuR9Tc4555xzSd4rHbrDgN8AI4B5MYmiLd40sw3OhzSzxWb2K+BA4Jg8GzOzCWY20sxGjh8/vo1Ncs4551xrfISuysXDoDua2TTge8DWwBYU5bK2YbtbSBqTWTQMz3F1zjnnOqV1ZmW7VaOqu8pV0s2EK1i3kfQU8BPgFElbEWal/pWZ/UPSn4HbJR0JfK0tuwK+J+lq4A3gX8C4MrwE55xzzrmyqroOnZmdWGLx1SXqVgFDMoseLa6JdWMy97fI3H8VOLSZdc5Na61zzjnnKqFaD5WWi2e5Vo6/0c4557qSiuahnj5or7L9nf3Nksc8y9U1ryYx868xZv59a1DS9Hn8akkYfKxJzIFsbKxvU3sO6TM4qR7g/tWLqcmRbdrY1ECvxPYA1DU1JWcWQsgtHJCjPcuaGhg65JDk+kW199O715DWC6M1dbXJ7z+En8Fuu56UXP/knJuA/NmsebNf837mVh29a1J937vmADBg572T6pctnwmQ/DNYU1cLwKVD0/J6v70o5B+vOHJUUj1A/3tm89Co9Hzig2ZPY/BO6Vmui5vqmTU67f0B2H3GTPbsmZZTCvB4/crkHFEIWaJ5v5Mje6Rvf25DyCpNzbutjZ+5Mb12TqqfXrccSM82XRlzSmsS36PGmLWaN/847+/p1N+7jU0NubZf2Efeelc53qFzzjnnXNWr1gmBy6XTX+UqaUdJ0yQtl7RU0jdyrv9OVqukRkmLMxMC7ympprlcWEndJF0maUlcb46kHs1tq/2v1jnnnHNt0dWnLamGEbq3ge+Y2XxJWxLmmZtsZsvauL39zOyFwoMYE/YucS6744DtgSFmtk7SDoSrXUtuyznnnHNuY+j0HTozexZ4Nt5/VdJy4BOSfgs8CexHmHvui2b2qKTNgD8AA4DlwGap+5I0jjBJ8aaEGLB7gWfNbF3c/1NlelnOOeecK6NqnT+uXDp9hy4rjqbtQujIAWxiZqMkHQqcA4wFvgq8bmZDJA0B5hdtZpqktcBbZrZbid3sQRiReymOyM2UtA8wFfijmS3IsS3nnHPOVUClD5VKOhj4NdAd+J2ZXVz0/MnA9+PD14Cvmtmi+FwjIQBhLfC2mY1sb3uqpkMnaQtCzuo3zeyfkgDujE/PA2ri/dHAZQBmViuptmhTrR0mnWxmL8X1n5LUD9g/3qZKOs7MpqZsS9J4YDzA1Ve/a6o855xzzlUhSd0JkaMHAk8BcyRNKjodrAHY18xelnQIMAHIDv6U9bStqujQSXofoTN3o5ndmXnqrfjvWjZ8Le3ppmfPkcPM3gLuB+6X9BxwFGG0rlVmNoHwAwSwCy+4oB3Ncs4551xzKjxCNwpYY2b1AJJuAY4E3unQmdnjmfpZwA4d2aBquMpVwO+B5WZ2acIqM4CT47qD2DAtIu++h0vaPt7vFrflE+s455xznUw5s1wljZc0N3MbX7S7TwB/yzx+Ki5rzhcJg0MFBjwkaV6JbbdJNYzQ7QWcAiyWtDAu+2EL9VcCf4iHWhcCs9ux748D10j6QHw8G7iiHdtzzjnnXCdXdIStlFJJEiWHCCXtR+jQZWcC38vMnpH0cWCypBVmNqPNDaYKOnRmNpPSb9xfMjUvEM+hM7M3gBOa2VZNiWWNwKB4fyIwMfPcA8ADqdtyzjnn3MZR4UOuTwE7Zh7vADxTXBQvzvwdcIiZvVhYbmbPxH+fl3QX4RBuuzp0nuVaOf5GO+ec60oqmod6XP8RZfs7e9uKeS22Pc5Vuwo4AHgamAOcZGZLMzWfBB4GPpc9n07SB4FucSq2DwKTgfPjIFKbdfoRuveSvBl7PRIz8xoKGX65s1xrEtvTCEDPHuk5kPUNK3Nn/uXNQu3Ro29yfUPDquTMRQi5i/lfb77s2oE56pc2NST/vML2G4E25Ebm/AzlzX7t+JzMjsmxLGRS9qjpk1QP0NC4mt45vgNr2pCTmTefOPV3CoTfK7m/A4mfHwifobz10PHZpqn5soVs2fztqUmsb2zj9vPVp34nIXwv82Zuv1eZ2duSzgAeJExbcq2ZLZV0Wnz+KuBs4KPAb+PMHIXpSbYF7orLNgFuam9nrrAh55xzzrmqtq7CB8LM7C9kTv+Ky67K3P8S8KUS69UDQ8vdnnZd5SrptXI1JG5vvKQV8TZX0ph2bGuMpHvj/XGS/p7JXb1e0hGSzmplG57l6pxzzlWBtWZlu1WjTjNCJ+lw4CvA3mb2gqThwCRJu5nZ02XYxa1mdkbRskmtrHM8nuXqnHPOuU6u7PPQSRomaZakWkl3SfqwpI9LmhefHyrJ4smCSKqTtDkhHuPMQgfJzOYTMllPj3WNkraJ90dKmh7vj5L0uKQF8d+kkz7iqN0V8f7EOBL3uKR6ScfGsu0oynI1s5fL8kY555xzrmzKOQ9dNeqIiYWvB75vZkOAxcA5ZvY8sKmkDwH7AHOBfSTtBDxvZq8DAwkRXllzgQGt7G8FMNrMdiGcgHhhM3XHZw6Pfr7E89sR5og5HCjksf0J+HRc55eSdilaZ1p87kmcc845t9Gsxcp2q0ZlPeQqaStgazN7JC66Drgt3n+cMEnwaEKn62DCJc2PtrTJhN1uBVwnqQ9hapD3NVO3wSFXSeOKnr87jsQtk7QteJarc84556pDJaO/HiWMzu0E3EO4wmNv1k+ktwwYUbTOcMIoHcDbrG/vppmanwDTzGwQ8Omi5/J4K3P/nY6kmb1lZveb2ZmEjuhRqRs0swlmNtLMRo4fX5ZkD+ecc86VsM7Wle1WjcraoTOzV4CXJe0TF50CFEbrZgCfBVbHkbCXgEOBx+LzPwd+JumjEM7FA44GCkNbjazv8B2T2e1WhEn9AMaV79V4lqtzzjlXLdZhZbtVo/Yect1c0lOZx5cCpwJXxQsd6oHPQ4jYipPoFUbkZgI7FC4yMLNJsfP0WJyB+b+AoWb291h/HvB7ST8Esues/ZxwyPXbhBmZy8mzXJ1zzjnX6bWrQ2dmzY3w7d5M/Scz9y+k6AKGOCHfVbFD9wfgfEmfteBR4F3RAGb2RNHy/4nLpwPT4/2JZDJai5eZ2bii57aI/3qWq3POOVcFqnX+uHLxLNfK8TfaOedcV1LRLNf9ew8o29/Zh9csq2jby6HTTCzcFeTNFEzN+lwa688fsn9S/dm14cj0BUMOSKr/UW24qHf2mL2S6gFGTX+MgQNGJ9cvXTaD0welb/83Sx7jnCH7JdefVzuNCUMPTK4fv2gyR/UrnqWmeXevXJA7y/WY/sOT6+9YMZ+Jw9LbP27hZADm7Z8WYDLi4ZAbveroXZPq+941B8ifzZo3+3VIYtZnbcz5TM0erW9YCUDdZ9Jeb68/hdd76y7pP4PjF0xmym5jkuvHPjmdPXumZ6c+Xr+Sf/7yyOT6D33nntzt/87g9O/wLxfPYNru+ybX7zfrER4alf4dPmj2NABOGTAqqf6GZbOB/L8XOzorNu/v9bzfgd0TM65nNawCaEPecE1yfSGP1lWGd+icc845V/WqdULgcqnktCWtkrStpJtiWsM8SU9IOrpEXY2kJSWWny9pbMJ+dolpFZ8qV9udc845t/GsK+OtGnWaDp3CJbB3AzPMrKeZjQBOAHYoqmt2VNHMzjazKQm7O5Fwle2JzbUlTlPinHPOOdfpdaZOy/7Av+OVrgCYWZOZXR5zV2+T9GfgoeY2EDNZj5V0iKQ/ZZaPiesWOo7HEuasO0jSpnF5jaTlkn4LzAd2lHSmpDkxl/a8zPbujiOIS2MahHPOOec2Is9y7TwGEjpSzdkDONXMUs5wnQzsLumD8fHxwK3x/l5Ag5nVEaY1OTSzXj/g+pgL2w/oA4wChgEjJBXOEP5CHEEcCXy9MBmyc8455zaOrj6xcGfq0G1A0m8kLZI0Jy6abGYvpaxrZm8T5o/7dDxEexghbgzCYdZb4v1b2PCwa5OZzYr3D4q3BYSOZn9CBw9CJ24RMAvYMbO8+DWMlzRX0twJEyakNN0555xzLrfOdJXrUjKRXmZ2uqRtWJ/l+q+c27sVOJ0QMTbHzF6V1D3u4whJPyLMkfNRSVuW2IeAi8zs6uxGJY0BxgJ7mNnrkqbTTH6smU0ACj05u/CCi3K+BOecc86lqNZDpeXSmUboHgY2lfTVzLLN27G96cBw4MusP9w6FlhkZjuaWY2Z7QTcARxVYv0HgS9I2gJA0ickfZyQHfty7Mz1p5lUDOecc85Vjh9y7SQsRFYcBewrqUHSbOA64PvNrNJP0lOZ23FF21sL3AscEv+FcHj1rqLt3AGcVKI9DwE3AU9IWgzcDmxJOJS7iaRa4CeEw67OOeeccxtNZzrkipk9S5iqpJSJmbpG4H0lam4r2t4ZwBmZx+NK7HMSMCk+HFT03K+BX5fYzyHNtNE555xzG0G1jqyVi2e5Vo6/0c4557qSiuahDq/pXba/s/Mb11RdlmunOeTqnHPOOefaplMdcn2vq9kpLWS5sSkGjSeGJtfHkOhRPUrOnvIusxtWAzCspldS/cLGOoA2hNWnvV4Ir/m4/iOS629bMY+9e/ZPrp9Zv4LD+g5Nrr9v1SJWH5sW3A7Q5/Y5jOm1c3L99LrlHHrYNcn1f7nvy/x4SHqQ+U9rQ5D5lwfukVR/zdInABiw895J9cuWzwSgpqZ3Un1j4xogf9A4S+9IqmdguEA+b/t7JX7H6gpB7IntB2hsrGfggPRw+6XLZtAjR1B6Q1NTcjA8hHD4vN+Bsb0HJtdPWbOUT/UZ1Hph9ODqJRzSZ3By/f2rFwMkr1Ooz/t7rlfPAUn1dfXLABiY+DNY2tQAwODE34uL2/h3IPUz1BDrU7df2EfqdwbWf29cZZRthE7SWkkL49xx8yXtWYZtDpN0aObxOEl/j/tZKOn6VtafLmlkvN8Yp0HJ3VZJW0v678zjMZLubWkd55xzzlVOV7/KtZwjdG+Y2TCAGHp/EbBvO7c5jJDG8JfMslvjxQ7tkbetWwP/Dfy2nft1zjnnXAeo1o5YuXTUOXQfAl4GkLSdpBlxRGyJpH3i8tck/Sxmok6RNCqOqNVLOkLS+4HzgePjuseX2lHxaJmkKySNa2Nbt5A0NY7aLZZ0ZKy5GOgV23FJXLaFpNslrZB0Y8yIdc4555yruHKO0G0maSEhNWE7oJC5ehLwoJldEJMaCpMFfxCYbmbfl3QX8FPgQGAAcJ2ZTZJ0NjCyMCIXO2rHSyqcKPNroKGMbX0TONrM/hkPz86SNAk4CxiUGdUbA+xCyJ99BniMkBE7sw1tcc4551w7dfVJOzrqkOsewPWSBgFzgGslvQ+428wWxvp/EybpBVgMvGVm/4mT+Na0sJ8NDrnGzlW52irgQkmjgXXAJ4Btm9nGbDN7Km5jYWzzBh06SeOB8QBXX301zjnnnOsYfsi1A5jZE8A2wMfMbAYwGngauEHS52LZf2z9JHjrgLfiuuvI19F8mw1fR8lc1ZS2AifHf0fEDt9zLWzvrcz9tZRos5lNMLORZjZy/PjxeZrlnHPOOZesQzp0MeO0O/CipJ2A583sGuD3hHzVVK8S4rZa0gQMkPQBSVsBB7S1rYSc1ufjSOF+QOH67JR2OOecc24jsTLeqlFHnEMH4dDlqWa2Nh4SPVPSf4DXgM+VXr2kacBZcbsXlSows79J+hNQC6wGFrSjrTcCf5Y0F1gIrIj7eFHSY5KWAPcD9+V4Dc4555zrYF39kGvZOnRm1r2Z5dcB15VYvkXm/rmlnjOzl4Di2V0nltjW94DvlVg+JnO/JqGtLwAlZ2I1s5OKFk3PPNfeaVScc84559rMs1wrx99o55xzXUlFp/Pqt1NN2f7OrmxqrLqpyDz6yznnnHNVr6uPmniHroLyZuzVJNY3vpMzmS9XM//28+VY9t+pJrl+RVNj7lzKnj3Ss1PrG5Ynv96w/abk3FEI2aOpWboQ8nT79EnPxl29ekGbMhTz5kD27jUkqX5NXS1A8s+sMeZY9uzRL609DSuB/NmsebNfUz/TjTFbNvX9gfAe5f3M9eqVnoVaV7ck93egX47v5Mqmxtzf+dTfQaF+Te7vPMCAxHWWxfr8vxdrEtvTCOT/jqX+DFbG7efN6M6bLZs3yzVv3rCrnHZd5VrB/NYrimreyWhtYTvZHNfjJC2XNC0mS7wS210bUyo+nrNN50r6btteoXPOOefKratnubZ32pI3zGyYmQ0FfkAzV6LmNAw4tLWinL4I/LeZ7RcfPxrbPYQw8fHpG6FNzjnnnCuTrj5tSTnnoatYfmuWpCslzZW0VNJ5JZ4/G9gbuCqTw1p4ToT55QrtHiXpcUkL4r/9WmjTgEzbv97WN80555xzrr3a26HbLHZyVgC/A34SlxfyW4cBQwlzusH6/NYRhMl6C/mtRwPnm9m/gbMJ8V7DzOzWuF6hM7Uwzh+XPdz6IzMbCQwB9pW0wUkuZnY+MBc42czOjIv3idv5KzAWuDYuXwGMNrNdYjsubKFN/YFPAaOAc2K0mXPOOec2gkqP0Ek6WNJKSWsknVXieUm6LD5fK2l46rpt0d6LIjZWfuv0zHOfiZmpmwDbAQMIkwy35FEzOzxu6/vAz4HTCEkR10nqQ/iZttRJu8/M3gLekvQ8IfP1qWyBZ7k655xzlVHJQ6WSugO/IQxKPQXMkTTJzJZlyg4B+sTbbsCVwG6J6+ZWtkOuFc5vBUBSD+C7wAHxfLj7yJnlCkyKbYUwwjjNzAYBn25lW57l6pxzznVNo4A1ZlYfj+TdAhxZVHMkcL0Fs4CtJW2XuG5uZevQVTi/teBDwL+AVyRtS+gN57U3UBfvb0XohAKMa2ObnHPOOVdh5TzkKml8PD+/cCselfkE8LfM46fispSalHVza+8h142S31pgZoskLQCWAvXAY4n7KJxDJ+AV4Etx+c8Jh1y/DTzcljY555xzrrqZ2QRgQgslpZIkio/6NleTsm5u7erQbaz81qKM1nHNtGFMM/enE0biSq3zBNA3s+h/WmhTdr302UCdc845V+2eAnbMPN4BeCax5v0J6+bmWa6V42+0c865rqSieag1ZcxybWwly1XSJsAq4ADCqVpzgJPMbGmm5jDgDMI8trsBl5nZqJR128Kjv5xzzjn3HlC5/qOZvS3pDOBBwvUD15rZUkmnxeevAv5C6MytAV4HPt/Suu1tk4/QVY716NG39SqgoWEVAHv37J9UP7N+BQBjeqXlOk6vW96m7U/ZbUxSPcDYJ6ez264nJdc/Oecm7ho+Nrn+6PlTmDTigOT6I+ZN5YSdW0yL28Aty+cyPEcu5fzGNblzNU8ZMCq5/oZls/nB4DHJ9Rctng7ABUPS3qMf1U4F4NKhaT+Dby+aAuTPA677TLNnLmyg15/mhH8Tt1+XM2+4kM2aN/t1bO+BafXAlDVLOWfIfq0XRufVTuOIvsOS6yetWshDo9K3f9DsaRzXf0Ry/W0r5uX+zt87cv/k+sPnPsyVQw9Mrv/qoslhP4k/gylrwt/HcQN2S6qfuOxJAPr2TXuPVq2aB+TPfs2bf5w7cztnfd8c+b6rmhrzZrlWeISuRxlH6Boq2vZyKGdSRIfT+uzYwq2mhdp3MmCz2auSJkpqKEyILOmchP2Ok7R95nGjpG3K8JKcc845VxYq4636VNsh13cmMm6nM83sdkmbAsskXW9mDS3UjwOWUIaTFp1zzjnXAaqzH1Y2VTVCV0p2tEzSyKIUidYUJg7+V1z/bElzYv7shBjbcSwhauzGOKq3WVzna5LmS1oc5+BzzjnnnNsoqq1DV8iOXSjprnZs55I4p9xTwC1m9nxcfoWZ7RqnIdkMONzMbmd9FuwwM3sj1r5gZsMJUR7fbUdbnHPOOddu3cp4qz7V1uo3YqdqmJkd3Y7tnBkP3f4XcICkPePy/SQ9GbNl9wdaOvv2zvjvPJrJoc3OND1hQkvzEzrnnHOuPVTG/6pRtXXoSnmb9a8jV46rmb0GTAf2jufT/RY41swGA9e0sr1ClmvJHNe4fc9ydc4551yHey906BqBwnXmx+RZMU7utxshy7XQeXtB0hbAsZlSz3J1zjnnOjOpfLcq9F7o0J0H/FrSo4TRshSFc+hqgcXAnWb2D8Ko3GLgbsLMzQUTgauKLopwzjnnXCfR1Q+5VtW0Jdks2MyyR9kwf7WwfCIxAzabG9tc9mt87sfAj0ssvwPIzkBak3luLjCmtbY755xzznUUT4qoHH+jnXPOdSUVHerq2WPnsv2drW9YXnXDdFU1Quecc845V4qq9Ny3cvEOXQX1qOmTVNfQuBrIn1mYN/dy8E5puZeLm0Lu5RcH7J5UD/D7ZbPo02eX5PrVqxdw+qC9kut/s+Sx3NmseXM4UzMXIeQu1iS+n6G+Pnd9ahYwrM8D7tVrUFJ9Xd0SAFYcmZYv2/+e2UD+z/Stu6Rldx6/IOR25s1m7d1rSFL9mrpaIP93LDn7FWDgMck5ohCyRFNzQSFkg3Z0VuzJO6dl7wLcuHxO7vpD+gxOrr9/9WIARvVI+8zNbgifubwZ1z179Euqr29YCeT/DuTPZk38DsTf0/0SP0MrY7ZszmxWeuaor4+vwVWGd+icc845V/30XrjOs+2q4tVLeq3o8ThJV7Syzjs1kj4WJwxeIGmfGBe2OF61uljSkQlt+GHmfo2kJW19Pc4555wrL9GtbLdqVJ2tzu8AYIWZ7RKvigXYL6ZFHAtclrCNH7Ze4pxzzjlXeVXfoZP06czo2xRJ2xY9Pwz4OXBoM/PIfQh4OVN/t6R5kpZKGh+XXcz6HNkbY2l3SdfEuod8fjrnnHNu45FUtls1qpZz6DaLEwEXfASYFO/PBHY3M5P0JeB7wHcKhWa2UNLZwEgzOwPeuRJmmsKdnsBnMtv+gpm9FDtocyTdYWZnSTojjughqQboA5xoZl+W9CdCSsUfs42OHcLxAFdffXUZ3gbnnHPOldTFz6Grlg7dG4XOFITz44DCJY47ALdK2g54P9CQuM39zOwFSb2AqZKmx2zXr0s6OtbsSOi4vVhi/QYzWxjvzyMz2XCBmU0AJhQeXnThJYlNc84555xLVy0dupZcDlxqZpMkjQHOzbOymdVJeg4YIGlzYCywh5m9Lmk66zNei72Vub8W8EOuzjnn3EYiH6GrelsBT8f7p+ZdWdLHgR5AE7A78HLszPWPjwv+I+l9Zvaf9jbYOeecc+VVrVenlst74dWfC9wm6VHghRzrTYvn5U0DzjKz54AHgE0k1QI/AWZl6icAtZmLIpxzzjnnOgXPcq0cf6Odc851JRW9XLR/v93K9nd2xconq+5S1/fCIVfnnHPOdXFS943dhI3KO3QVlDfDLzVjryHWH9Z3aFL9fasWAflzLFMzAiHkBObONk3M7YSQ3Zn6fobtN9E/R/tXNDXSt++I5PpVq+ZRU9M7vT2Na5J/XhB+ZnnbDzCspldS/cLGOgAeGpWWDXrQ7GkA9E78GayJn9Epu41Jqh/75HQABg4YnVS/dNkMIP93LDUL9bza8HrzZrPmzX49beCeyeVXLX2cv41LzzPeceJcBuTIJ17Whnzi4Tm+A/Mb1zAkx3e+Nub1pmaJ1rcxszr1e1b4jvXqOSCpvq5+GZA/mzV/9mu++vyfiXy/d13leIfOOeecc1Wvq1/l2uqrl7Q2JiQskjRfUvr/Qob1z5X03bY3sW0kfUvSm5K2yixrNQO2xHb6SLpXUl1MkJgmKW3YwDnnnHMVIXUr260apbT6DTMbZmZDgR8AF5Vjx5I6enTwRGAOcHRrhc2RtClwHzDBzHqZ2Qjga4R0ieJaH+10zjnn3EaRtxtanHt6pqQ5kmolnZdZ/iNJKyVNAfpllk+XdKGkR4BvSDogZrAulnStpA/EuuaWN8b1n5A0V9JwSQ/G0bPTMvvpBWwB/JjQscvaUdIDsX3nxPqfSfrvzPrnSvoOcDLwhJkVYsYwsyVmNjFTN0HSQ8D1Od9L55xzzpWJ1L1st2qUMqpUyFHdFNgO2B9A0kGEWKxRhEuTJ8VDkf8CTgB2idufT4jGKtjazPaNo1+rgQPMbJWk64GvSroKmFi8HPjfuP7fzGwPSb+KdXvFti0Froo1JwI3A48C/SR93Myej8+NAgYBrxOyWu8Dbonb/22s+QxwMPDN2P6WjAD2NrM3ip/wLFfnnHOuMqr1UGm55Dnk2p/Qybk+htofFG8LCJ2e/oQO3j7AXWb2upn9E5hUtL1b47/9CHmoq+Lj64DRLSwvKGxvMfCkmb1qZn8H3pS0dXzuBOAWM1sH3Akcl1l/spm9GDtgdxI6YwuAj0vaXtJQQlrEX4vfCEl3SVoi6c5se0p15iBkuZrZSDMbOX78+FIlzjnnnHPtluu8LzN7QtI2wMcIo3IXmdkGQ0+SvknLk+j+q1DazPOtTeZXyFBdx4Z5qusIKQ9DCB3LyaHfyfuBeuA3hZdRtL3C49uBY4H/IozYQRj1e6czaWZHSxoJ/KLE63HOOefcRlKth0rLJdf4ZMw37Q68CDwIfEHSFvG5T8Rc1BnA0ZI2k7Ql8OlmNrcCqJFUmLjoFOCRFpanOhE418xq4m174BOSCpPnHCjpI5I2A44CHovLbyGM7B1L6NwB3ATsJemIzPY3z9EW55xzzlWAn0PXusI5dBBGz041s7XAQ5J2Bp6II2GvAZ81s/mSbgUWEgLvHy21UTN7U9LnCTmsmxCuSL3KzN4qtTzHazoBOKRo2V1x+XPATOAGoDdwk5nNje1ZGjugT5vZs3HZG5IOBy6V9L9x/VeBn+Zoj3POOec6WLcufg6dZ7lWjr/RzjnnupKK5qHuMvSwsv2dXbDoPs9ydc4555yrtGo9VFou3qGroJrEjMDGmBF4ws5pOY23LJ8L5M84zJsp2L9feo7lipVPJmccQsg57N1rSHL9mrra3O3p2aNf64VRfcNKevUalFxfV7ckdxZt3tzLtmQopmZlFnIy8+Ze5s2N3LNn2s/g8fqVQP4849SfWV3dEgCO6DssqX7SqoVA+ncYwvc4bzZr3uzXHydm0QL8tHZa7s90jx59k+sbGlZRkyMXtLGpgR41fdK337gayJ+dmvq9r28In7n8v0drkuoLv9dT39OGhlVt2n7e72TejO6+OepXxTZVSlfv0HXtA87OOeecc+8B7erQSXotc/9QSaslfVLSaZI+F5ePk7R9K9vJnbGa0LZ7JD1RtGyipGNzbudgSbMlrYiZtrdK+mQ52+qcc8659vGrXMtA0gHA5cBBcULe7FWp44AlwDPl2Fdie7YGhgOvSephZg1t3M4gwus6wsyWx2VHADXAX4tqNzGzt9vTbuecc861jbpVZ0esXNp9yFXSPsA1wGFmVheXnSvpu3E0bCRwYxzd2kzSrpIel7QojnxtGTe1fcxYXS3p55ntHxSzW+dLui0z712jpPPi8sVxjryCY4A/s35uuayxkh6VtCpOSYKkJyUNzOxzuqQRwPeBCwudOQAzm2RmMzJ172TTtve9dM4551x1i3PdTo79mcmSPlyiZkdJ0yQtl7RU0jcyz50r6enYb1oo6dCU/ba3Q/cB4B7gKDNbUfykmd0OzAVONrNhwFpC9Nc3zGwoMBYoxGYNA44HBgPHxxe7DfBjYKyZDY/b+nZmFy/E5VcC380sL2S53hzvZ9UA+wKHAVfFTNlbCPmtSNoO2N7M5gEDaT3LdWsz29fMfln8hKTxkuZKmjthwoRWNuOcc865tuqm7mW7tdNZwFQz6wNMjY+LvQ18x8x2BnYHTpeUveLnVzF2dZiZ/SXp9bez0f8BHge+mFjfD3jWzOYAmNk/M4cpp5rZK2b2JrAM2InwIgcAj8XJjU+NywsKmarzCB01JG1LmDR4ZsyDfTseOi34k5mtM7PVhEiw/sCfWJ/3+hngtuKGS/po7CmvkpTtPN5aXFvgWa7OOedcZXSic+iOJOTQE/89qrjAzJ41s/nx/qvAcuAT7dlpezt06wgdoF0l/TChXjQ/wW42l3Ut4fw+AZMzvdQBZvbFEusU6iGM8n0YaJDUSOjoZQ+7vivL1cyeBl6MObDHs2GW6/BY9GIcZZwAbJFZ37NcnXPOufeQ7BG2eMszKrNtJnHqWeDjreyrBtgFeDKz+AxJtZKuLXXItpR2n0NnZq8DhwMnSyo1UvcqUDhPbgXhXLldASRtGeO9mjOLkKXaO9ZvLqm1SXxOBA4uZLkCI9iwQ3ecpG6SegE9gZVx+S3A94CtzGxxXPZz4Ecx4qzAs1ydc865TqacI3TZI2zxNmHDfWmKpCUlbkfma7O2AO4Avmlm/4yLrwR6EU5FexZ41yldpZTlKlcze0nSwcAMSS8UPT2RcK7aG8AehBGwyyVtRjh/bmwL2/27pHHAzZI+EBf/GFhVqj72cj9J6AgWttEg6Z+SCrPQrgQeAbYFTouHeAFuB34N/CSz7uJ4ouL18eKNFwlXt57TwtvhnHPOuQpreXyovMys2b6LpOckbWdmz8bz8p9vpu59hM7cjWZWOIUMM3suU3MNcG9KmzzLtXL8jXbOOdeVVDQPdc/dv1S2v7OPz/pdm9su6RLgRTO7WNJZwEfM7HtFNSKcX/eSmX2z6LntCodsJX0L2M3MimfseBeP/nLOOedc1SvD1anlcjHwp3ga2l+JF10qhCz8zswOBfYCTgEWx4s+AX4Yr2j9uaRhhIGgRuArKTv1Dl0F5c3YS83MK+TlnZOY63he7TSA5JzJq5Y+DsD5Q/ZPqgc4u/Zh+vXdNbl+5ao57J4jN3JWwyoO6zs0uf6+VYv48sA9kuuvWfoEA3LkUi5raside5ma1Qshrzdv+wEuHnJAUv1ZtVMBmDV676T63WfMBEh+j5Y1hbm9//nLtNNLPvSdewCSs0Eb4/Z79ti5lcqgviFMLfnQqLTvzEGzw3cm9TsG4Xv2t3HpP+MdJ87Nnc2aN/v19EF7JZf/ZsljjO09sPXCaMqapbl/R3xn8Ojk+l8ungHA3j37t1IZzKwPM2mdvHPa76Ebl88BoFfi7+m6+Hs6bzZr3nzi/ol/B1bEvwPDanol1S9srAPyZ7mmZoDD+hzwSuksEwub2YvAu375mtkzwKHx/kyaGcE0s1Pasl/PcnXOOeecq3LJHbrMPGwLJf1f0SzG7y+q/aakzTOPG2OaQ62kRySl/S9KWru+JelNSVtlluXOhpXUR9K9kuokzYszOCf972N8fdvkbbtzzjnnykPapGy3apTcoSvMwxbnYruKDWcx/ndR+Td59/Qe+5nZEGA64UrVcjkRmAMc3dYNxLSI+4AJZtbLzEYAXyNMa1JcW50/aeecc+49rBNNLLxRtOuQq6QDJC2Io2/XSvqApK8D2wPTJE0rsdoTxNmQJdVIWiHpd3H+lhsljZX0mEIG2qhYt29mNHBBnEKEOJfcFoQOYnHE144K2bArJZ0T638m6b8z7T9X0neAk4EnzGxS4TkzW2JmEzN1EyQ9RJjC5KOSHoptuZoKX8njnHPOOZfVng7dpoQ55o43s8GECyy+amaXAc8QRuRKneF7MHB35nFvwvxvQwgxXCcBexOyWQvpE98FTo+jg/uwPv+1kNn6KNBPUnY25lGEjtowwmTCIwmTBx+fqSnEfKVkto4AjjSzkwjz0M00s12ASYS5795FnuXqnHPOVYQfcm277kBDzEuFMJ9KS+ecTZP0PGEi4ZsyyxvMbLGZrSNEbU21MDneYmI+K/AYcGkc/ds6k/96AnBLXPdO1uexQogMe9HM3ojP7W1mC4CPS9pe0lDgZTP7a3FDJd0VRwzvzCyeFLdFfJ1/BDCz+4CXS71gz3J1zjnnKqObupftVo3a06HLm2G6H7ATodN2fmZ5NsN1XebxOuK0KmZ2MfAlYDNglqT+CrmrfYDJCpmtJ7DhYdd3ZbbGf28HjqWZzNa4v6OBccBHMusXv16fKNg555xznUJ7D7nWFHJWCRPkPRLvZ/Nb3xFHuL4JfE7SR4qfb46kXnEU72fAXMKh2ROBcwuZrWa2PfCJzBW0B0r6SIwYO4owygehE3cCoVN3e1x2EyEz9ojMblvKbJ1BOJyLpEOApOBc55xzznUMddukbLdq1J4O3ZvA54HbJC0mjKhdFZ+bANxf6qKIGGdxM3B6jn19Mx4CXUQ4f+5+QqfsrqK6u+JygJnADcBC4A4zmxv3v5TQ2Xy6EK0RO5qHA6dJqpf0BOFCi582057zgNGS5gMHEWaCds4559xG0tXPofMs18rxN9o551xXUtEZIMbuf27Z/s5Oefjcqpu9ojq7oc4555xzGdU6f1y5eIeugmpq0jLwGhtD/l3ejMBTBoxKqr9h2WyA5GzQQi7opUPHJtUDfHvRlOTXC+E1H9JncHL9/asXM7pXWm4nwIy65Xx9UFpOKcBlS2byvcH7Jtf/fPEjDMnxemsb66mp6d16YdTYuCZ3ewCuGHpgUv0ZiyYDsGfPfkn1j9evBPLnUt66S1p7jl8Q2pOa13vfqkVAei7lyph7eVz/EUn1t62YB8ARfYcl1QNMWrUwdx5wr16Dkuvr6pbkzmbNm/2aN185bxZtW7Jc8/6ey5v9mjfLtXdi/ZpYn/p7a0ZdyBsenvh7Yn7jGgB6JranPrYntb6wTup3HtZ/7yulWg+VlotnuTrnnHPOVblO052VtJYw91zBUcBNZrZnmbbfCIw0sxfKsT3nnHPOdR7VenVquXSmV/9GTILIeldnTlJ3M1tbmSY555xzrhr4IddOTNJr8d8xkqZJuglYLKm7pEskzZFUK+krmboZMelhmaSrJL3rNUq6W9I8SUsljc8sP1jSfEmLJE2Nyz4Yc2rnxOzWI+PygZJmx3zZWkl9KvKmOOecc84V6Uzd2c0kLYz3G2JaQ9YoYJCZNcRO2CtmtqukDwCPSXooUzcAaAIeAP4f6ycQLviCmb0UJx2eI+kOQuf2GmB03Edh4uMfAQ+b2RckbQ3MljQFOA34tZndKOn9hCi0DcR2jge4+uqr2/SmOOeccy5BFx+h60yvvtQh16zZZtYQ7x8EDJF0bHy8FSEG7N+xrh5A0s3A3ry7Q/d1SYUO445x3Y8BMwr7MLOXMvs6QtJ34+NNgU8CTwA/krQDcKeZrS5usJlNIEyyDGAXXnhxS6/fOeecc23k59BVj2yWqoCvmdmD2QJJY2g+wzVbMxbYw8xelzSd0ElTiXUL+zrGzFYWLV8u6UngMOBBSV8ys4dzvB7nnHPOubLo1OfQteBB4KuS3gcgqa+kD8bnRknqEc+dO54QAZa1FfBy7Mz1B3aPy58A9pXUI26zcMj1QeBrkhSX7xL/7QnUm9llwCRgSEe8UOecc861rqtHf1Vnq+F3QA0wP3a0/k6Y5gRCx+xiYDAwg3fnvT5AyGytBVYCswDM7O/xnLc7Y2fweeBA4CfA/wK1cV+NhNzX44HPSvoP8H/A+R3wOp1zzjmXwg+5dg5mtkVzy8xsOjA9s3wd8MN4e0ccRHvdzI4vsa2azMNDmmnD/cD9RcveAL5SovYi4KLSr8Y555xzrnJk9t7JjI/nx33XzA7fyE0p5b3zRjvnnHOtq2jA/aFHXF+2v7N/mfS5ira9HDrNCF05FI/kdTY9atKmqmtoDBfMDqvplVS/sLEOIDlLtDZmxebN/Fty6G5J9QCD/vIkBx7wk+T6yVP/h9eve9dAaLM2P/Vq/jZuZHL9jhPnJmdAQsiBTM35hJD1mfrzhfAzzptFe/GQA5Lrz6qdCsBpA9OCVq5a+jjQhtzIHmnZr/UN4Zqi1OzOQm7n2N4Dk+qnrFkK5M9LnrLbmKT6sU9OB+DknXdNqge4cfkcanbKkWfcVJ+c3wwhwzn1/YHwHuXNZs2b/br44N1br4sGPzCLWaPT85V3nxFOhz6m//Ck+jtWzAfy5/WOGX1mUv30GZcA+X+P5v07UJOYT9wY84nz5iv3Tdw+wKqmRmpyZLk2VjrLtYsfcq3WiyKcc84551zU4R06SdtKuklSfUxneCIzB1zFxGSHVXEy4cKy+ySdUKJ2jKRXMikQUyR9PD43TtIV8f5RkgZU7lU455xzriRtUr5bFerQDl28KvRuwoS9Pc1sBHACsEPi+u9KX2grM1sK3ElIfkDSUcD7zOyWon0WfpKPmtkwMxsCzAFOL7HZowipFM4555zbiKzbJmW7VaOOHqHbH/i3mV1VWGBmTWZ2uaQaSY/G7NT5kvaEd+e2xmXNZa9+MY66TZd0TWbk7GOS7oj5q3Mk7RVXOR84TtIwwtQmp8f6cyVNiPFh12dfQOyUbgm8XLR8T+AI4JI4kpd2wptzzjnnXJl1dDd0IDC/meeeBw40szdjsP3NQOEs93dyW+PjUtmrHwD+BxgOvAo8DCyK9b8GfmVmMyV9kjA58M5xMuHvEuanu7QormsEsLeZvRGvlt0nZst+lJBSscEUKWb2uKRJwL1mVhwtBniWq3POOVcx3cp2UK8qVXRcUdJvCNmq/ybEb10RR8vWAtnLr7K5rVA6e/W/gEcKmauSbstsYywwIM5LB/AhSVua2atm9mdJ/wB+W9S8SXHOuYJHC9OfSPo+8HPgtDyvtzjL9aILL8mzunPOOedSeYeuQy0Fjik8MLPTJW0DzAW+BTwHDCUc+n0zs947ua2tZK82p1usf6OZ59fFW9a/ShVGk4Ac1+8755xzzlVOR59D9zCwqaSvZpZtHv/dCng2pj6cAjTXtW4ue3U2IXv1w/FChmMy6zwEnFF4EEcB22NvoK7E8lcJ59c555xzbiOybt3LdqtGHTpCZ2YWryb9laTvETJX/wV8n3Bu3R2SjgOm0fwIWXPZq09LuhB4EngGWAa8Etf5OvCbuM4mhHPmch0uZf05dIrb/VKJmluAayR9HTjWzEp1+pxzzjnXwaq1I1YuHX4OnZk9S5iqpJQhmfs/iPXT2TC39S2ayV4FbjKzCXGE7i7CyBxm9gLwrjzXzDZrih6fW/R4OmFksNS6E4GJ8f5j+LQlzjnnnNvIqjrLVdIvCOfXbUrozH3DOu8L6qztcs455zpCRfNQDz5latn+zj5wwwGe5VpJZvbdjd2GPPJm+OWt/8HgMUn1Fy2eDsC4AWnZrBOXPQnAssPTs1AH3DubwYPSs0cXL5nK4/vsk1y/56OPMnHYgcn14xZO5vwh+yfXn137cHIOKoQs1NQMRQg5inv37J9cP7N+BTfkeL2nLJwMkLxOob6js1On7b5vUv1+sx4B4FN9BiXVP7h6SWxPWhZtY8yivXdk2mfi8LkPA/mzXFOzcSHk49bs1CO5vrGpIfdn+sdD9kuu/2nttNzZrHmzX9+697zk8g8cfg5AcibzDctmA3BIn8FJ9fevXgxAnz67JNWvXr0A6Ljs1FUxmzVvfWrWaiFndUCOz9yypoZOneVq3bp2mmnXfvXOOeecc+8BFenQdZY810x7DpE0V9JySSvioVvnnHPOVSm/yrWDZfJcrzOzk+KynQixWSnrdzeztWVszyDgCuAwM1sRL6gY38pq2fU3MbO3y9Ue55xzzrXfuu5d+6BjJV59Z8tz/R5wgZmtiG1528x+G9f5tKQnJS2QNEXStnH5BlmvkgZKmh0zXGtjdJlzzjnnNhLr1q1st/aQ9BFJkyWtjv9+uJm6RkmLY19ibt71i1WiQ5eS5zqcMM3IZZnnRgE/MrPCtCBfMLMRhLzXr0v6qKTtCXmuuwMHAtmzzAt5rrsSJh3+XVw+CJjXTHtmArub2S6EOea+l3luBHBkHGU8Dfi1mQ2L7Xmq1MYkjY+HdudOmDChVIlzzjnn3lvOAqaaWR9ganzcnP3MbJiZjcwsy7P+Oyp+levGznNtpXk7ALdK2g54P5Ddfzbr9QngR5J2AO40s9WlNuZZrs4551xldKKrXI8ExsT71xHm1v1+R69fiVe/FBheeGBmpwMHAB9jwzzXkYROVEFzea5DgQWk57kOi7dPmNmrsT0jmlnncuAKMxsMfCXu413tMbObCOcAvgE8KCl97gDnnHPOld26bt3KdmunbWOoQiFc4ePN1BnwUDydLHsuf+r6G6hEh66z5bleAvxQUt+4vJukb2f283S8f2pzL0hST6DezC4DJrFh4oVzzjnnqlj2lKl4G1/0/BRJS0rcjsyxm73iKWeHAKdLGt2eNlci+qtT5bmaWa2kbwI3S9qc0EO+L65zLnCbpKfjPpqbcfF44LOS/gP8H3B+rjfFOeecc2VlZbzKteiUqVLPj23uOUnPSdrOzJ6Np3A938w2non/Pi/pLsK1AzOApPWLVeQcus6W52pm9wL3llh+D3BPieXnFj2+CLiomfY455xzrsKsW6dJ65pEOMp3cfz3Xf0KSR8EupnZq/H+QawfHGp1/VKqOssVqirPtTO2yTnnnOsoFe1hjf7GvLL9nZ3x6xFtbrukjwJ/Aj4J/BU4zsxeijNz/M7MDo2nbt0VV9mEMDh1QUvrt7rfztn3eU+yvNmsfXoPTapfvWYRAJcMbXYEeANnLpoCwHcGpx2u/+XiGQAsGLtHUj3ALlOeYMDOeyfXL1s+k0kj0rNfj5g3Nfn1QnjNeeu/NzgtdxTg54sfoVeOjMO6NmS5puaOwvrs0ft3TVvnkDmhfmSPtOzRuQ0hCzVvlutDo9KyRA+aPS20K2cOZ2oWamNTuID9yqFpWbdfXTQ5V3sKbRqS+P4A1DbWJ+c3Q/g9kfodhvA9zls/a3T6d3j3GTPzZ7PmzH6F9DzdG5fPAeCY/sNbqQzuWBFm1+rVKy0/uK4u5Aenfu/rYq7pwMTP6NL4Ge2XmOW6so3Zr/0T6wFWNDXmzXKtaIdu72/PL1uHZualwzvNcF+qik9b4pxzzjlXbp3okOtG0SWzXGOb7pH0xMZsg3POOedcOXR4h056J8t1hpn1jGkPJxAm8U1Zv+wpuZK2JsyNt7WkkuPf8SIL55xzzlUB66ay3apRV8xyhTBf3Z8J8V4nZLY1UdKlkqYBP5PUS9IDcb+Pxjnwms18dc4559zGYd3Ld6tGlRiFSslyfVMh4P5mQmIEhPlYBmXiv74QrxLZDJgj6Q7gA4Qs1+HAq4RJjBfF+kKW60xJnwQeBHaOz50InEdIqbidDacg6QuMNbO1kqYS5q5bLWk34LeEDmoh89UkfYmQ+fqd4hcXO57jAa6++uqU98o555xzLreumOW6OdAbmBk7ZG9LGmRmS2LdbbEztwWwJ2Gi4cI2PhD/bSnz9R2e5eqcc85VRrUeKi2XSnTolpKJ5DKz0yVtA8xlwyzXbsCbmfWay3J9XdJ00rNc38gulPR54MNAQ+yofYhw2PXHRfvtBvzDzIaV2PblwKVmNim27dwW2uGcc865jlaRyzw7r66Y5XoicLCZ1ZhZDVC4SGMDZvZPQqfvuLi+JBUmhkvKfHXOOeecq4QO79DF1IajCB2vBkmzgesIWa6/BU6VNItwqLSlLNdNYi7rT8hkuQKFLNcpvDvLdaSkWknLCFmwNYSZl2dl2tcA/DOeI1fsZOCLkhYRRhoLobvnEg7FPgq8kOsNcc4551z5dS/jrQp1xSzXT5RoX2Eq8SeLljcAB5eoL5n56pxzzrmNpIsfcq366C/PcnXOOec6pYpepbD7+YvK9nd21tlDq+4Ki6qfPNfMvrux25CqZ2IGXn3M/MubSzm6186tVAYz6pa3qf6LA3ZvpXK93y+blZwpCCFX8Lj+I5Lrb1sxL3f9/r0HJNc/vGYZJ+w8svXC6JblcxnVIz2Hc3bDanr2SHv/AeoblidnUsL6XMqj+u2SVH/3ygUAydmjtTGbNTXXsTF+pk8ZMCqp/oZls4H8Wa4DEr8zy+J3ZmzvgUn1U9YsBcj/M86Re1nf1ESvnumf0br6ZbnzgFPffwg/g7yfubzbT81lhfXZrMn5rzH7Ne/v3T590r4zq1eH70ze7NSanRLzj5vCdyz1Mze7IWSAD69Jy2Oe3xjymPN+RlP/LsH6v00V08VH6Kq+Q+ecc845py7eoSvry5f0K0nfzDx+UNLvMo9/Kenb7dj+GEn3xvvjJP09pjWsjvvas43brZG0pMTyzSXdKGmxpCWSZsb56ZC0VtLCzK2mra/LOeecc649yj1C9zhwHPC/kroB2xDmeSvYE/hmGfd3q5mdASBpP+BOSfuZ2fIybf8bwHNmNjjuox/wn/jcG83MUeecc865ClO3rn2qerkHKB8jdNogRH4tAV6N88R9gBC9tXUcVVss6dq4HEkHNLP8YEkrJM0E/l9zOzazaYRUhvFxveZyWLeVdJekRfG2waiepJ6xHbsC27F+vjnMbGW84tY555xznYi6le9WjcrabDN7Bng7ZqfuCTxBmApkD0JG6yrgd8DxcdRrE+CrkjYFJjaz/Brg08A+hKivlswHCmcJTwC+ZmYjgO8S5rwDuIwQFzaUkAG7tLByHIG7A/i8mc0BrgW+L+kJST+NebMFm2UOt95VqjGSxkuaK2nuhAkTSpU455xzzrVbR1wUURil2xO4lDDv256ECX+fBt40s1Wx9jrgdGAa0FBi+fS4fDWApD8SR+CaoVjXUg7r/sDnAMxsLfCKpA8DHyPMLXeMmS2Nzy+U1BM4iDA1yhxJe8RDuq0eci3Ocr34ggtaKnfOOedcG3Wr0gmBy6UjOnSPEzpTgwmHXP8GfAf4J2EE7cAS67Q030ueg+K7AMtpOYe1Oa8Q2roXmVE7M3sNuJNwft464NC4D+ecc851Et2q9FBpuXTEy38MOBx4yczWmtlLwNaEw65/AGokFSbKOQV4BFjRwvIeknrF5Sc2t1NJ+xJG765pJYd1KvDVuLy7pMJFG/8mRJR9TtJJ8fm94ugdkt4PDACa2vSuOOecc851kI4YoVtMuLr1pqJlW5jZU5I+TzgUugkwB7jKzN5qYfl44D5JLwAzgUGZ7R4vaW9gc6CBcLi0MHp2MnClpB8D7wNuARYRrlydIOmLwFpC5+5ZADP7l6TDgcmS/gVsFbchQuf3PsI5ds4555zrRLr6Va5l79DF89I+VLRsXOb+VMKh0eL1mlv+AOsvdMgun0i4kKK5djSXw/occGSJVQbF5/8BZKcvv76Z7W/R3L6dc845V1ld/ZBr1We5VhF/o51zznUlFc1D3f/yBWX7O/vw13bxLFfXvLy5l3mzXPPmUo5JzHKdHrNcTxuYHsRx1dLH6dN7aOuF0eo1izii77Dk+kmrFubOjUx9vRBe8549+yXXP16/kpoc2bWNTY3JmYsQchfbkns5bsBuSfUTlz0J5P9M5P2Mnj9k/6T6s2sfBmBYTa9WKoOFjXWhPYnvaWPMseyo9wfCezQ4MbcTYHFTPT17pH/m6htW5v5M5M1+zZuXnJq9CyF/ty35xHmzWfNmv+bNWk393jfGLNdeie2vi+0fmPgdWxq/Y6k/45n1K4D0v0sQ/jbl/T1XSV19hM47dM4555yrel29Q1fuLNcdJN0Ts1XrJP06Xh1azn2cK+npOKHvEklHlGGbEyUdW2J5N0mXxf0sljRHUo/4XGNcVphcuE05ss4555xz7VW2Ebp4JeidwJVmdqSk7oRJdS8AzizXfqJfmdkvJO0MPCrp42a2LqGN3eNFG6mOB7YHhpjZOkk7AP/KPL+fmb2Qr+nOOeecKzcfoSuf/QkpEH+Ad652/RbwBUn/HUfuHpC0UtI5hZUkfVbS7DjKdXXsCCLpNUkXxLzVWZK2Ld5hnKLkbWAbSSfGEbMlkn6W2f5rks6X9CSwh6TPSaqN270hs7nRkh6XVJ8ZrdsOeLbQWTSzp8zs5TK+Z84555wrg27dynerRuVs9kBgXnZBnOD3r4SRwFGEueGGAcdJGhlH2I4H9oqpDmtjDcAHgVkxc3UG8OXiHUraDVhHmGfuZ4RO5TBgV0lHZbazxMx2A14GfgTsH7f7jczmtgP2JkyKfHFc9ifg07Gz+UtJxdOqTIvPPZnyBjnnnHPOdYRyXhQhSk/NUVg+2cxeBJB0J6Hz9DYwgpCRCrAZ8Hxc79/AvfH+PDaMDPuWpM8CrxI6hCOB6Wb297j9G4HRwN2ETmLhMqf9gdsLh0ljikXB3XEkbllhNDBOhNwvrrc/MFXScXHOPGjlkGucFHk8wNVXX91cmXPOOefaqbtPLFw2S4FjsgtirNaOhE5V8TtthM7edWb2gxLb+4+tnyRvbVFbf2Vmv8js56gW2vVm5ry55jqdAG9lm/5OI83eAu4H7pf0HCEebCoJzGwC4TxCALvwggtSVnPOOedcTtV6qLRcyvnypwKbS/ochAsQgF8S0hxeBw6U9BFJmxE6RY/FdY6V9PG4zkckpU+Ks96TwL6Ston7PZGQBVuqjZ+R9NHC/lraqKThkraP97sBQ/AsV+ecc851MmXr0MXRtKMJ58etBlYBbwI/jCUzgRuAhcAdZjbXzJYBPwYeklQLTCacy5Z3388CPwCmEfJa55vZPSXqlhKuun1E0iLg0lY2/XHgz5KWALWEQ8RX5G2fc8455zpWV78ooqwTC5vZ34BPFy+P58c9b2ZnlFjnVuDWEsu3yNy/Hbg93j+3mX3fBNzU0nbi4+uA64qWjSu1TsyRfaCZ/dWUWu6cc865yutepR2xcqlIlqukccDIUh26LqRrn63pnHOuq6loHuqxN8wv29/Z208Z7lmupZjZRMK5dF1a3gy/vJmCeTMOU7NE58fcy4EDRifVAyxdNiM5gxBCDuHBB1+eXP/AA19j91GnJNfPmn0DffoUzzrTvNWrF9C715Dk+jV1tfTOkYm4pqmJHjnqG5qa6NkjPUe0viFkraZmg9Y3rASgX2JO48qY0TiyR9pnaG5D+AzlzTPu1XNAUn1d/bK4/ZrE7TcC0LdvWlbpqlVhRqa8Wav9c+RermhqTP4OQ/gep/5OgfB7JW/9mNHpc8JPn3FJ7u9Yr16D0ttTtwQgeR+rVy8A8v8e7fDs15x5w6mZ0o/Xr4ztyZsZXpNUH9ZpzP17q5K6VV0XrLw8y9U555xzVa+rH3LN/fIlrc3kly6UdFZbdhyzULdpy7oJ266JFzIgaYykVyQtkLQ8m1LRju2Pk+QXRzjnnHOuU2jLCN0bMdWhmjxqZodL+iCwUNK9ZjavtZUkbWJmb1egfc4555xrh2q9OrVcyvby44jbeZLmx0zV/nH5FpL+EJfVSjqmxLrfjhmsSyR9My77oKT7YubqEknHx+UjJD0iaZ6kByVtl1m+SNITwOml2mhm/yKkTvSSNCxmxNZKukvSh+N2pku6UNIjwDck7RozXhcpZM5uGTe3vUI27WpJPy/X++icc865/Lp3K9+tPeKcupNj/2ByoX9RVNOv6GjnPzP9n3MlPZ157tCU/bal2ZsVNeL4zHMvmNlw4Ergu3HZ/wCvmNlgMxsCPFz0okYAnwd2A3YHvhwzUw8GnjGzoWY2CHhA0vuAy4FjzWwEcC1hXjmAPwBfN7M9mmt4nFB4d0KqxfXA92ObFgPZQ7Fbm9m+cV+3At+I2a9jgTdizTBC7Nhg4HhJO7b+1jnnnHPuPe4sYKqZ9SEEGrzr1DQzW2lmw+IRzxGEAIa7MiW/KjxvZn9J2Wm5D7neGf+dB/y/eH8scEKhwMxeLlpnb+CuOHpWyHndhzD/2y8k/Qy418welTQIGARMjnPbdQeelbQVoRNWSIe4ATgks499JC0A1gEXA08V1V8H3JapL8yL1w941szmxLb/M7YRwg/rlfh4GbAT8LfsC/MsV+ecc64yOtFFEUcCY+L964DpwPdbqD8AqDOzdl0WXO6rXAt5qNns1ZbyUwvPv4uZrYqjd4cCF0l6iNB7XVo8Cidp61b28aiZHZ6p36qlFwH8K6Ht2ezX4qzZwmvYIMv1Z57l6pxzznWITtSh2zYmWGFmzxbiTVtwAnBz0bIzYpTqXOA7JQbD3qUSL/8h4J0JhUscS54BHCVp83jRwtHAozFD9XUz+yPwC2A4sBL4mKQ94rbeJ2mgmf0DeEXS3nGbJ7fUoDiy9rKkfeKiUyid/bqCcK7crnF/W0ryqV6cc8659zBJ4yXNzdzGFz0/JXPuf/Z2ZM79vB84gg2PEl4J9CKc2vUs8MuUbbWlc7KZpIWZxw+YWUtTl/wU+E2cRmQtcB7rD81iZvMlTQRmx0W/M7MFkj4FXCJpHfAf4Ktm9m9JxwKXxVG2TYD/JZwT93ngWkmvAw8mvI5TgaskbQ7Ux/U3EPd3PHC5pM0I58+NTdi2c8455yqonFe5Fh1hK/V8s30BSc9J2i6Ozm0HPN/Crg4h5M8/l9n2O/clXQPcm9Lm3B06M+vezPKazP25xOPHZvYaofPUUv2lwKVFzz9IiY6ZmS0E3hVZEKchGZpZdG5cPp1w/LrUdnYvsXxM0eM5Jeomkkm+yB7Odc4551zlde88SRGTCP2ei+O/97RQeyJFh1sLncH48GhgScpOK5Ll6gDPcnXOOde1VLSLdcafy5flesWn257lGmfU+BPwSeCvwHFm9lI8lex3ZnZorNuccDFlz8JFlnH5DYTDrQY0Al/JdPCa5eeDVVBNYrZpY1NDrM+bEZhY3xjrc2f+pWf4NTY15c5yzbv9vLmUeTMIhyS+nwC1jfW5MxFTMx0h5Dr2zbH9VTGrNDVfdk3hZ5wzZzL/ZyjfdyD1M7Q01ufNM86b/dqjpk9SPUBD4+rkLFoIebS5czV79E1vT8Oq3HnDebNl837H8n6HgeTvQeE7kPdn3OmyXzvo9/Q7eckdnAfcFZnZi4QrV4uXP0O40LPw+HXgoyXq0oPKM7xD55xzzrmq14muct0oNvrLl7RpTGBYJGmppPPi8t0lPRknL14u6dw2bn+6pJVx+49J6leGNndYDq1zzjnnXF6dYYTuLWB/M3stJkHMlHQ/YTK+z5jZIkndCZP8ttXJZla47PgSwiXCLZLU3czWtmOfzjnnnHMVsdFH6Cx4LT58X7wZ8HHC/CuY2VozWwYgad9M7NiCODfcmDgSd7ukFZJuVIxzKDID6K3gkjhnzGKtz4kdI2mapJuAxZK6S/qF1ufQfi2zra+pKLfWOeeccxvHJt1Utls16gwjdMQRuHlAb+A3ZvakpF8BKyVNJ8SAXWdmbxIyYk83s8ckbQG8GTezCzAQeAZ4DNgLmFm0q08Tclv/H+EKkqHANsAcSTNizShgkJk1SPoq0APYxczelvSRzLZeMLPhkv47tulLZXo7nHPOOZeTn0PXCcQRuGHADsAoSYPM7HxgJCFp4iRCpw5CZ+1SSV8n5LG+HZfPNrOnzGwdsBCoyezixjgZ8l6EztfewM1xv88RUiJ2zWynId4fC1xV2IeZvZTZZja3Nruvd2Rnmp4wodn5CZ1zzjnn2qVTjNAVmNk/4ojcwcASM6sDrowzJf9d0kfN7GJJ9xEu/Z0lqTBbc0vZqifHyY4BaOZwbMG/MvdTslxL5rjG17NBluuFF1zUwm6dc84511adaGLhjWKjj9BJ+pikreP9zQijYiskHZbpePUhdJz+IamXmS02s58RQmvbcv7aDOD4eI7cxwjJE7NL1D0EnFbIby065Oqcc865TqJ7t/LdqlFnaPZ2wDRJtcAcYLKZ3QucQjiHbiFwA2GUbS3wzXgxwyJCtur9bdjnXUAtsAh4GPiemf1fibrfEWZ5ro37O6kN+3LOOeec61Ab/ZCrmdUSLmgoXn5CM/VfK7F4Opm8VjM7I3N/TIltGHBmvGWXF2/nbeDb8Zatq8ncfye31jnnnHMbR7WOrJWLZ7lWjr/RzjnnupKKntV2/vSFZfs7e/aYYVV3Rt5GH6HrSlIz8Ar5dz17pM2lXN+wEoCRPdJyOOc2hBzO1KzS2pj9mjfDL29uZGrGIYScw7y5mnmzZXfPkZM5q2FV7qzVvDmfedsPMDwxm3X+O9msNUn1jW3MycybzTo48TOxOOZe9ktsz8p32p83Xzlf3nDuz3TObNa82amje+2cXD+jbnnu71je70BbPtN5s1Dz/t7Nn2fc0dmvebNcaxLrG4H0vGco/J5O/5kVvjeuMrxD55xzzrmq19UPuXboy5f0o5jPWhuTHXbryP010wbPcnXOOefe47qrfLdq1GEjdJL2AA4HhpvZW7ED9P6E9TbJTBZcLp7l6pxzzrn3rI4coduOEI/1FoCZvWBmz0jaVdLjccRsdsxiHSfpNkl/Bh6S9EFJ10qaE/Naj4TQyYoZrHPiqN9X4nLPcnXOOee6sO7dVLZbNerIc+geAs6WtAqYAtwKPBH/Pd7M5kj6EGEuOYA9gCFm9pKkC4GHzewLcdLh2ZKmACcDr5jZrpI+ADwm6aG4vme5Ouecc12Un0PXQczsNWAEMB74O6Ej9xXgWTObE2v+mTm8OjmTlXoQcFacVHg6sCnwybj8c3H5k8BHCSkS4FmuzjnnnOuiOvQq13gO2nRguqTFwOk0Px9bcYbqMWa2MlsQD6N+zcweLFo+hirIcv3ZBRe0sFvnnHPOtVW1Hiotlw4boZPUT1J2EqNhwHJge0m7xpotCzmpRR4knKOmWLdLZvlXJb0vLu8r6YNtaJ5nuTrnnHPvIV09y7UjR+i2AC6P58C9DawhHH79Q1y+GeH8ubEl1v0J8L+EDFUBjYQrZn9HOLw5Py7/O3BUG9p2F+GcvUWEEbjvmdn/lbi44XdA39iO/wDXAFe0YX/OOeeccx2mwzp0ZjYP2LPEUy8AuxctmxhvhXXfIJxvV7zNdcAP4y1rOp7l6pxzznVZ3Vo8m+q9z7NcK8ffaOecc11JRXtYV89fXLa/s18ZPrjqeodVeqTYOeecc84VeJZrBeUPDk+rL4Qsp4bJz2pYBcCePdNS0B6vX5mrPYU21dTkCCZvrM8ffN7B288bfJ7684XwM84b3J63/QDDE4PG578TNJ43CDxf/ZDEn1ltYwgy75m4/fq4/VE90sLkZzesBtryejv2Z5b3O9Y/R/2KpsbkzwOEz0Te9vTNUb+qqTH3dwby/4wHJobJL41B8vl/L3bsd4aldyTVM/CYNm2/R03a+wnQ0Liamhyfocb4e6VS/CrXTk7S2pgDuySmSWzehm2Mk3RF0bJFkm4uX0udc845t7F09atcq6HZb5jZMDMbBPwbOK29G5S0M+G1j25u2pNmplNxzjnnnOt0qqFDl/UoIYv1I5LujvmqsyQNgTBPXKnlJZwE3ECYZ+6IwsKYB3uhpEeAb0gaIekRSfMkPShpu1j35Zgnu0jSHW0ZNXTOOedc+XT1LNeq6dDFEbNDCFms5wELzGwIYQqT62NZc8uLHU+IIrsZOLHoua3NbF/gMuBy4FgzGwFcCxSiHu40s13NbChhsuQvluElOuecc66NunqHrhoOK24Wc1ghjND9npDjegyAmT0s6aOStiJktJZa/o6YUvF3M2uS9BRwraQPm9nLseTW+G8/YBAwOQZWdAeejc8NkvRTYGvCBMobRJFl9jWeMJkyV199ddvfAeecc865FlRDh+4NMxuWXdBMFqtRes6b4nlpTgT6S2qMjz9E6AT+Lj4uZLkKWGpme5TY5kTgKDNbJGkczUwsXJzl+ssLLixV5pxzzrl2qtaLGcqlWl/+DOBkAEljgBfM7J8tLCcu6wYcBwwxs5qY+HAk7z7sCrAS+JikPeK675M0MD63JfBszJQ9ucyvzTnnnHM5deumst2qUTWM0JVyLvAHSbXA68CprSwvGA08bWZPZ5bNAAYULngoMLN/SzoWuCwett2EkC+7FPgfwmHfJsI5fVuW7ZU555xzzuXU6Tt0ZrZFiWUvEUbWUpdPZH1W7O5Fz60FCp25MUXPLSR0Aou3dyVwZeutd84551wlVOvFDOXiWa6V42+0c865rqSiPay761eU7e/sUT37V13vsFrPoXPOOeecc1GnP+T6XpI78y8xM6+Ql9fRmYW9c+RSrmlDFmre+rw5mXlzLMf02jm5fnrd8vxZtznbn7c9AGN7D2ylMpiyZikANYmfocb4Gcr7mc6bN5z6mWiI28/7Hcjb/vx5vXmzXPPVD6vplVy/sLEuORsXQj5uR38n82a/Qv584r179k+qn1m/Auj4bNa8Gd0dnf3a0Z+JSurqh1yreoROkkm6IfN4E0l/l3RvfHyEpLNybvNcSRcVLRsmaXkr63w3b/udc845Vx5+lWt1+xdhkt/NzOwN4EDgnStYzWwSMCnnNm8G7gd+kFl2AnBTO9vqnHPOuQ5y2Cf7VmdPrEyqeoQuuh84LN4/kdAhA0DSOElXxPvHSVoS81dnxGXdJf1C0uKY//o1M1sJ/EPSbpl9fAa4xTNcnXPOOdcZvRc6dLcAJ0jaFBjy/9s773A7qqqN/96EkgASWmgihB4pCaEoJXRQQECFIAQLooIo0iyoWKgqCh8dadKlg3TphIQOgVSa9CaICEikB9b3x9qTO2fOzJyZc0vuTfb7PPOcM3PW3rNnzpS1V3kXzg+Xh98CXwz1V7cP2/YElgVGhPqvF4TtF+FWOSStC/zHzJ4k1nCNiIiIiIiI6IXo8wqdmU0GhuDWub+XiN4NnCNpD7wuK8AWwKlmNj309UbYfjEwKlSW2IUOq99qku6UNAWvEFEacS5pT0njJY0//fTTy0QjIiIiIiIiItpGn1foAq4Bjiblbs3CzPYCfg18BpgoaWGcI6eJt8bMXgSeAzbG67xeGn46B/iRma0OHAoMKBuUmZ1uZmub2dp77rlnzUOKiIiIiIiIiKiGWUWhOws4zMymFAlIWt7M7jez3wKv44rdzcBekuYIMgulmlwEHAs8bWYvhW2xhmtEREREREREr8MsodCZ2UtmdnwLsaNC8sNUvH7rJOAvwAvAZEmTgF1T8pfhLtWLU9uSGq63AI931fgjIiIiIiIiIjqDPk1bUlDn9Q7gjvD9HEINVzPbIaeL6cCPw5Lt59/AnJltuTVczeyQeiOPiIiIiIiIiOg6xFquPYd4oiMiIiIiZifM1rxwPY1ZwuUaERERERERETE7o0+7XPsa6tZOrVvHsrvlu7suZXfXFKwrX3c8dY+37niWryH/dM3ao0+0WTeybv9V6wE/Ffqv+h8806Z81VqiSR3Ruv/ZKhVrywI8+vyztWvF1pWve03XrbVa93iH1uj/8fAf1P2Pu7vWatX7Mrkn694Dyw5ZsZL8s889CdQ/P5VrvwKsuiOrrrJRZfFHHh1Xve+ITiNa6CIiIiIiIiIi+jj6vEInySSdn1qfQ9K/JV3Xot1ikq4LZbwelVRGSoykISFDNu+3OySt3d4RRERERERERER0DrOCy/UdvILDQDN7D9gSeLlCu8OAWxK6E0nDunGMERERERERERHdhj5voQu4AfhS+D6aVMUISQtJukrSZEn3pRS3JYCEMDgpIYYcR0maGnjrds7uTNJASReHPi8BBnbXgUVEREREREREtMKsotBdDOwiaQAwDCf/TXAoMMHMhgEHAeeF7ScDZ0oaI+lXkpYM23cA1gCG47Vej5K0RGZ/PwDeDX3+Dlgrb1CxlmtERERERERET2CWUOiCdW0Ibp3LxsKNBM4PcrcDC0saZGY3AcsBZwBDgQmSBgf5i8zsYzP7FzAWWCfT50bAX1P7nlwwrljLNSIiIiIiIqLbMUsodAHXAEeTcrcG5BEbGoCZvWFmF5rZN4EHcUWtKhFiJAqOiIiIiIiI6BWYlRS6s4DDzGxKZvs44OsAkjYBXjeztyVtJmmesP1TwPJ4XddxwM6S+geL3UbAAyV9roa7eSMiIiIiIiIiZgpmhSxXAMzsJeD4nJ8OAc6WNBl4F9gtbF8LOEnSdFyx/YuZPShpPLAeMAm3wh1oZq9KGpLq85RUnxNpVvgiIiIiIiIiInoMsZZrzyGe6IiIiIiI2QmxlmsPYlZyuUZEREREREREzJaYZVyufQHXv/CPSnJfWnoloJ0ahNXqKD73/LNA/RqBVWsc+j7q142s23/VmojgdRGHLLNcjf6faaP/ITX6f44hQ2qM57lnateKhfp1Ketec3XrWHZ3nczurmfc3fV969ZOrXtN161F2931m9u7pus95+pec3Wvifr3QM3xD1mhmvxzTwH1x1+7NmvN2q8RPYdooYuIiIiIiIiI6OOYpRS6UOXhLklbp7Z9TdKNObLfCZUgJoeqEF9u0fc5kkblbN+kVd3YiIiIiIiIiIjuxCzlcjUzk7QXcJmkMUB/vJLDVomMJAGfAX4FrGlm/5U0HzB4Zow5IiIiIiIiIqKzmKUUOgAzmyrpWuDnwLx4qa+PJT0GjMEpSfYHpgH/C23+l3yXtAZwKjAP8DTwHTN7M70PSVsBxwGvAw939zFFRERERERERJRhlnK5pnAosCuwNfCnsG1l4DwzGwHcBfwLeFbS2ZK2S7U9D/h5qNM6BTg43XGoF3sGsB2wIbB40SBiLdeIiIiIiIiInsAsZ6EDMLN3JF0C/M/MPnAvK8+b2X3h94+DlW0dYHPgWElrAccCC5jZ2NDVucBlme6HAs+a2ZMAkv4K5BZqNbPTgUSTs6pZrhERERERERERdTCrWugAPglLgnfSP5rjATP7A7ALUCe/OpIER0RERERERPQazMoKXSEkLSlpzdSmNXAL3n+BNyVtGLZ/Exibaf44sKyk5cP66G4dbEREREREREREC8ySLtcKmBM4WtKSwPvAv4G9wm+7AadKmgd4Btg93dDM3pe0J3C9pNfxeLzVemzkEREREREREREZxFquPYd4oiMiIiIiZifEWq49iNnS5TqToLxF0veLfovy3S/fG8cU5aN8T8r3xjFF+VlGPqInYWZxmYkLMD7Kzzz53jimKB/le1K+N44pys/a8nHpniVa6CIiIiIiIiIi+jiiQhcRERERERER0ccRFbqZj7olJKJ818r3xD6ifJTvzfI9sY8oH+UjuhkxyzUiIiIiIiIioo8jWugiIiIiIiIiIvo4okIXEREREREREdHHERW6iF4FSX+ssm12hKSlZ/YYIiIiIiJ6J2IMXUSvgqSHzWzNzLbJZjYss22hsn7M7I3uGN/MRPrcSLrCzHbshn2sa2b3dZf87AhJ/YFzzewbNeT3NbNjW8j12D0gaVkze7bVtu6GpKXN7IUa8vH67EJIupaSqkdmtn0PDicig9m1lutMgaQdyn43s78VtNsAmGhm70j6BrAmcLyZPV+yr5HAimZ2tqTBwHxlD19JqwGrAANS4zkvIzON/JtZLm7zZ+R/XLS/0P8xKdkfAD8ElpM0OSX2KeDunOYPhbEIWBp4M3xfAHgBWLZs32GfXwJWpfGYD2vVLtV+SzO7JWf7/MBgM3s6s32YmU3ObNsh+d8lLWhmb5btMvV9uarjrIk/49cXku41s/W6WH4GJC0IrEjj+R9XIr8+MITUcyt7jWbk+wOLZeRzlQFJKwE/A5bJyG9WID83sGPOeJquHzP7WNJgSXOZ2YdF483IfxkoVejo5D0gaXngJTP7QNImwDDgPDN7K0f8CsL/nMLlwFpd0DfhGfVzmp9B2fN/FR3XW5VJTdvXZ2jzaZqvidxrtI1rqK78AOC7ND+zvpORq/zcDfLZ/zUr/3Bq9ejwuQOwOPDXsD4aeK6sn4juR1Toehbbhc9FgfWB28P6psAdQK5CB5wCDJc0HDgQOBM4D9g4T1jSwcDawMrA2cCc+I23QYn8JvjD9O/A1sBdYR8zYGafKj+8JiTyKwPrANeE9e2A7EPxQuAG4A/AL1Lbp+VZGsxs2TD2U4FrzOzvYX1rYItWAwvt5sHP/V+AUcADlY6qA2fiL9J0v18DjgNekzQn8G0zezD8fA7NL8Vf0/G/35bzexpW8L0Qkn5vZgeF77kKaLZJ6vuAQqn25ZNxfQ/YD1gKmAisC9wLFL3MzgeWD7Ifh81G5hpNye8DHAz8C/gkJT8sTx64DDgVOCPVfxmuBv6LK1UfVJB/Drhb0jXAO8nG7Ms1hbslnQRckpF/OPW9U/cArqStLWkF/Fq+Br8Pt0kEJA3FFYhBmQnp/JT/3y37zuCCcKxfAvYCdgP+nSNXd1LT1vUJM0I9dgYepfGaK5p01L2G6sqfDzwOfBE4DPg68FiOXN3n9P+FzwH4e2MSft6GAfcDIxNBMxsLIOlwM9so1ce1kgonYxE9hJldqmJ2XIDrgCVS60sAfyuRfzh8/hb4bnpbgfxE/IackNo2uUR+Ch5POSmsLwZcW+E4FsUVmqWBpUvkbgY+lVr/FHBjRmahsqWk74dytrUsQ5Ocj9TnfMDNOXLXFCzXAu8UnPslwvfP4Q/gHcL6hBz5CXnfC8b8MfA2MA2Ynvo+DXi77Nppdc2kZCYBCwILp74X/g915TPX3ADc8gwwFLikRP4xQohIlQV4Cli4hnzTddRCfmpN+YPzlhL5MTnL7V18DyTPlZ8B++Rdg8CX8Unhf8JnspwArN+ZvvOOgdRzChjb09dzpu0TwNzdeA3VlZ+QPkf4RD33mmhnAS4GVk+trwacUyD7GLBcan1Z4LGuGktc2luihW7mYIiZvZJa/xewUon8NEm/BL4BbBRcSXOWyH9oZibJACTN22I875nZJ5KmB3fha5TMfiVtj8/qlgyyy+A3+KoFTZYG0q6mD3FXVRqJ+wiaizpbyXhel/Rr3AJp+Dn6T9HYU3gvfL4racnQJs9FtWHo83+Z7cIVtiz6J/+tmT0gaVPgOklLkW9VGyhpBK5QDwjfZxy/NVpk+lc4rs5iEP5fJGNIu1vy/oe68gneN7P3JSFpbjN7XNLKJeOairt4XimRSeNF3IJWilQc2rWSfghcScriZsVxaPdIWt3MplQZjJkdWkUuJb9pDfF274GPJI3GrWGJ96DhuWJmVwNXS1rPzO6tMaaWfWflw+crIRTin7j1Novhkt7Gr7eBqe9huI1hH7R/fQI8E8ZcaoGtew114ppLztFbIUTmVZqfo+n9VHLRpjA0fT2b2VRJaxTIHgDcIemZsD4E+H7RWCJ6BlGhmzm4Q9JNwEX4Q2UXfAZehJ2BXXHr3Ksh2/GoEvlLJZ0GLCBpD+A7uFm/COMlLRBkHsKVlzL34+G4i+xWMxsRlJbRJfLnAw9IuhI/3q/S7M5tGfNWgNG4tePKsD6uxVgSXBeO+Sj8IW+46zWL+4B3Lbga0pD0RI78NEnLW4ifM7NXQgzRVeQrvK8Cx+R8J4xphgtS0jzAR2b2UVhfGXdhPWdmV5KPRUNMjVLfO3aQcfmZ2ZCCfnJRVz6Fl8L5vwq4RdKb+Eu8Aakg7E8Bj0p6gMaX3/YZ+eT4nsHvs+sz8lkXZzoODdyiNEOczAtf0pSwfQ5g9/BC+4COONJs8s5I3JJxXli/HLcOARxhZrdn5JfCJ3x3pY5nvvDzhWb2FM1I3wOJS7DKPbA77t78nZk9K2lZOmKikvGcGPokKGgNMLN92+07gyMkDQJ+ApyIu3QPyNlfrUlNO9dn6pjfBSZKuo3Gayh7zLWuoTbkE5we4k5/g3sJ5sO9NkWo6qJN8Jikv9A4MciVN7MbJa2IW9YBHjezKqEHEd2ImOU6kxDiUTYMq+NKXsiJhe1982DplfCb6IbkxV7QZkvgC/hD4yZrHTuVtBsCzG+Z4P2MzHgzW1vSJGBEsO49YGZ5FqukzVp0xGKMM7MJmd+HBitNbgyZNQbmdilCgPsAM2tp0anQ13DcFftUZvucwNfM7IJO9D0OV+qfDLFJD+CxR6sAD5rZL3LaHFzWZ9ZyJGkZ4K3kXARl/St4DNjJlgnqrytfcFwb45aUG3P637jF+BsU7RbHa1aQ9CJpgJm9X2HbMi3G05CoFJSBfczs0bA+Bfg2MC9wkJltlZG/CLjAzK4L60/gZZXmwS0oX8/I18qgzULSQDxcIm9ygqTdytqb2bk5bTo1pjLUndS0c322c8y9EZImhAn3ZDMbFp5BN1l50sUPgCQ2bhxwSvoeUJuJfRE9hJnl641L9QWf0c0DfBp3JV2JP/S7qn/hs7HfhvWlgc+VyN+Kzw5PxK2MxwP3tNhHf9xFmxtzB5wePivFDuExbEXxbddUOOZ58JnuGWF9RWDbHLl1a57LuvLrAIun1r+FB9yfQCbGB5iS+n44/kICmCv9WyevhfuBJcP3NYDXcavJucBfOiufaTsS2D18HwwsWyL7xyrbUr/tVGVb6remeKy8banfzq+47cHM+t9S3+9utU8aYyzvLBjLTcBcbfzX2+FxYs+m/r+W907FvmuNCQ85uY0Qm4gH5P86R24cnr0PsALwBv4cug04souvz3nxEIpkvT8wT4n83sACqfUFgR92ofxieILJDWF9FUJMdYH8A6lzthqwCPBMJ//Xs0uWs7ri2olLJ/6fmT2A2WkhBK/nLIVB7aFdEmC8D3Bg+D6x5n4SRXC5HPlTgJMJQa3hwfJgSf/zhofbHHiMzL6UBKCHcb8OPAJMxgPiC5M0Kp7LjcNyPJ4dt11YLgR+X6H9JXjGcPICGZh3TmkMwr63Qr+15QmKGz4z/idOh3E4cHlGNh0wfjfwldT6pIL+96DjBSjgLDy2bDJuXc3Kp/dxNPCn8L1f3n9WVz4lezCulP8jrC9JjoKTd17z9l1RPm/b4jj1xmPACDzLeE086/vxqv2H++HRHLknS/p4Kmfbo5n1hYp+S20/DXgQn6D8OFkqXHsP4ZbRCaltUzIyi4T/al98EncKHs94NbBCSd+1xgSMxWNS02NpSjyh5qSm3eszyNyH0z0l6/NRMnEl//kxoQvlbwC+Rkfy2hx5x5yS/x7+LN8YD0F4Dfh+3jnFnwe5S6vrKC69Z4kxdD0Iq0/7kUCS1sNjIL4btpXFkhyDKwYX4i/xXfAX1xP4C32TjPznzWxNSRPCON+UNFdR52b2Tmq1ivthP2BlM2sZqC3pWwX7zMbcdTZ9fnkz2zmJCzKz9yRlkzGg+2k8+ltHEPTOuKXyCuAKSRMzspMlHY3/tyvg2cOEWLQi7IfTpYDHVQ3HY3RG4FbADTPy6fFvBvwSwNytntd/XfkEXw1jeDjI/1NS0/2hcn7Ce3Lkt8ZdcJ+WdELqp/nxzOAsvoi7QJeiMX5xGnBQTv+/DNuTgHzwc/Ah7hrN4nFJXzKz6zP9bIvfj1lMk7SSmf0DOgLk5fQh2cScBP8MSz/qUVZMN7P/Zv4ny8hcCIzHLdgP4JaY4/Hr5i80P0vaHdM85klEDePLkUuPbzNCLLGZfSjpkxz5dq9P8DCMGefczP4XXL5F6CdJZq4lBddz4XO0DflFzOzScA1iZtMlNdGdSHoUD8W42JzXcizlyR/blvyWixDveDAd7tmxwGHWBWErEe0jKnR9A/vhD6IrzewRSctRnkSxlZl9PrV+uqT7zOwwSU0vKTwjrT8dwc+D6eDuaoIaCYbnwjPB3rHmDLMElTIOA9ZJfR8AbI6/9HP5xoDBkpYzs2fC2JbF3Xet8GGIH0qOeXnys9n6hUDkfqnv6SzUbEZaXfn+kuYws+nhWPdM/Za9P/fAr4WlgS+Y2bth+yp0EH5mMd06Yi23xcld/wPcKulPOfK3S7oUT9BYkMCVKGkJGjOVs/KvVJRPUDUTuxY/Ia5EjAe2xy1QM+TJD7I/FzhX0o5BkS6Fmf0B+IOkP5jZL1vJh31eL2kUHRmWa+E8lHkv0oPxhJ3fZeQPwv/7vDHVyqBNYaqkXfFrcEXcCpdVkhczs4PCZOd5M0uSsR6XtHdRx22M6fVwDybXwyjyM5qTSc3LVJvUtHt9ArwjaU0L8bshDvi9Evmb8YS0U8Nx7AXc2IXy70hamI5ztC75z9XR+CT+Zkmv42Exl1gjs8IMWCruU9JidDyDHzCz1wrGchZuqf1aWP8mruyXxthFdC9iUsQsCEn34izzl4dNo3B3x7qSJprZGhn5r+PWoTVxi9soPH7lsor7+woec5enLCLpTJxcuFXGYV7bQXhsUm5JGUlb4ZaRhvR5M7upRb9b4qS+q+AP1g1wEuA7MnLP4cpt3nTezCybBVlX/le4Rel1XFFbMyg6K+CB5U1k0JL2M7PjW20L2x/GyVrfBJ4HNjOzR8Jvj5nZZzPywq+FxYHLzOzlsH0EsGj2vKbklwAubSWfavdT3OqzJa6sfQfP4jwxTz7VblEaKRheKJCb00qShgra1KocooqVLuRJN1+nI8v5EfxY38/KBvnV8HCARH4qcJSZTc3I1cqgzdnPPMCv8OQp8Li3I6wxCD5dbq6hLF92PdP34NQxlFV+SOSXw+/j9fFr9Vng69acZDIQV2yXwGO2JoXt6+NW9/Mz8m1dn0FmbTw0I8m+XgLY2cweKpAXTt2xBX7/34zH6eWSBrchvyYeL7gafk0MBkZZeQLbuuH4d8S5GS8ys1zGAzkp+lE4yb1wK+zPzOzyHNm890jTtogeRmd9tnHp/gW/cY/Cqzjcniwl8svh8Umv42zr1+Kz2YHAyIxsP/whOhQP0v0R8Nk2xnhfyW8H5y0V+52TFoSVwNy4K3F4+L5YC/l++MxyYVzZ2RZ3Z8yM/3ZZnALmq8C8qe0r4cpdXpu8WLAJBbLb4taMVwkJIGH7xsD1BW3645Q03XXMAj6DK3NH4dbFLVu02Q54Eq+a8CyuND9SIr8iPqF5FFf2n6EkIBxn7D8PtyYfjMcVnVki/70g8yZuLX+vxT15ALBUjXPUFN+YI3MbsEpqfQpuzduIDHF3J/6rt+gg0k6+J+tvlrS7GQ8PeSxca2dRkMQSrrejwvd5SZGQl/S/Vt410oXXaP/wn82JK1CrA3OWyPejBtl0G/LJeObAleTVysaT034TYALwQYnMJFzJTdYHUxybey+pdwk+IW4ZMxyX7l1m+gDiUuFPqvFwbLP/WjciblZPllHAkVX6wGNp5mshk85evS68iJuy13LaDcKtPLcCL1eQH1fxWJcBBqXWN8VjiA4gJ4uvDfmEIf+2CmMZnbxIaczqHUOJAoa7xTfMbJu37L8I/Q5qNabMNfEk7gKqkuhTlyV/Eq6AT0id19NL5O/CXdiTw39yCHBoiXylyiEp+bqVLg7GLXN34hOnVpOOMTiH2OHAqgUytTJoc9rfQnOW5U0ZmY3Lllb/Ly0qP6R+q1XxAHdHp6sajAbu76rrM7S5o+aYLqCkYk4XyNcdzzp4XOjzeIzbDyiZuNKcENMvuy312/BwTz4X+p8ADK8zvrh0/RJj6PoGFjazM4NbbSwwVtLYImHVZwi/WdKO+AvBKoxnu9T36fhN/eWS8ayGk1wuFNZfB75lwfWXQToWbDoet/NSQb8D8VipXXF38adwjqkqSRG3BLdftlZmNi7rUtx69l85a/pluItwDbzw9/c6Kd9Pzp22knKKalujW/oePBZoETrqL4K/nArdLuYB438C1ktte6dIPuB9YIqkW2g8P0VEsn/CLSRlxKVp3CdpHeuoc9sKH5nZfyT1k9TPzMbIa20WYaCZ3RaCzp8HDpF0J65Y5aFq5ZAEtSpdmMeUHSppGO4CGyvpJTPLrblqZptKWhy3JJ8ur+ByiZkdkRJbINMmHb+0WMnYEyxiZm+l2r8ZXNrpPseqPV65qpUfEkyQ17m9jMbrrYjXbBRweQgXGYnT/XyhQBbqX59QoZ5uBksAj8jJr9PyueEibchXGo+k3+PX2Jt4Oa8Nip6hGdyoDsJ7Qh9/zxM0d3UPD9clZvZ2nlxEzyIqdH0DdR+OdRnCf4xbbKZLeh9msN7nJjmY2e71hs/peAzfGABJm+BVKdbP6XuGoippEQpKGEm6AHct3QychLuhn7JMDFwJEuU2HdhtNGeDDTSzJIbmG3jczv9J6ofXbc2irvwuuBI6By2yAYNi8jxBMQsP0+Qenh/n5CpCXaX9+rBUxb9qviw3Bb4v6Xn85ZRcc8MK5N+SNB+urF8g6TXysyATvB/O+ZOSfoS7nRctkc+rHFJWXaVSpYscvIa7v//TYjyY2avACZLG4PFovwXSCl3dDNosPpG0tIU4RDkJb9O1YU5oPljSXFaBKDqgUuWHFBbCz0k6xs6AXIXOzJ6RtAt+/l/Ek4TKEhbqXp/Q8XxKx1FaZoxp1E0EqStfdTwfAFtbyJSuCjP7mZw4eCR+P55uGbJmSdvhVtcktnF/YMdwH+9nZs/W2WdE1yImRfQBhAf0nXjcUfJwPNTMrimQn2A1GMJrjuXL+MslCaYfj6er3yVpkOWkrUuaZGbDy7aF4N0jcaXkcFwpXQQ3+3/LzG7MtscfOufhlosXJT1jmaSDmsfW9MKSNMXMVg/fHwZ+aSGQOjm/nZFPtdvazG6oOM498XP0Hh0JGFZ27PLM5HlxJail0l4Xko7HEymuojHxJfeFrIKKC5YJgk/Jz4sfbz98gjIIJ9cuUvjXwScxC+DnahDOQXZfhWOpVTlEJZUuUjI/wC0eg/HYvkssVI8okP9skN8Jj4W9GLjCUlmH8sSZ63HLbVMGbasXujoSipJJ1EbAnpaTKCAvJbgm7opPW4daJja1izwLrjpKryVYFHejfhDGk70fE6vlxtS4Pjsx5qpZom3J57U3s38V/LY3fo+8FdYXBEab2Z9b9LkInhDxgmUSQOTUQeua2bvhvXQM7u4egRN3f7HO+CO6FlGhmwWhUIZLzsf2Q9wi8EAVZUdOHbALfuOvlvnth7hl60BckQNYG7caHI+XMmpQ3EK7K/EXTpKB9g1gbTP7SkpmPE7NMAh/yWxtZvfJ+bcuMrMROf0Oxd2tO+OWj6F4XM2rrY4z1Ydwa9GuuEtmsczvx+OukVdw9+5KZvaRnPbgWjNbuzPyqXZz45loQ0hZzi0ny1LSk8B6ZvZ61eOsipwXZgNKFNKz88UL3fzptvPiVspdzexLFeQXAf5T0dLYEkG5fMfMXg8Ti5G4tfeqCm3nwTOlnzezf5fIHYnzgk2sOKb7cNfXZSmLb55crQzanPaL4Ek5wuNgc68pFZRUs+bScX/Ck09OzWw/AK+I8vMW41mF8PwB/ptzf+VOBFLjyWbF5l2XKfHi61M1udZUI0u0HfnMuHbEn1mfNbNPF8hNtOZM1AnZZ6mk64BfmNnU8Jx6GH++L4cnUh2Xkp0xEZd0FvCEmf0xrBdmPUf0DKJC14uhVGHsPFhBPJOk7wFX4JlZ5+AB3r8xs9MK5Jeg4yE6DI/5+puZTcnIPYbHY7yR2b4w8BLuVj0lp/8FcffCjFquuIXxzZTMjIePMnQaeQ+hnH2sjT/gRgEvmVmTOzcj//kg/1Xc3bM3XvbozYxcLdqDuvKpdjfiloaHgBm0BWb2fwWyO1gHD13ZcZY+YK05/qbWC7NdyImrt8H/g63w6/VvZnZtRq6u5XYR/L98E08eOgp/UT4N/MSaa+z+BicWNtwKtgX+gv08nuG3f0Z+e5yQ+Q2c9uZk4F+4Iv5zK6nzGf6LkWFfd2fPfcE5Ghrknyix/h2AK35V4qSybUupVyR9zcwurdHfo8BqZvZJZntSmWG1nDbL4M+e0bgFeRl8wvdcyX7WxbOcp4X1T+EZv/dXHWuFY7kCpwdJ/tNv4oH/uVxrwWuwZWJlk1O33Jo3ya0rr5J44ey5TrWZHMZrYb0//h+smpF7JNkm5ykdambfCuf07vQkLvS5PvAunm2+o5mND789amar5I0loodgvSAzIy75C15Wq3ApaddUE7Ng2x547Nk/cCvbMEJdx4J+C+lDyCmThL8kBudsXwx3aaW3PZz3PW+9xTkT5dl3v8Oz3W7DExQWLjvmHvyv61AYjMDj8U7DlYsTgBMKZMeULHk1cuvWok1K0Z2YHkvRmHCqkrPwmLa/4gk2z5X0Px4Pdt8JV9LWDduHkkPVgsdU/j6M51HgZ0F2D3KyBIPMXLhr9m1CrU7cSppXemoSTimzDl65YbmwfVHKyzD9Bs+MPTQsk8ipVZqS3waPDbsDtwy9gFut82QPpkYGbapdS+oVPNP8RnJKBhb0WUYl0/Qb7i5+JJyfpETdsxX2M4FgkAjr/SivvXsuzRm9pbVHyS/N1bQt9VvlLNE68ng27It4HdctcQqTKufoKDzJZHM8zu5S4P/Kjgl/Lu5SdLy4h+Yp3Ip3Y2r7CCpk6sele5eYFNG7cQnOydTgypFnopVlFV2Bz+LSuByPr0njZJxPaFfrmGWVmWzfljTcAplnajzDyWcsPwF/GWTjVLbALRU/SG0bLi+lJJrLKjWV0GplvaQjLiiLPfGA8VOA68wzFVuaqUMszh/xF7doEYNWVx64R9LqlrGKFuA0XBGfQklFD3yHm1boL40/E64dSfea2Xot5JM4sPGlUh24CVc8RloIoA5u6iLMYWZJNYDDLMTAmWeV5snXrWzwvrnl60NJT1uwepqXVcqziH1iITZN0rMWKpSY2WuSypI0dsW55d4PbY/EX4pHFMgfA2xqwaIYQiGux6tmNMBqZtCmsB+umN5nnlU7lEygvpltKycOv17Shfh980nq92wizruSVjSzJ9Mb5ZUo8pIW/o0neC2Gxxc+Sfl9PaNLM5shZ17Kq+x9NsyaM3pLrf7Ae5JGmtld4Rg2KDiGBJWzRGvKr4Yr3Y/hE+ePqzyzgJ/jz7sfQAdxcY7ci5L2wb0saxKqVQSr4JxpQTM7S575vixODZTgVaBuslxEFyMqdL0bRQrRljQrRElM2arAIHUEA4MnUeTVFV0St3wcIw/OvZTMDZzBT4BrQlzKQ/iDdx3cYphHaTDSzPbMbjSzC5QpQWZmZbVp85AoEBvgMUyXhPWdaCz5lMXiuMVnNHCcPINwoDrKbxWhLu1BXfmRwLclPYsHbZdlfU43syaKkzxI+r2FCh6StjSzW1o1SX2vUot2FK4YnytpNytxOQashbv3b5X0DO7mLPvv0wpr9mWa91L7GPzEyelxivpKsEC4VwTMn7pvhMdzZpEu7faJGku79Ss+DJ7Dz2cS2zY37gYuwmvW6B5OiquXoXIGbUAl6hUzuypcl+NwOqTkvBvNWeG/BW6QdAQd9+HaeOnC/XP6/nIqJuxQeaLHApI+Z2YPlIz9GUn74gomeKzwMyXy/SQtaCGkQtJCtH7/7QWcF8YHrlTtViRsFbJE25E3s+HqiBe+VZ7h/SlJi1tJvLC5K/ZU4NRwvEtZfhWK7+KZs1vglTDeCtvXxct5Zft9UdJVZrZWaltuWbGInkWMoevFKItJSMc9pLZ9GY+r2B7PRkswDQ/Ibipmnmq7FB1xdPPgdWPzipMvjj88V8UfQo8AJ+c9WJRTWqrst7I4m5Jxj8EpCz4K63PihLAtLVNyvr5t8WMeibsMdi2QvdtyynCV9F1XPjd2zXJi1uR1Pp/HSYbTGXtNtCUqKd1UMI5JOKt8P9wKuAkpJS+7j7r9Z9pugJ/7HXEX8pVmdnpG5mM6aE0G4rE7hPUBZjZnRv4tXPFIgszHpeRHmtmCGfmyoHksQ9ETFBujUfFNiTeVdkssyUvjk59bwvqWwF1mtktGPlEot8RjyS4N8jvhcXQ/ye5UNTNoU+2uxK0q++MuuTfx6gPbpGTmxmMFR+EB+9dV6Hc13NWd3MdTgaOrWJ+D92Fn/Lr4jJl9pkTuhDBuw12F+1tBlqikb+FK5eVB/mvA7yxTKiwlPwJYHn++vQzFXGvB+nh0kJ8C/NRC7GxXyOe0rxQvLOkO/F0wB35//Rsndy6dDIbYOTOz/5XInAycY9V5JCN6AFGh68WoqxClflvPzO6tsZ+5zeyD1PrKeF3TKsXHy/odi78EHshsXweP5dgop80FONXHCxX38QSe8flGWF8QdyEVkrwGxXGUpYK95ZxuXy2yMKk+LUcl+TBzTsOAt6zkxgxKRRZNykSQravQPUe9WrRtK3SpPvrhCswuWQWqjb42LvvdUjyHbfY/0pyiZ4BVyCSVVGjR8eF4HdaUfJmCaZaTlamaGbR5UAH1Sri/rgAOt3Ket6J+5ytTDHLk57VAei1pmbwJTRtj6Idbm97CFUDhk7dcpVfSb3GPw0N4cswfrKD+aZC/E6dPGofHhK5vBYkT7ciX9CNgo6JrWh30Vd/DleODVU6dlCaAF64A5hLAy5NfVsInllV4JCN6AFGh68VoRyEKvw/Gg8CH0EiBkZuin/ciLthWRGmRezNL+hxuYTiHRvfLt/CXd1NGmqTbcUtGJfZ0SbvjZZ3GhE0bA4cUKWapduOKzl+BfN6LNvcFW0e+wOIzHx40/z0ryfSrAkkv4fFYwoldG3jDrJM8YsH9c3Hof+fwPd1/USb2NUH2aiupWpGj8DYgx2J4m5ltLumP1oIiI8iXWiuy50fSQ2a2VrvKa6qfz+D3wFEthTvaFFbWUI0M2jrnVNIqacUnrXC1GOt6eBD/fGa2tDzO9vtm9sMC+fXx+K6q8ivh7tbFzGw1efzg9tZYSSMtXyUeNJF9BFjHnGttYVzJXadEfqKl6EFaXRttyLfLdjAFDy85F/iVmT3YQqG7J8iNCeubAL9PWwAl/R330OSOpysU8Ij2EWPoejd+Blwq6RxyFKKSdlfjgee3kqLAyELuPv00HkM2gg6lYn7c7ZrFtnUGb2YPyOlBfohTQ4C7MD5f5BqhJnu6mZ0t6QZ8Jm04n1IVHrqqpb+S7bUsR1XlzWzZvO3B9XYqTumR/W0n/CUzTdKv8UDmw81sQk5XZ9BRgSL9HXIeyqpJc4JfowmqJkaAly7bGfiDvPTRJYQklYxcEquZazGkOYZriWBt2l5SomiWjb+0OkcOPgrK+lKSTmgaUHFpNOSUKjvh7sRPA4XxVak2Dbxs+P2flfkN7kJMrL9nS7qsSLmhxjlNlLm0wgW0VLiA4/BKNdeEfiZJKptAHVtT/gz82jstyE+WJ20UHXOdSinvW0dyzH+Cha8MAzLPz4bnac41V1e+zn2VxmF4EtJdQZlbDk86KcK8iTIXxnGHnCMyjXPw5IpzcaLuj4joNYgWul4OeazI3jTGo5xcohA1zQBL5HbDFa21aXxoTMPjI7qURb27IOcGm0H+aRkus4I2ldyWkg40sz8VzZKzL/C68i3GmDtzV0cFkJE4Z+DROKnz50v62sDM7q6wLXmgD8Cvi0n4i2YYXvx8JF0IOTfWZrhFeSvrZOUKSaPwIO+RNL8IzTpZLSUoZVvgGcy/zf6etQzL45G+isc8rYQrcTubWWHpPtXkZZPzQ6YzaAfiFB65IRntQNL9eMzWNRY4ISVNtYJ4V0n3m9nnleKQVE7FmE7IP2hm62TkC5976qiU8jEdiSmWd72pIw4TaIrFbPIWpO6ZPDRdc3Xlc8bXMsatHagCAXyQmxe/9rcKsums526rHBLRGtFC18sRFLeD5SSjn8VvnrdaNLtO0jZmVpYyn7x8zpW0o5ld0Wos4aFY5nKdPyNfy0Ub2qyLc4h9FucH64+z+BfRgxyJu2gvCJv2lbS+tYj/K7KM5aAuLUdd+VzI65YWWQYSq+uXgFPM7GpJh7To8kSaqWyatllIJgnWrT0tBLLL42t+mjPOayl3B+W6ykPbgXgM0c5hHE1u8roWQ3OW/csl/cbMDi9rG/pvsrJl+ts3s/46cLE8hnVSQbM0XsPDB36NW0pM0ldLxnMPHst2MR7n+aScHuW5kn08R70M2vT+kixLA+60kuoY5tmN6U2F1n+cCmN9wMKza1/K60nXlX9dTuVi4ThG4dVZisZexxL75cz60WXCVpMaqK58AmVi3CTlxrh1YlL5HdxD8jf8GT2OfCqSj3Cvxty4hbuUOimi5xAVuj4ASdvgroWn8RttWUnft+Lan/sBB8l5tBKTeO5sNOA2ScfQosRNzYci1HTRBpyEu5guo8O9vGKJ/DbAGhbY0iWdi5OOtkzoCC+QITTGGZ6XEatLy1FLXvkxXAvi2WknFTR7WV5bcwvgj/JMxFzlTx7LtD4wOLOv+SmnCxlqqaxE87JAa+TIlb7siiDpEtxNfiPOh3iH5TPeN1XKSMHIFCZPKYDX5ymDOe6sMoqbJqRfksrhwct5WR6EX8+nABeG4y5DZV621Fg+AB6R84PNyKCtcCx/BlaggwdtLzm1TR5fX12Fay+8HOCncX6zm3FPQ1fJ742XCBwq6WW8asHXS+Szlvw7rCBj1zqROFPxmdKu/Ol4NZ50jNsZ+P2dRvK/1JpUmlO6lHoQ5PV/j8Fd42tahWo1ET2H6HLtA5D0OF5su4Fk1MyGdlH/tUrcpNotSmPJoEqZqS36HG9maysVvCvpHitOzZ8MbGIdWa4L4Q/r0mwrSefjtAET6bA0WI4LtW6WaF35gzObDOcRG2cFNA/y+qFb4azyT8pLt61ugYA3I7sxTj2yFx6Tl2AaXls2N6ZG0kX4LPyvYUzfwAPWR5cdT1WEF8Mtls+L1Zl+O+XOqtB/WdZqk8s11W453IW6Cz5BORinaflHjmzCyzYaV7gWAL5ozclRtTJoc/bzCF6mK1FQk0oFq+bILoIrXFvADJLa/czsP2X76G4E918/CyXASuSylvzRwENm9osc2XbrGVd6pnRCvskFXeaWrgp5glIh0lZ2eYbuXlmrYETvQFTo+gCUyciUmwbGWkmWZtXZaJBtij1pEY+yPW45WRJ3KS2DlwXL8uLVctGGNuPwl8ZfcJLUV3AKlaJYmtF4rc8xod+NcNqTi/PkU+0ew2s/lt4A3a3QpdrtZGaXtdqW+b1SYfggu4yFDLTw4p7PCni1gswAnLg6uYbG4e7dXKoOObfWH8J40kp+NiZxMzO7XY3E16Tki2hgvlUgX6q0VEVQBPPcU51SAAv2tTquUOxsZsu3kF0Md0nvQgkvW6ZNpQxaSX8DDkhdF8sAR3aF0l7gyv4vMN7Mru6MvJxWaU+8pBu4Rer0POU41WYyjZb8/njpuLywj4QTMrEQJjFlXwfeNbPDCvZR6ZnSCfmqMW6VFbQg/2+8tNhFwP3QlEjUKaqfiJ5DVOh6MdQGyWhoV3k2GuTvxelR0iVujraCNH85+exmeCHpEZI2BUZbTlWIuggP03/h8XMH4PFEf7ZMUfVMmyXw4xUeuN8yy1XSZcC+1oLhXDVpOerKp9q1pI5R5wrDX4hb6T7G3YyDgGNavfSrQtJduNXpWDwubnf8+XJwRu5Qcz6sujQwJ6ZWB+D1KR82s1ElY1qNZgUzVwGUlC6LNwC3kE03swML5AfjpZWy/bcKaJ+fRvdablZ1QdsZSnnOb00ZtGbWFPOYaTOWDoogwvd7CeTNGctMXQXtdFzhSiYkO+IZ7p8BnjGz/duRl4cQ/A0PQZmA32cj8KSaHSyUhssZT21LvnLIwfO2pX6r9EzphPyCeIxbUlliHE7R9GZGrpaCFpTbLfFrZxheYu6iaIXre4gKXS9GwUsvQdnLr/JsNPw+HCe6HBQ2vQnsZmaTC+QTt+gkPLvuE0kPmNnnWhxPJRetPFh+aTN7oqy/lPwwmuNQSjN0g0VmDfxllib+zc5ea7nY2pDfGo8D/Bod5cvAY9xWSZ/TcL53wv+nMXh9ymfCeb3NzFYv2m9icZX0dbz81s9xJb/omtgA5/dbhsbz2kReHOQTfrYpyTgk3WlmGxaNKaePSsk5QXYQcH72/0r9fjDual4Fr4+5NZ6UUKgA5vQx1sxyiYol3Yz/Xz/FFeXdgH9bAfedpO/jNBLv0WEJtJLzuRJOy5E9/5ulZGpn0Gb2UZmEuQ0F7Xa8gsv0sD4H7qbdEnfrrtKOvJyi6I9mdkfOsfzCzLYuONbalnxJE4EfpSa66+OTyzUK5Cs9U9qVT7WbH68pnJvl2hkFTR6POxo4Co+jPrFFk4hehJgU0YthnWPNXwC35EB+Tcr0fiYBw8ODAjN7W9L+QK5CB7wlz8IcB1wQrFKFdVCLXLR4+bCs7HZ4oP1cePLHGviDpeiheBb+0HqEjmwro7n+bRaHtPjdOwoKWJFLtLPywD/x4OXtaQzQn4ZbKNNotzA8wJzysmhfAU4ys4+UX9w+wZlh/w9Rns2Y4P3gyn1S0o/wcklV6ommcSxelaAK3qU8WWYUMByfyOweXJd5hcmBGRabBP1wpXfxkv4XNrMzJe0XFJ+xweJVhJ8Cq5pnyVbBZXjM4xkUn/9aGbRZmNnYYBFf0cxuDROpOSw/Hm0FYLOUwnUKKYUrR/7TOE1Iklg1L7CkeWH5Dzohv3xWmUsdy+nZ7Sncgid7rY0rdD+vYMn/LnBWmDxYGFvuJDrgkBb9dUpe7qo/D89yRV6reDczm5qWM49LvRG4MaWg3SGpUEELcl8KskNwT0CfoK2KSMHM4tLLF7xA8lnZpUR+F7wkyzl4osOzeExNnX2+UPLbvPhLbw7cMrEv/oIrkp8ELIy/XAE2xWNe8mQTd+CE1LbJJX0/WvO4TsJL7dT9Dx6usq0T8nNWGMMkPAN24dT3hcIyqUXbfXEl6+/4C20ZnKaiSP7+mudnHZx0dqlwvf4NWLdmHy+W/HYtnll3DXAdXoj9yBL5B1LX0/zhmB8pkX82tTyJKysjS+TvC5834S/CEcDTJfI3AvPUOBcPVZA5AHepTcWzaZfHrWVV97EH8GAyblxBvq1A9glgUGp9EPB4+D4hR/674VyeHZ5DzwDfC8+Oo9qVLzsvBffcdnjm8Ct49uwGda7J0Mf86WMvkUsSNMAtptu3uq/DfbhF+D4P8KkS2XuATVPrmwD3FMjODeyATwweBH4DfLpA9txwnxyBJ8nUOj9x6T1LdLn2AcgZzhMMwN0s/7SceKxgJRmFV4qoFVeW6edFywnADub8m8xsixp9VXbRKp9gtKxczZl4GbSWxciD/H64wrsE7jK7yEpqYNZxibYjn2q3LXA4HS62psQR1ayzWga5ee57VlCjUh6H2R9XzNLuoCztR5dB0gtmtnTBb2n34HQ8EeSlkr7+TAdlyE+A/wETLWP1lrS0tZGdHf6vO3GX44n4/3uomeUGpMsrAZyNK2Dp81kUU3kIboG7MiPfFHOnGhm0mXYTgc/hz4fkXpvhMs/Ifhe3BN5Bh8vy93ic1iFm9rOcNkuE/oUr2P9sMZ6W8uqIUW36CfiamS2WkZ8ctj8ur1rzJytwo+fsa7FwjEua2dbyqh3rmdmZBfIP4STECwL34Zb3d80sl05F0h54csdCZra8PLHoVDPbvEC+UparnLppNeAGvMZvgwUvp99P6KiWk1YICpPXInonokLXBxGUtlutIABbNeuUFvRR9nK9BvimZXjqSvq6FXf1/QFYBH9RrWPNNQL3xl8atwG/wON09sVnuXsV9L0Rbr15FX/xFZIWZ9otg78Ad8GV5Ivwh98/MnLD8TiXw2isDDANGGPNAcm15FPtnsJn1FOsh27KFv/xmJzNVnLNjaFClqjKyaZXMrO5M/LnmNm3w/fdrDUPYN7YhgDzW05MqBqzkq8wsx2zMl0BeXmzu3D3ZJpZv4jm5NmczS2VdtXLoG2YPIW4tYdLJk91FbQFcQUzHTc7rjPyqh+jmk0sqpN5fgOuhP/KzIaH8zMhT+FN9y1pH2CgObnvRCuOuZtIRYU6/HYl1bJco4I2myLG0PVNrAjkvogDKtUpVTmtyMCS/t8HpsiJTNP9F5FSfhkPBj8AT/0fhCs8aZyDu6/Ox2eXHwAXhm1ljP9n4bx5DS/KVjDPFvwjTsw7IvRzMBmyXfP4wkmSLrQKdQvryqfwIjC1TJlT/TqriYUitzucvDZvP0Nx98v9lgq8DtbHIqQzKmdkiebI1SWbTlsf9iOnmkQaZedI0po55yht7Wxp4VSbhdLxjNk8EumifpatKivnYnvPPAnqA9w6dEiFpmMlHYTXEt0Sr7lcVjbvfdx1OQBYQdIKRQqapO/h/9dSOM/aungGbdGEoJJ8Gwr9omok1G5Yt/JSVYuY2aWSfhlkp0sqiyeVPAv367gLGcrJuz8wsw8VYlmDwlg2mUtXcoCCSg5mVlRhJmIWR1To+gBSipfC56t4lmIRksDdNNO6kXlhWf3KDwmuD0tLBBft1cFF+wkFL+Tw4LyexhqBycNtb5ydPA8vFLm5WoxrzrCfXXAKjLH4w7IIX5RU6hLtpPyBwN/lgfVpF1v6uGtVTQhYDC96nrUMCo/Jadwo7Yuf78eAJOg/oaX4He7Gad65Wbbiwt3KSRIws+druu3rWivT52gtGhNN8s6RFXwvQpp9/1B8ElAFYyTtiStMpS5UADnH4I/xbO89gztuZcvnkxwHbBgsXLeFMX4Nt+CU4ed4nNoU4Pt4fGVu4khdBS3IroPHGm4aJgll91cledUvNXcGXp6qaL0M70haONmfvCxhmVdiP7xCzZVm9khwhedZuhNUUqjlnJB74YkpU4Cf1JwsRswmiApdH0BVxUvSDmb2NzNbVtJCRS+LLhjPuapILWKepfaupEEVXLTpGoHzUe0F+7icYy37oiwiqE3S+bfF45mSmqXv5MmncBz1XKJ15X+Hx3kNwDN8m2Dt1YC8DicRnpj9QdIdOfJ7AGuZ2f+Cq/JySUPM7HjyY/eSvipnida8JpaSc6Ap9T3d176Z9RnnKLgSW52z4ZLeDv0PDN+hQAFPW4gk7V/DYrRr+EyXpGuaZKVwNq6MJmEJL+EB7nkKnczsXXmc24mJq69sMCFsY7KZrYYrOa1QV0F738zel4Skuc1j2FbuAvmk1NwO+PX117A+Gq9p2wAzKxtjK/wYT8JZXtLdeCm2QtqbYK0cBzPcx8+WWGzBw0q+S2uF+lz82XgnTr/zWWD/mscSMRsgKnS9GCHO663kpScn8P0K/uA62cw+zDT5NR3m+FtpLsbeVeOqRS1CBRet2q8ROBBX5L6Q2mYUp9wfhLtyf1pT4W3pEu2k/EJm9oXWYg5VJM01s+9mt6V+2zVnc//EzWpmz0naBFfqlqFEoaPREjYdz1gs3DfV3fbpYPtatSmpMCEwszKXWKf7T+2nsgs1YHkz21nOn4aZvScV8szUdfVhnpg0SdWTQuoqaC9JWgC4Cg8BeROn6OmUvAVuPEmHW2Oc8LXyKjO5kBNB70EzX2UhDYmZPSxPxlkZv/afyLOMSfotcGk4J3PjVuw1gOmSdjWzWwv6/0TSX/Eyf2UT41Wsg9vxTDqIoCMiGhAVut6NS/GM1v8GpekyPLFgDeDPuLskDRV872ocggfz3gFgZhMllb2wqrhofwXsZDXZya0mV19isZG0vKR3zOyDoLQMA84zs7cKmlZxiXZG/lZJX7CceqxZqIA0F+eo6ixelbRGYtELlrpt8RjDQuLiNhSW9DWRKEZN16zV5/XrlQgu/nQptTuA00pcZx8GK3ji7lue1HWUwf7Uc/UlWAJ4RJ6wkVaq8yZmtRQ0M0v48A6RJ8wMwqlbukQeGCxpOQtcjOH5M7hE/mrcwnUr1XgVk+vrxnBOfw2sKekIa47D3JmOON/dcAv1YJy65Nywz7z+t8cJfFtNjGdcIyGOr8rwI2ZDxCzXXgw1Fqg/GieWPTC4SyZaJhtN0uO466Ef7orYldRLMudB1O64alGLhN9rVX+oMZaVgFOAxcxsNXnViO3N7IgW7SbiJKND8MSLa/AYpW0K5G/GXaLZLMVcl04b8tNwHqsP8Ad4YcydPFN0OJ5xN1yBNNfMtis75iqQtBQewN9EcyNpAzO7O2f7EnjcXcL+Px5XVpoKt0v6MrCUmZ0c1h/AX36Gk73m1q5VhdJoYVuStCBqlF6rCjUmEs1DKJNFixhJSX8B5qQjhvSbwMdmlp2UJfJb4hb3VXBOvA3wmsZ3dGb8mX3k0ndYi9qdod0gXNnJegmy7twq46glH9psBZyO89WB38d7Fk2IVJJtWrKPyWY2TNJIfCJ9NHCQmX0+I5d+Dl4B3Gxmp4X1wqxaOc3JZngJssLnqDwRI1G4k4S1d4lZqxEZRAtd70Z6KrYZIf4mmOrz5F+hI3ngVRoTCYqC5tvBVEm7Av3lwdr7khNgn6ANF20dnIG75U4DMLPJ8pi6UoUOV46ny5n1jzOzEyVNKJGv5RKtK2/1ElTeC9fAdHl1j9eokKFZcRyF3G4FytzG+OQhIYQV7uq/XdJX8P/5m6kmB+KJKAnmwuPt5gt9ZK1wCa/fp9UYPzc/+Vm04wu+dwlq/k9prGONfGG3y3kZi/Zzi6SH8eQDAftZQZUJSWvjoQRDaHQnllL3tFLcUv03KFyt2tV157bh/sXMbgzPnqFh0+NmVmTBBLhO0jZm9vcq/QcklrwvAaeY2dVyfsAsPpCHQPwL2JTGjO95Svqfbmb/bWVx62RYQMRshKjQ9W7cLulSXFFbELgdZlhEmmbGKXfiADN7P/2bPFOqq7AP7iJNU4uUKVCHUM9FWwfzmNkDmYdiqzJYAB/J45N2w9nkwS0oRajsEm1HXl47daKZvSPpG7hSdFzBC258cH+dgceu/Y+ZF1dzFG4RTSvDV8s5sybhxLhpzGVmL6bW7zKPZXxDTr+RRZ3SaL3ZRfuxpOXN7OkwluVo7fobgGcnzwGsIqmIx+0CfFJTibpH0l1mNlLNtEVFiSC1FS7quXNrywcX9vdJubAllbmw9wMOkvQh/uysYt16WdJpwBY4vdHcuPcji/2By3FL87Fm9mwY4zZA2SSx1sQ4IqIVosu1F0OupeyMP+wuNbOXw/YRwKJmdlNBu0ruqU6Ma0TmBd5KvraLtkbfNwA/Ai4zJ/UcBXzXCop0p9qtglMB3GtmFwUFc2czO7JAvrJLtE35ybgbdRhO2XImsIO1YLVXCWluT0DSo5YptJ767Uncjf1JattTZrZCgfzTVkCGK2nOkpd1nny33gN1IWlz3AL5DMwovba7meXGukn6I37vN9QozlNwEgWtxliWMedhrDP+2/Es16oKVy13bhvytVzY7UBOHbMVnqn+ZJhIr543SZNXofjEzB4Mz5atcKthoUUw9P8rOhK6bgKOyE7GIyKqIip0fQhyTqSNcO61LO8XkhbHi1xn4+fmx0vKDM22aXMcY3Al8zK8ukJpIoM8M6ty9YeaY1kOj6VZH7dmPAt8ve4La2ZDHSzzvwVeNi/8nquASLrNMuWB8rb1BCQ9htfGzVbMWAi428w+m9l+AR4zdEZm+/eBTcxsdMF+WpZGC3JtlV7rCQQLT5IxWeoilPQEMKyFGzGR3RyPnb2NatQ9tatj1FW4uhuqWAYr9ZvwLOBlzexwSZ8BljCzUsu2vPLLhmH1TnPi8KzMwXhi0hzALcDncW/EFjjf4u9y2tQuoRgR0QrR5dqLIek64BdmNjXMDh/G3U/LSzrdzI7LNPki8G2c/DMdPzcNj7HpEpjzUC2OvzRPl8dxXWLFiQh1XbR1xvIMsEVw1/XDK1LsDJQqdMHF8QeaqT9yY9FqukRrywPT5Iz03wA2Cg/8BhdwcJvPAywi57lKK+xLlh1vN+JY4GZ5ZZIk6WYtvArHsTnyBwBXBVdTWn5unJKnCMdRjdevlou2p9CGi/AZ/P9vqdDh1QKGBvkZ1jyKqXtqVceA+oqbnIT3RJwzbS6cRuWdEgt1LXnqu7D/jJ+bzfCJwf+Ak3GrY9Ex7IdTnSTn8a/huXtiRnQUzjwwNx67vJSZvS3pKJzrskmhs3pcjBER1WBmcemlC/BI6vtBOK0GONP55JJ2O/bgGFfHXYQflsiM6Ib9zo8niZwEbIm/pH6Ec/RdXaH9XXiFiMm41ecQvLh6kfzksI/h4ft+wNgulF8cJzLdMKwvDXwrI7MfboH8IHwmyyTgRzPxOt0WJ1T9T1jGAdu1aLMZrujvA2xWYR9jgH41xjTnzDofBeP5C+4e3CwsZ+OZyVm5E4ETgCuAp/BknxOSpaDvKTXH8nDe9xZt1gUexBWhD3Hl6e0S+fF4ZYMJuHK2O/D7LpTfHHgBt4SNDff9pq2OGc8MT7ZNanHMk4F5U+vzkvPczfQ5IfPbxJL+Lw3HcGar/zgucamyRAtd70Z69r45gdHdzKbJCzAX4TZJx9BhDRiLZxt2yUxQ0mdxK9go/AV+CfCTkibHBAtjJRdtRZyPu1jvxWfRB+Iz+69YTlWEHAw0s9skydw9e4ikOyku5TTdzExOu3G8uUt0t5L+a8mb04QcAyBpEeBFyxAFm1drOF7SPtZsJZhpMLPrJN1qNWJ/zOx2QpJPRdTl9atbeq27UTXLNcnMfQin0qmC+yStYmaPVpSvVR0j4CQ8O/kynO7nW3hN6UKY2VOS+pvZx8DZkkoD/uvIh3t3RSq6sPEkqP508PoNpnUCiWi0+n0MufyeH0qax5wMfa0ZjaVBLfZRuYRiREQVRIWud+NFSfvgZX/WJBBtyjndyjIyzwSm4i5R8IDhs3GXVVfgHLwE0Q+AB1u9yK2+i7YKlrMO9vS/AK/jPHfTKrZ/X07H8KSkHwEvA4uWyLd0ibYjH1xNRwJv4K6g84FFgH6SvmVmeeSqp8lrrlYlqe0JTJX0L5y8dRweP9eVrqSWpdEyOI56pde6G5VchNaRpTsvXp3h47DeH3fp5WEksJukxHqbKGa5SUfWJg1GTQXtXUlzAZMk/QnP1M/LYm5Lvg0X9gl4xvVikn6HT0Z/XTIe8Gfm/fKMbfCQgDNz5DZKlElLJQDh93vZpO9yqv/HEREtEZMiejEkLQochicgnGwhu0peAmwtMzu6oN1Ey5Bo5m1rYzxzAL8HvoO7CoTH650N/KqKQiFpddzasrOZVXkxF/XTkDBQlEBQ0n4dvAD9ArgiNQj4k5ndVyC/OJ5o8qCZ3SlpaTyIP7c6Q1V5SeNxd/ogPLljazO7T14r8yILWcGZNt2e4dcOwjFuiJPgboOXrVuji/oeb2Zr15AfA2yeecHONEjaDJ8IVc1yvQ/YwkIZNknz4YS16+fILpPXh3VhYpC8rNYWuELzSli+bcVJCMvgvGxz4bGL8+Ncbk91kXzteyDcU0ni0O1m9ljhAXe0WRNXmIWX6Kqc3V+h78r/cUREFUSFbhaEpHuBn5nZXWF9A+BoM1uvk/0ei8fvHZBYwoK17Wic7Ha/gnZ5LtrLzey1TowlYU9PXCA9xp4eXKL/qWr5KZNPK9qSHrNUVqhSNC9hfQ5zMuRaGX49AXmFiQ2BjfG4wTdwjrk/dFH/R+Iv4aq8fuvginpVF223IVhe9sUD86tmubY9KZNzFO5tOdmV7aKqwqXmSiD345ZvAw40s8s7I59qV/seSClnhluQcyvnyDO0C2H1akAXorsm3hGzL6LLtRdDUmkMjRWTdO4FnBdiOMBjzcpM/1WxLbBSWjExz+b6AfA4HrSfh3Oo4aKtgnbdRnXPaV2XaBsu1LQF6b3scDLrD+Cu93ZIarsbL+BB87+3LqCjycHewIGSKvH6Ud9F220wz2jc3syOxQPtq+AdSWsmSoektchcH3Lqjd/gGc5X4Rnkh+PWqou6Yuw5CtdYOhSue/HEjTSylUDmprESSFZBqyufoNY9IKcD2glPNhHuMr6sIOzjITrKx0FjrWGji6qy0Pwfr03zMyAiojKiQte7sR7wIv5wvp/8gNwmmHMlDQ/Ws0Tp2p/qL5OSrputTOGFlWd9Sly0ywNfxWOalpJU2UVbBrVRA5L65/QkOlyit5NxidJcQLyufFmAera6RzLWnwJjJKXrWO7e4ji6GyNw68eukn4BPIln9ebFHNWG1S+5VbdUW3fjHkkn4dbpNDFvUX3l/YHLJP0zrC9Bo+IDcB5ugbwCJ7K9DyciHmY5tXjbRF2Fq24lkFry4Tl2N85peXuIGwS/B75Tchyj8Wz790M/R+K0OU0KnZl1VRWbVtifjv/YcMV85x7ad8QsiKjQ9W4sjlNyjMbjsa7H46oqZYma2dup1R/jgeKdwaPBypSNA/sGbqHL4ijcRbtsjov2aIotepVg7ZUkqntO50jFLh6WxNiZ2ePKr8FYS76mpXGwpB+H76cRuLpwxW8ETu0xU2BmkyQ9DTyNu16/gQesd4lCp/q8fnVLtXU3kriow1LbjOL6ypNxbrkZLlqay04tZGaHhO83yZNS1ilz5baBugragukVM/tRanVwF8gvBRyP89X9A7eEPwScbWb/zJFP8Bx+nyTegbnxa7UQ8jrPt1tI7gmu7E3M7Kqydq0QwgFeNK8qMRRP7tgBn+w9W9o4IqIEMYauj0DOMj8aV5IOs5q0FZJeNLPPdHIMn8ZJNt+jwy2xDh6/9lULpclS8k+ScdGG7f3xGKJS2oOKY6pVkijTtuU5VSOrfstEjLrydSDpFeAUCqyKZnZou313FvLkjrnxWpR34QHkXRmUX6s0mmqWXuttaHVthfVJwCZ0XA9j0utdEeulmqXaVLMSSF351O9z4fQp6+NW9/XwJJyiMnRX4c+JW8KmLfDr9DUAM9s3p01ejFtDXGs7kPQwngzxhqSNgItxPsY1gM+a2ajO9B8x+yJa6Ho5gtLxJVzxGIKn3xcxwJeh05p7UNg+L8/YWxV/cdxgZrcVN6nuom0TtZWYmue0jku0Hfk6eMXMDmstNlOwtZn9uxv7r8vrV9dF2y1IWVRzYZkkDXWU7xsor9mcrgYyT6b5IHxilVbwExduV8V63S9pjwKFK69sVt1KIO1WDhmIn5NBYfknMKVE/ia8NNoneKxdFWt21iIKXfPO7J9StncGTjezK4ArJE3sgv4jZlNEha4XQ9K5wGrADXgVg6kt5KeRr7gJfwB2Caw6KWxdF207Y6lbkqjWOa2bfNFuskZFVIqhnEn4UN1IZk1NHsA2XLTdhUSxXBm3ECVJOdvhfH1ZVC7fZ2ZDunCcRailcJlnrq+fmvQBXB+eGU2oKy/p9CA3DY+BvQc4xjK1hFPyaaql53El7TN4/N9BLeJ4x4dr+mT8uboPjeXk2kV/hYx1nEZlz9Rv8Z0c0Taiy7UXQ14NInEjpv+oPuE+quuibXMfdWtG9tlzKmmhrnCjdQckXYGTWad5wYabWZeQWas+D2AtF213Q9LNeEm+JJb0U8BlZrZVgfyOwWpTpe/bzGzzVts6g4zC9UiRwtXdkHQjnjU+FVfm7gWm5nkCgnwZ1dK7ZrZ/yb7mxbOIt8CfDzcDR5jZO0VtKh7Dr3CextfxEn9rBuvzCsC5ZrZBZ/qPmH0RFbqIbkfGRftIiYu2nb7Hk1OSyMwOKm0Y0aUoiDfqFk4tVeABTOLN5HQVLwcXbadiGDsDSY/jCu4HYX1uvJbo0IzcN8zsr5J+Qo61Pe2ilTQAjxO8ncZYuvnxUIjPZtvPCpBnF62Kx8+tj1vc3wDuNbODM7LdHsfbDsJEdAmcSPidsG0lYD4rznyOiChFNO9GdDtquGjb7b9WzciIbsF7kkZaI5l1pzm11F5pNKhfqq27cT7wgLyMlOE0PnnWxSRzdL6c37IK3vdx6osl6XCHAryNuwlnSQTlbKqkt4D/hmVb4HM012JuO443KFg/xeNsZ7wrzawoM7kyLKcijZn9o7P9RszeiBa6iD4NdZQk+gvwKi1KEkV0DyQNxxWUBjJrM+sU96HaKI0W2tVy0fYE5JUKNgyruWWkJC1lZi8VtN/OzK7N2b6P1cx676uQ1zBeHy8v9xHOSXdv+JximVJvIbv1bwVxvF+zkmz4kEV8Kh4uMoO02My6Io4uIqLLERW6iD4NNZckGgT82QpqQEZ0L5Qhszaz4zrZX+XSaCV91CrV1l2QNBIPBzhb0mDcvfZsRuYJ4Itm9lxm++7Ary1DExJ+mwuvDjOjUD1QVqi+zyIkKdyDl+56pYJ823G8kh4ys7W6ZOARET2AqNBF9HlIGggsbWZPzOyxRHRA0gtmtnQn+6jLA1joogXKXLTdCkkH4zGeK5vZSpKWxJMiNsjIbYMT525jZk+Gbb/ErY1b51nv1Eah+tkN7cTxSjoE56m7ksZ6wL0yMSkiIip0EX0akrbDM9bmMrNlJa2B02W0JBaO6F6oa8isP8azkhPqnXeTn4ABZjZnRr4tF213I/CLjQAeTsYgabKZDcuR3RyvBPIV4Hu4RWnbLDVHQn2hNgrVR7SGOsqKpWFm1lW1XCMiuhQxKSKir+MQPBj6DgAzmyhpyEwcT0QHuoLMui6vX91SbT2FDwM1hYWx5ZXNAsDMbpP0bfyavgfY3EIN0gwewPn1ahWqj6gG67marhERXYKo0EX0dUw3s//O5Jf1bAv1EJl1DaSD4rNZtjPTHXGppNOABSTtgRPdnpEVSp1P4eS9mwOvBaoOs0aexOSi/ykwRtIzYX0IsHu3HMVsAEkHmtmfwvedzOyy1G+/j5RIEb0V0eUa0Sch6e/A3sCv8ZI+vwB2BPYF5jSzvWbi8CJmEuq6aHt4bFsCXwhjucnMbmnRpFV/L9FRTWIggVQbLzH3nmXKikVUQ924zYiI3oJooYvoqzgHr894Pk4s+gFwYdh2+MwbVsTMRBsu2h5DUOBuSbJuu6DL/jhfXdo8nfDX9Ypatn0UKvietx4R0WsQFbqIPgkzu1TS9cBvga1wxS4xN+9NYx3MiIiZgk4QI1fBK2Z2WBcMM6IRVvA9bz0iotcgKnQRfRkf4S6muXHLRHzYRvQ2nERH1u3tZLJugc4odNFa1D0YLultgts+fCesD5h5w4qIKEdU6CL6JCRthVvhrsGLW7/boklExMxAd2bdbt7ZDiKa0Zvd9hERZYgKXURfxa+AnczskZk9kIiIEnRb1m0kuI2IiEgjZrlGREREdBN6c9ZtRETErIWo0EVERERERERE9HH0m9kDiIiIiIiIiIiI6ByiQhcRERERERER0ccRFbqIiIiIiIiIiD6OqNBFRERERERERPRxRIUuIiIiIiIiIqKP4/8Bz0zFw7cUbAQAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 720x720 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "def correlation_heatmap(test_data):\n",
    "    correlations = test_data.corr()\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(10,10))\n",
    "    sns.heatmap(correlations, vmax=1.0, center=0, fmt='.2f',\n",
    "                square=True, linewidths=.5,  cbar_kws={\"shrink\": .70})\n",
    "    plt.show();\n",
    "    \n",
    "correlation_heatmap(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "vulnerable-setting",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Number of Null Values</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Feature</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>PoolQC</th>\n",
       "      <td>1453</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MiscFeature</th>\n",
       "      <td>1406</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Alley</th>\n",
       "      <td>1369</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Fence</th>\n",
       "      <td>1179</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>FireplaceQu</th>\n",
       "      <td>690</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LotFrontage</th>\n",
       "      <td>259</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>GarageType</th>\n",
       "      <td>81</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>GarageYrBlt</th>\n",
       "      <td>81</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>GarageCond</th>\n",
       "      <td>81</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>GarageQual</th>\n",
       "      <td>81</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             Number of Null Values\n",
       "Feature                           \n",
       "PoolQC                        1453\n",
       "MiscFeature                   1406\n",
       "Alley                         1369\n",
       "Fence                         1179\n",
       "FireplaceQu                    690\n",
       "LotFrontage                    259\n",
       "GarageType                      81\n",
       "GarageYrBlt                     81\n",
       "GarageCond                      81\n",
       "GarageQual                      81"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Removing the null values\n",
    "null_values = pd.DataFrame(train_data.isnull().sum().sort_values(ascending=False)[:10])\n",
    "null_values.index.name = 'Feature'\n",
    "null_values.columns = ['Number of Null Values']\n",
    "null_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "provincial-suspension",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     Alley FireplaceQu PoolQC Fence MiscFeature Alley PoolQC Fence  \\\n",
      "0      NaN         NaN    NaN   NaN         NaN   NaN    NaN   NaN   \n",
      "0      NaN         NaN    NaN   NaN         NaN   NaN    NaN   NaN   \n",
      "0      NaN         NaN    NaN   NaN         NaN   NaN    NaN   NaN   \n",
      "0      NaN         NaN    NaN   NaN         NaN   NaN    NaN   NaN   \n",
      "0      NaN         NaN    NaN   NaN         NaN   NaN    NaN   NaN   \n",
      "...    ...         ...    ...   ...         ...   ...    ...   ...   \n",
      "1459   NaN         NaN    NaN   NaN         NaN   NaN    NaN   NaN   \n",
      "1459   NaN         NaN    NaN   NaN         NaN   NaN    NaN   NaN   \n",
      "1459   NaN         NaN    NaN   NaN         NaN   NaN    NaN   NaN   \n",
      "1459   NaN         NaN    NaN   NaN         NaN   NaN    NaN   NaN   \n",
      "1459   NaN         NaN    NaN   NaN         NaN   NaN    NaN   NaN   \n",
      "\n",
      "     MiscFeature Alley  ... Alley FireplaceQu PoolQC Fence MiscFeature Alley  \\\n",
      "0            NaN   NaN  ...   NaN         NaN    NaN   NaN         NaN   NaN   \n",
      "0            NaN   NaN  ...   NaN         NaN    NaN   NaN         NaN   NaN   \n",
      "0            NaN   NaN  ...   NaN         NaN    NaN   NaN         NaN   NaN   \n",
      "0            NaN   NaN  ...   NaN         NaN    NaN   NaN         NaN   NaN   \n",
      "0            NaN   NaN  ...   NaN         NaN    NaN   NaN         NaN   NaN   \n",
      "...          ...   ...  ...   ...         ...    ...   ...         ...   ...   \n",
      "1459         NaN   NaN  ...   NaN         NaN    NaN   NaN         NaN   NaN   \n",
      "1459         NaN   NaN  ...   NaN         NaN    NaN   NaN         NaN   NaN   \n",
      "1459         NaN   NaN  ...   NaN         NaN    NaN   NaN         NaN   NaN   \n",
      "1459         NaN   NaN  ...   NaN         NaN    NaN   NaN         NaN   NaN   \n",
      "1459         NaN   NaN  ...   NaN         NaN    NaN   NaN         NaN   NaN   \n",
      "\n",
      "     FireplaceQu PoolQC Fence MiscFeature  \n",
      "0            NaN    NaN   NaN         NaN  \n",
      "0            NaN    NaN   NaN         NaN  \n",
      "0            NaN    NaN   NaN         NaN  \n",
      "0            NaN    NaN   NaN         NaN  \n",
      "0            NaN    NaN   NaN         NaN  \n",
      "...          ...    ...   ...         ...  \n",
      "1459         NaN    NaN   NaN         NaN  \n",
      "1459         NaN    NaN   NaN         NaN  \n",
      "1459         NaN    NaN   NaN         NaN  \n",
      "1459         NaN    NaN   NaN         NaN  \n",
      "1459         NaN    NaN   NaN         NaN  \n",
      "\n",
      "[6965 rows x 6965 columns]\n"
     ]
    }
   ],
   "source": [
    "nan_rows=train_data.iloc[np.where(train_data.isnull())]\n",
    "print(nan_rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "recorded-arthritis",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     Alley FireplaceQu PoolQC MiscFeature Alley FireplaceQu PoolQC  Fence  \\\n",
      "0      NaN         NaN    NaN         NaN   NaN         NaN    NaN  MnPrv   \n",
      "0      NaN         NaN    NaN         NaN   NaN         NaN    NaN  MnPrv   \n",
      "0      NaN         NaN    NaN         NaN   NaN         NaN    NaN  MnPrv   \n",
      "0      NaN         NaN    NaN         NaN   NaN         NaN    NaN  MnPrv   \n",
      "1      NaN         NaN    NaN        Gar2   NaN         NaN    NaN    NaN   \n",
      "...    ...         ...    ...         ...   ...         ...    ...    ...   \n",
      "1457   NaN         NaN    NaN        Shed   NaN         NaN    NaN  MnPrv   \n",
      "1458   NaN          TA    NaN         NaN   NaN          TA    NaN    NaN   \n",
      "1458   NaN          TA    NaN         NaN   NaN          TA    NaN    NaN   \n",
      "1458   NaN          TA    NaN         NaN   NaN          TA    NaN    NaN   \n",
      "1458   NaN          TA    NaN         NaN   NaN          TA    NaN    NaN   \n",
      "\n",
      "     Alley PoolQC  ... GarageType GarageYrBlt GarageFinish GarageQual  \\\n",
      "0      NaN    NaN  ...     Attchd      1961.0          Unf         TA   \n",
      "0      NaN    NaN  ...     Attchd      1961.0          Unf         TA   \n",
      "0      NaN    NaN  ...     Attchd      1961.0          Unf         TA   \n",
      "0      NaN    NaN  ...     Attchd      1961.0          Unf         TA   \n",
      "1      NaN    NaN  ...     Attchd      1958.0          Unf         TA   \n",
      "...    ...    ...  ...        ...         ...          ...        ...   \n",
      "1457   NaN    NaN  ...        NaN         NaN          NaN        NaN   \n",
      "1458   NaN    NaN  ...     Attchd      1993.0          Fin         TA   \n",
      "1458   NaN    NaN  ...     Attchd      1993.0          Fin         TA   \n",
      "1458   NaN    NaN  ...     Attchd      1993.0          Fin         TA   \n",
      "1458   NaN    NaN  ...     Attchd      1993.0          Fin         TA   \n",
      "\n",
      "     GarageCond PoolQC Alley PoolQC  Fence MiscFeature  \n",
      "0            TA    NaN   NaN    NaN  MnPrv         NaN  \n",
      "0            TA    NaN   NaN    NaN  MnPrv         NaN  \n",
      "0            TA    NaN   NaN    NaN  MnPrv         NaN  \n",
      "0            TA    NaN   NaN    NaN  MnPrv         NaN  \n",
      "1            TA    NaN   NaN    NaN    NaN        Gar2  \n",
      "...         ...    ...   ...    ...    ...         ...  \n",
      "1457        NaN    NaN   NaN    NaN  MnPrv        Shed  \n",
      "1458         TA    NaN   NaN    NaN    NaN         NaN  \n",
      "1458         TA    NaN   NaN    NaN    NaN         NaN  \n",
      "1458         TA    NaN   NaN    NaN    NaN         NaN  \n",
      "1458         TA    NaN   NaN    NaN    NaN         NaN  \n",
      "\n",
      "[7000 rows x 7000 columns]\n"
     ]
    }
   ],
   "source": [
    "nan2_rows=test_data.iloc[np.where(test_data.isnull())]\n",
    "print(nan2_rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "rubber-shannon",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['MSZoning', 'Street', 'Alley', 'LotShape', 'LandContour',\n",
       "       'Utilities', 'LotConfig', 'LandSlope', 'Neighborhood',\n",
       "       'Condition1', 'Condition2', 'BldgType', 'HouseStyle', 'RoofStyle',\n",
       "       'RoofMatl', 'Exterior1st', 'Exterior2nd', 'MasVnrType',\n",
       "       'ExterQual', 'ExterCond', 'Foundation', 'BsmtQual', 'BsmtCond',\n",
       "       'BsmtExposure', 'BsmtFinType1', 'BsmtFinType2', 'Heating',\n",
       "       'HeatingQC', 'CentralAir', 'Electrical', 'KitchenQual',\n",
       "       'Functional', 'FireplaceQu', 'GarageType', 'GarageFinish',\n",
       "       'GarageQual', 'GarageCond', 'PavedDrive', 'PoolQC', 'Fence',\n",
       "       'MiscFeature', 'SaleType', 'SaleCondition'], dtype=object)"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.select_dtypes(exclude=[np.number]).columns.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "least-continuity",
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_feat=['MSZoning', 'Street', 'Alley', 'LotShape', 'LandContour',\n",
    "       'Utilities', 'LotConfig', 'LandSlope', 'Neighborhood',\n",
    "       'Condition1', 'Condition2', 'BldgType', 'HouseStyle', 'RoofStyle',\n",
    "       'RoofMatl', 'Exterior1st', 'Exterior2nd', 'MasVnrType',\n",
    "       'ExterQual', 'ExterCond', 'Foundation', 'BsmtQual', 'BsmtCond',\n",
    "       'BsmtExposure', 'BsmtFinType1', 'BsmtFinType2', 'Heating',\n",
    "       'HeatingQC', 'CentralAir', 'Electrical', 'KitchenQual',\n",
    "       'Functional', 'FireplaceQu', 'GarageType', 'GarageFinish',\n",
    "       'GarageQual', 'GarageCond', 'PavedDrive', 'PoolQC', 'Fence',\n",
    "       'MiscFeature', 'SaleType', 'SaleCondition']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "liable-kenya",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Id', 'MSSubClass', 'MSZoning', 'LotFrontage', 'LotArea', 'Street',\n",
       "       'Alley', 'LotShape', 'LandContour', 'Utilities', 'LotConfig',\n",
       "       'LandSlope', 'Neighborhood', 'Condition1', 'Condition2', 'BldgType',\n",
       "       'HouseStyle', 'OverallQual', 'OverallCond', 'YearBuilt', 'YearRemodAdd',\n",
       "       'RoofStyle', 'RoofMatl', 'Exterior1st', 'Exterior2nd', 'MasVnrType',\n",
       "       'MasVnrArea', 'ExterQual', 'ExterCond', 'Foundation', 'BsmtQual',\n",
       "       'BsmtCond', 'BsmtExposure', 'BsmtFinType1', 'BsmtFinSF1',\n",
       "       'BsmtFinType2', 'BsmtFinSF2', 'BsmtUnfSF', 'TotalBsmtSF', 'Heating',\n",
       "       'HeatingQC', 'CentralAir', 'Electrical', '1stFlrSF', '2ndFlrSF',\n",
       "       'LowQualFinSF', 'GrLivArea', 'BsmtFullBath', 'BsmtHalfBath', 'FullBath',\n",
       "       'HalfBath', 'BedroomAbvGr', 'KitchenAbvGr', 'KitchenQual',\n",
       "       'TotRmsAbvGrd', 'Functional', 'Fireplaces', 'FireplaceQu', 'GarageType',\n",
       "       'GarageYrBlt', 'GarageFinish', 'GarageCars', 'GarageArea', 'GarageQual',\n",
       "       'GarageCond', 'PavedDrive', 'WoodDeckSF', 'OpenPorchSF',\n",
       "       'EnclosedPorch', '3SsnPorch', 'ScreenPorch', 'PoolArea', 'PoolQC',\n",
       "       'Fence', 'MiscFeature', 'MiscVal', 'MoSold', 'YrSold', 'SaleType',\n",
       "       'SaleCondition', 'SalePrice', 'LT_SalePrice'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "binding-aside",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = train_data.select_dtypes(include=[np.number]).interpolate().dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "designed-receipt",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Outliers: 154\n",
      "Number of rows without outliers: 1306\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import IsolationForest\n",
    "\n",
    "clf = IsolationForest(max_samples = 100, random_state = 42)\n",
    "clf.fit(data)\n",
    "y_noano = clf.predict(data)\n",
    "y_noano = pd.DataFrame(y_noano, columns = ['Top'])\n",
    "y_noano[y_noano['Top'] == 1].index.values\n",
    "\n",
    "data = data.iloc[y_noano[y_noano['Top'] == 1].index.values]\n",
    "data.reset_index(drop = True, inplace = True)\n",
    "print(\"Number of Outliers:\", y_noano[y_noano['Top'] == -1].shape[0])\n",
    "print(\"Number of rows without outliers:\", data.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "flexible-limit",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>MSSubClass</th>\n",
       "      <th>LotFrontage</th>\n",
       "      <th>LotArea</th>\n",
       "      <th>OverallQual</th>\n",
       "      <th>OverallCond</th>\n",
       "      <th>YearBuilt</th>\n",
       "      <th>YearRemodAdd</th>\n",
       "      <th>MasVnrArea</th>\n",
       "      <th>BsmtFinSF1</th>\n",
       "      <th>...</th>\n",
       "      <th>OpenPorchSF</th>\n",
       "      <th>EnclosedPorch</th>\n",
       "      <th>3SsnPorch</th>\n",
       "      <th>ScreenPorch</th>\n",
       "      <th>PoolArea</th>\n",
       "      <th>MiscVal</th>\n",
       "      <th>MoSold</th>\n",
       "      <th>YrSold</th>\n",
       "      <th>SalePrice</th>\n",
       "      <th>LT_SalePrice</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>60</td>\n",
       "      <td>65.0</td>\n",
       "      <td>8450</td>\n",
       "      <td>7</td>\n",
       "      <td>5</td>\n",
       "      <td>2003</td>\n",
       "      <td>2003</td>\n",
       "      <td>196.0</td>\n",
       "      <td>706</td>\n",
       "      <td>...</td>\n",
       "      <td>61</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2008</td>\n",
       "      <td>208500</td>\n",
       "      <td>12.247699</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>20</td>\n",
       "      <td>80.0</td>\n",
       "      <td>9600</td>\n",
       "      <td>6</td>\n",
       "      <td>8</td>\n",
       "      <td>1976</td>\n",
       "      <td>1976</td>\n",
       "      <td>0.0</td>\n",
       "      <td>978</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>2007</td>\n",
       "      <td>181500</td>\n",
       "      <td>12.109016</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>60</td>\n",
       "      <td>68.0</td>\n",
       "      <td>11250</td>\n",
       "      <td>7</td>\n",
       "      <td>5</td>\n",
       "      <td>2001</td>\n",
       "      <td>2002</td>\n",
       "      <td>162.0</td>\n",
       "      <td>486</td>\n",
       "      <td>...</td>\n",
       "      <td>42</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "      <td>2008</td>\n",
       "      <td>223500</td>\n",
       "      <td>12.317171</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>70</td>\n",
       "      <td>60.0</td>\n",
       "      <td>9550</td>\n",
       "      <td>7</td>\n",
       "      <td>5</td>\n",
       "      <td>1915</td>\n",
       "      <td>1970</td>\n",
       "      <td>0.0</td>\n",
       "      <td>216</td>\n",
       "      <td>...</td>\n",
       "      <td>35</td>\n",
       "      <td>272</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2006</td>\n",
       "      <td>140000</td>\n",
       "      <td>11.849405</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>60</td>\n",
       "      <td>84.0</td>\n",
       "      <td>14260</td>\n",
       "      <td>8</td>\n",
       "      <td>5</td>\n",
       "      <td>2000</td>\n",
       "      <td>2000</td>\n",
       "      <td>350.0</td>\n",
       "      <td>655</td>\n",
       "      <td>...</td>\n",
       "      <td>84</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>12</td>\n",
       "      <td>2008</td>\n",
       "      <td>250000</td>\n",
       "      <td>12.429220</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 39 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Id  MSSubClass  LotFrontage  LotArea  OverallQual  OverallCond  YearBuilt  \\\n",
       "0   1          60         65.0     8450            7            5       2003   \n",
       "1   2          20         80.0     9600            6            8       1976   \n",
       "2   3          60         68.0    11250            7            5       2001   \n",
       "3   4          70         60.0     9550            7            5       1915   \n",
       "4   5          60         84.0    14260            8            5       2000   \n",
       "\n",
       "   YearRemodAdd  MasVnrArea  BsmtFinSF1  ...  OpenPorchSF  EnclosedPorch  \\\n",
       "0          2003       196.0         706  ...           61              0   \n",
       "1          1976         0.0         978  ...            0              0   \n",
       "2          2002       162.0         486  ...           42              0   \n",
       "3          1970         0.0         216  ...           35            272   \n",
       "4          2000       350.0         655  ...           84              0   \n",
       "\n",
       "   3SsnPorch  ScreenPorch  PoolArea  MiscVal  MoSold  YrSold  SalePrice  \\\n",
       "0          0            0         0        0       2    2008     208500   \n",
       "1          0            0         0        0       5    2007     181500   \n",
       "2          0            0         0        0       9    2008     223500   \n",
       "3          0            0         0        0       2    2006     140000   \n",
       "4          0            0         0        0      12    2008     250000   \n",
       "\n",
       "   LT_SalePrice  \n",
       "0     12.247699  \n",
       "1     12.109016  \n",
       "2     12.317171  \n",
       "3     11.849405  \n",
       "4     12.429220  \n",
       "\n",
       "[5 rows x 39 columns]"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "gothic-personality",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>MSSubClass</th>\n",
       "      <th>LotFrontage</th>\n",
       "      <th>LotArea</th>\n",
       "      <th>OverallQual</th>\n",
       "      <th>OverallCond</th>\n",
       "      <th>YearBuilt</th>\n",
       "      <th>YearRemodAdd</th>\n",
       "      <th>MasVnrArea</th>\n",
       "      <th>BsmtFinSF1</th>\n",
       "      <th>...</th>\n",
       "      <th>OpenPorchSF</th>\n",
       "      <th>EnclosedPorch</th>\n",
       "      <th>3SsnPorch</th>\n",
       "      <th>ScreenPorch</th>\n",
       "      <th>PoolArea</th>\n",
       "      <th>MiscVal</th>\n",
       "      <th>MoSold</th>\n",
       "      <th>YrSold</th>\n",
       "      <th>SalePrice</th>\n",
       "      <th>LT_SalePrice</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.235294</td>\n",
       "      <td>0.150685</td>\n",
       "      <td>0.102935</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.428571</td>\n",
       "      <td>0.955224</td>\n",
       "      <td>0.883333</td>\n",
       "      <td>0.12250</td>\n",
       "      <td>0.439601</td>\n",
       "      <td>...</td>\n",
       "      <td>0.116635</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.090909</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.422014</td>\n",
       "      <td>0.701395</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.000685</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.202055</td>\n",
       "      <td>0.119492</td>\n",
       "      <td>0.555556</td>\n",
       "      <td>0.857143</td>\n",
       "      <td>0.753731</td>\n",
       "      <td>0.433333</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.608966</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.363636</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.356378</td>\n",
       "      <td>0.646975</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.001371</td>\n",
       "      <td>0.235294</td>\n",
       "      <td>0.160959</td>\n",
       "      <td>0.143246</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.428571</td>\n",
       "      <td>0.940299</td>\n",
       "      <td>0.866667</td>\n",
       "      <td>0.10125</td>\n",
       "      <td>0.302615</td>\n",
       "      <td>...</td>\n",
       "      <td>0.080306</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.727273</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.458478</td>\n",
       "      <td>0.728656</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.002056</td>\n",
       "      <td>0.294118</td>\n",
       "      <td>0.133562</td>\n",
       "      <td>0.118772</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.428571</td>\n",
       "      <td>0.298507</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.134496</td>\n",
       "      <td>...</td>\n",
       "      <td>0.066922</td>\n",
       "      <td>0.824242</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.090909</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.255493</td>\n",
       "      <td>0.545102</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.002742</td>\n",
       "      <td>0.235294</td>\n",
       "      <td>0.215753</td>\n",
       "      <td>0.186580</td>\n",
       "      <td>0.777778</td>\n",
       "      <td>0.428571</td>\n",
       "      <td>0.932836</td>\n",
       "      <td>0.833333</td>\n",
       "      <td>0.21875</td>\n",
       "      <td>0.407846</td>\n",
       "      <td>...</td>\n",
       "      <td>0.160612</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.522898</td>\n",
       "      <td>0.772625</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 39 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         Id  MSSubClass  LotFrontage   LotArea  OverallQual  OverallCond  \\\n",
       "0  0.000000    0.235294     0.150685  0.102935     0.666667     0.428571   \n",
       "1  0.000685    0.000000     0.202055  0.119492     0.555556     0.857143   \n",
       "2  0.001371    0.235294     0.160959  0.143246     0.666667     0.428571   \n",
       "3  0.002056    0.294118     0.133562  0.118772     0.666667     0.428571   \n",
       "4  0.002742    0.235294     0.215753  0.186580     0.777778     0.428571   \n",
       "\n",
       "   YearBuilt  YearRemodAdd  MasVnrArea  BsmtFinSF1  ...  OpenPorchSF  \\\n",
       "0   0.955224      0.883333     0.12250    0.439601  ...     0.116635   \n",
       "1   0.753731      0.433333     0.00000    0.608966  ...     0.000000   \n",
       "2   0.940299      0.866667     0.10125    0.302615  ...     0.080306   \n",
       "3   0.298507      0.333333     0.00000    0.134496  ...     0.066922   \n",
       "4   0.932836      0.833333     0.21875    0.407846  ...     0.160612   \n",
       "\n",
       "   EnclosedPorch  3SsnPorch  ScreenPorch  PoolArea  MiscVal    MoSold  YrSold  \\\n",
       "0       0.000000        0.0          0.0       0.0      0.0  0.090909    0.50   \n",
       "1       0.000000        0.0          0.0       0.0      0.0  0.363636    0.25   \n",
       "2       0.000000        0.0          0.0       0.0      0.0  0.727273    0.50   \n",
       "3       0.824242        0.0          0.0       0.0      0.0  0.090909    0.00   \n",
       "4       0.000000        0.0          0.0       0.0      0.0  1.000000    0.50   \n",
       "\n",
       "   SalePrice  LT_SalePrice  \n",
       "0   0.422014      0.701395  \n",
       "1   0.356378      0.646975  \n",
       "2   0.458478      0.728656  \n",
       "3   0.255493      0.545102  \n",
       "4   0.522898      0.772625  \n",
       "\n",
       "[5 rows x 39 columns]"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "col_train = list(data.columns)\n",
    "col_train_bis = list(data.columns)\n",
    "\n",
    "col_train_bis.remove('SalePrice')\n",
    "\n",
    "mat_train = np.matrix(data)\n",
    "#mat_test  = np.matrix(test_)\n",
    "mat_new = np.matrix(data.drop('SalePrice',axis = 1))\n",
    "mat_y = np.array(data.SalePrice).reshape((1306,1))\n",
    "\n",
    "prepro_y = MinMaxScaler()\n",
    "prepro_y.fit(mat_y)\n",
    "\n",
    "prepro = MinMaxScaler()\n",
    "prepro.fit(mat_train)\n",
    "\n",
    "prepro_test = MinMaxScaler()\n",
    "prepro_test.fit(mat_new)\n",
    "\n",
    "train = pd.DataFrame(prepro.transform(mat_train),columns = col_train)\n",
    "#test  = pd.DataFrame(prepro_test.transform(mat_test),columns = col_train_bis)\n",
    "\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "equal-lancaster",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Linear Model for the  train and test\n",
    "y = np.log(train_data.SalePrice)\n",
    "X = train_data.drop(['SalePrice', 'Id'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "certain-antenna",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "                                    X, y, random_state=42, test_size=.33)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "beneficial-consciousness",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train=train_data[['MSSubClass', 'MSZoning', 'LotFrontage', 'LotArea', 'Street',\n",
    "       'Alley', 'LotShape', 'LandContour', 'Utilities', 'LotConfig',\n",
    "       'LandSlope', 'Neighborhood', 'Condition1', 'Condition2', 'BldgType',\n",
    "       'HouseStyle', 'OverallQual', 'OverallCond', 'YearBuilt', 'YearRemodAdd',\n",
    "       'RoofStyle', 'RoofMatl', 'Exterior1st', 'Exterior2nd', 'MasVnrType',\n",
    "       'MasVnrArea', 'ExterQual', 'ExterCond', 'Foundation', 'BsmtQual',\n",
    "       'BsmtCond', 'BsmtExposure', 'BsmtFinType1', 'BsmtFinSF1',\n",
    "       'BsmtFinType2', 'BsmtFinSF2', 'BsmtUnfSF', 'TotalBsmtSF', 'Heating',\n",
    "       'HeatingQC', 'CentralAir', 'Electrical', '1stFlrSF', '2ndFlrSF',\n",
    "       'LowQualFinSF', 'GrLivArea', 'BsmtFullBath', 'BsmtHalfBath', 'FullBath',\n",
    "       'HalfBath', 'BedroomAbvGr', 'KitchenAbvGr', 'KitchenQual',\n",
    "       'TotRmsAbvGrd', 'Functional', 'Fireplaces', 'FireplaceQu', 'GarageType',\n",
    "       'GarageYrBlt', 'GarageFinish', 'GarageCars', 'GarageArea', 'GarageQual',\n",
    "       'GarageCond', 'PavedDrive', 'WoodDeckSF', 'OpenPorchSF',\n",
    "       'EnclosedPorch', '3SsnPorch', 'ScreenPorch', 'PoolArea', 'PoolQC',\n",
    "       'Fence', 'MiscFeature', 'MiscVal', 'MoSold', 'YrSold', 'SaleType',\n",
    "       'SaleCondition', 'LT_SalePrice']]\n",
    "y_train=train_data['SalePrice']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "sharp-shopping",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Id', 'MSSubClass', 'MSZoning', 'LotFrontage', 'LotArea', 'Street',\n",
       "       'Alley', 'LotShape', 'LandContour', 'Utilities', 'LotConfig',\n",
       "       'LandSlope', 'Neighborhood', 'Condition1', 'Condition2', 'BldgType',\n",
       "       'HouseStyle', 'OverallQual', 'OverallCond', 'YearBuilt', 'YearRemodAdd',\n",
       "       'RoofStyle', 'RoofMatl', 'Exterior1st', 'Exterior2nd', 'MasVnrType',\n",
       "       'MasVnrArea', 'ExterQual', 'ExterCond', 'Foundation', 'BsmtQual',\n",
       "       'BsmtCond', 'BsmtExposure', 'BsmtFinType1', 'BsmtFinSF1',\n",
       "       'BsmtFinType2', 'BsmtFinSF2', 'BsmtUnfSF', 'TotalBsmtSF', 'Heating',\n",
       "       'HeatingQC', 'CentralAir', 'Electrical', '1stFlrSF', '2ndFlrSF',\n",
       "       'LowQualFinSF', 'GrLivArea', 'BsmtFullBath', 'BsmtHalfBath', 'FullBath',\n",
       "       'HalfBath', 'BedroomAbvGr', 'KitchenAbvGr', 'KitchenQual',\n",
       "       'TotRmsAbvGrd', 'Functional', 'Fireplaces', 'FireplaceQu', 'GarageType',\n",
       "       'GarageYrBlt', 'GarageFinish', 'GarageCars', 'GarageArea', 'GarageQual',\n",
       "       'GarageCond', 'PavedDrive', 'WoodDeckSF', 'OpenPorchSF',\n",
       "       'EnclosedPorch', '3SsnPorch', 'ScreenPorch', 'PoolArea', 'PoolQC',\n",
       "       'Fence', 'MiscFeature', 'MiscVal', 'MoSold', 'YrSold', 'SaleType',\n",
       "       'SaleCondition'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "civil-frank",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test=test_data[['MSSubClass', 'MSZoning', 'LotFrontage', 'LotArea', 'Street',\n",
    "       'Alley', 'LotShape', 'LandContour', 'Utilities', 'LotConfig',\n",
    "       'LandSlope', 'Neighborhood', 'Condition1', 'Condition2', 'BldgType',\n",
    "       'HouseStyle', 'OverallQual', 'OverallCond', 'YearBuilt', 'YearRemodAdd',\n",
    "       'RoofStyle', 'RoofMatl', 'Exterior1st', 'Exterior2nd', 'MasVnrType',\n",
    "       'MasVnrArea', 'ExterQual', 'ExterCond', 'Foundation', 'BsmtQual',\n",
    "       'BsmtCond', 'BsmtExposure', 'BsmtFinType1', 'BsmtFinSF1',\n",
    "       'BsmtFinType2', 'BsmtFinSF2', 'BsmtUnfSF', 'TotalBsmtSF', 'Heating',\n",
    "       'HeatingQC', 'CentralAir', 'Electrical', '1stFlrSF', '2ndFlrSF',\n",
    "       'LowQualFinSF', 'GrLivArea', 'BsmtFullBath', 'BsmtHalfBath', 'FullBath',\n",
    "       'HalfBath', 'BedroomAbvGr', 'KitchenAbvGr', 'KitchenQual',\n",
    "       'TotRmsAbvGrd', 'Functional', 'Fireplaces', 'FireplaceQu', 'GarageType',\n",
    "       'GarageYrBlt', 'GarageFinish', 'GarageCars', 'GarageArea', 'GarageQual',\n",
    "       'GarageCond', 'PavedDrive', 'WoodDeckSF', 'OpenPorchSF',\n",
    "       'EnclosedPorch', '3SsnPorch', 'ScreenPorch', 'PoolArea', 'PoolQC',\n",
    "       'Fence', 'MiscFeature', 'MiscVal', 'MoSold', 'YrSold', 'SaleType',\n",
    "       'SaleCondition']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "encouraging-nation",
   "metadata": {},
   "outputs": [],
   "source": [
    "#handling null values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "turned-biography",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Empty DataFrame\n",
      "Columns: []\n",
      "Index: []\n"
     ]
    }
   ],
   "source": [
    "X_train=X_train.fillna(0)\n",
    "nan_rows=X_train.iloc[np.where(X_train.isnull())]\n",
    "print(nan_rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "quality-island",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>MSSubClass</th>\n",
       "      <th>MSZoning</th>\n",
       "      <th>LotFrontage</th>\n",
       "      <th>LotArea</th>\n",
       "      <th>Street</th>\n",
       "      <th>Alley</th>\n",
       "      <th>LotShape</th>\n",
       "      <th>LandContour</th>\n",
       "      <th>Utilities</th>\n",
       "      <th>LotConfig</th>\n",
       "      <th>...</th>\n",
       "      <th>PoolArea</th>\n",
       "      <th>PoolQC</th>\n",
       "      <th>Fence</th>\n",
       "      <th>MiscFeature</th>\n",
       "      <th>MiscVal</th>\n",
       "      <th>MoSold</th>\n",
       "      <th>YrSold</th>\n",
       "      <th>SaleType</th>\n",
       "      <th>SaleCondition</th>\n",
       "      <th>LT_SalePrice</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>60</td>\n",
       "      <td>RL</td>\n",
       "      <td>65.0</td>\n",
       "      <td>8450</td>\n",
       "      <td>Pave</td>\n",
       "      <td>0</td>\n",
       "      <td>Reg</td>\n",
       "      <td>Lvl</td>\n",
       "      <td>AllPub</td>\n",
       "      <td>Inside</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2008</td>\n",
       "      <td>WD</td>\n",
       "      <td>Normal</td>\n",
       "      <td>12.247699</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>20</td>\n",
       "      <td>RL</td>\n",
       "      <td>80.0</td>\n",
       "      <td>9600</td>\n",
       "      <td>Pave</td>\n",
       "      <td>0</td>\n",
       "      <td>Reg</td>\n",
       "      <td>Lvl</td>\n",
       "      <td>AllPub</td>\n",
       "      <td>FR2</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>2007</td>\n",
       "      <td>WD</td>\n",
       "      <td>Normal</td>\n",
       "      <td>12.109016</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>60</td>\n",
       "      <td>RL</td>\n",
       "      <td>68.0</td>\n",
       "      <td>11250</td>\n",
       "      <td>Pave</td>\n",
       "      <td>0</td>\n",
       "      <td>IR1</td>\n",
       "      <td>Lvl</td>\n",
       "      <td>AllPub</td>\n",
       "      <td>Inside</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "      <td>2008</td>\n",
       "      <td>WD</td>\n",
       "      <td>Normal</td>\n",
       "      <td>12.317171</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>70</td>\n",
       "      <td>RL</td>\n",
       "      <td>60.0</td>\n",
       "      <td>9550</td>\n",
       "      <td>Pave</td>\n",
       "      <td>0</td>\n",
       "      <td>IR1</td>\n",
       "      <td>Lvl</td>\n",
       "      <td>AllPub</td>\n",
       "      <td>Corner</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2006</td>\n",
       "      <td>WD</td>\n",
       "      <td>Abnorml</td>\n",
       "      <td>11.849405</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>60</td>\n",
       "      <td>RL</td>\n",
       "      <td>84.0</td>\n",
       "      <td>14260</td>\n",
       "      <td>Pave</td>\n",
       "      <td>0</td>\n",
       "      <td>IR1</td>\n",
       "      <td>Lvl</td>\n",
       "      <td>AllPub</td>\n",
       "      <td>FR2</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>12</td>\n",
       "      <td>2008</td>\n",
       "      <td>WD</td>\n",
       "      <td>Normal</td>\n",
       "      <td>12.429220</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 80 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   MSSubClass MSZoning  LotFrontage  LotArea Street Alley LotShape  \\\n",
       "0          60       RL         65.0     8450   Pave     0      Reg   \n",
       "1          20       RL         80.0     9600   Pave     0      Reg   \n",
       "2          60       RL         68.0    11250   Pave     0      IR1   \n",
       "3          70       RL         60.0     9550   Pave     0      IR1   \n",
       "4          60       RL         84.0    14260   Pave     0      IR1   \n",
       "\n",
       "  LandContour Utilities LotConfig  ... PoolArea PoolQC Fence MiscFeature  \\\n",
       "0         Lvl    AllPub    Inside  ...        0      0     0           0   \n",
       "1         Lvl    AllPub       FR2  ...        0      0     0           0   \n",
       "2         Lvl    AllPub    Inside  ...        0      0     0           0   \n",
       "3         Lvl    AllPub    Corner  ...        0      0     0           0   \n",
       "4         Lvl    AllPub       FR2  ...        0      0     0           0   \n",
       "\n",
       "  MiscVal MoSold  YrSold  SaleType  SaleCondition  LT_SalePrice  \n",
       "0       0      2    2008        WD         Normal     12.247699  \n",
       "1       0      5    2007        WD         Normal     12.109016  \n",
       "2       0      9    2008        WD         Normal     12.317171  \n",
       "3       0      2    2006        WD        Abnorml     11.849405  \n",
       "4       0     12    2008        WD         Normal     12.429220  \n",
       "\n",
       "[5 rows x 80 columns]"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "nutritional-vaccine",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Empty DataFrame\n",
      "Columns: []\n",
      "Index: []\n"
     ]
    }
   ],
   "source": [
    "X_test=X_test.fillna(0)\n",
    "nan_rows=X_test.iloc[np.where(X_test.isnull())]\n",
    "print(nan_rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "greenhouse-discovery",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>MSSubClass</th>\n",
       "      <th>MSZoning</th>\n",
       "      <th>LotFrontage</th>\n",
       "      <th>LotArea</th>\n",
       "      <th>Street</th>\n",
       "      <th>Alley</th>\n",
       "      <th>LotShape</th>\n",
       "      <th>LandContour</th>\n",
       "      <th>Utilities</th>\n",
       "      <th>LotConfig</th>\n",
       "      <th>...</th>\n",
       "      <th>ScreenPorch</th>\n",
       "      <th>PoolArea</th>\n",
       "      <th>PoolQC</th>\n",
       "      <th>Fence</th>\n",
       "      <th>MiscFeature</th>\n",
       "      <th>MiscVal</th>\n",
       "      <th>MoSold</th>\n",
       "      <th>YrSold</th>\n",
       "      <th>SaleType</th>\n",
       "      <th>SaleCondition</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>20</td>\n",
       "      <td>RH</td>\n",
       "      <td>80.0</td>\n",
       "      <td>11622</td>\n",
       "      <td>Pave</td>\n",
       "      <td>0</td>\n",
       "      <td>Reg</td>\n",
       "      <td>Lvl</td>\n",
       "      <td>AllPub</td>\n",
       "      <td>Inside</td>\n",
       "      <td>...</td>\n",
       "      <td>120</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>MnPrv</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>2010</td>\n",
       "      <td>WD</td>\n",
       "      <td>Normal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>20</td>\n",
       "      <td>RL</td>\n",
       "      <td>81.0</td>\n",
       "      <td>14267</td>\n",
       "      <td>Pave</td>\n",
       "      <td>0</td>\n",
       "      <td>IR1</td>\n",
       "      <td>Lvl</td>\n",
       "      <td>AllPub</td>\n",
       "      <td>Corner</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Gar2</td>\n",
       "      <td>12500</td>\n",
       "      <td>6</td>\n",
       "      <td>2010</td>\n",
       "      <td>WD</td>\n",
       "      <td>Normal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>60</td>\n",
       "      <td>RL</td>\n",
       "      <td>74.0</td>\n",
       "      <td>13830</td>\n",
       "      <td>Pave</td>\n",
       "      <td>0</td>\n",
       "      <td>IR1</td>\n",
       "      <td>Lvl</td>\n",
       "      <td>AllPub</td>\n",
       "      <td>Inside</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>MnPrv</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>2010</td>\n",
       "      <td>WD</td>\n",
       "      <td>Normal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>60</td>\n",
       "      <td>RL</td>\n",
       "      <td>78.0</td>\n",
       "      <td>9978</td>\n",
       "      <td>Pave</td>\n",
       "      <td>0</td>\n",
       "      <td>IR1</td>\n",
       "      <td>Lvl</td>\n",
       "      <td>AllPub</td>\n",
       "      <td>Inside</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>2010</td>\n",
       "      <td>WD</td>\n",
       "      <td>Normal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>120</td>\n",
       "      <td>RL</td>\n",
       "      <td>43.0</td>\n",
       "      <td>5005</td>\n",
       "      <td>Pave</td>\n",
       "      <td>0</td>\n",
       "      <td>IR1</td>\n",
       "      <td>HLS</td>\n",
       "      <td>AllPub</td>\n",
       "      <td>Inside</td>\n",
       "      <td>...</td>\n",
       "      <td>144</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2010</td>\n",
       "      <td>WD</td>\n",
       "      <td>Normal</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 79 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   MSSubClass MSZoning  LotFrontage  LotArea Street Alley LotShape  \\\n",
       "0          20       RH         80.0    11622   Pave     0      Reg   \n",
       "1          20       RL         81.0    14267   Pave     0      IR1   \n",
       "2          60       RL         74.0    13830   Pave     0      IR1   \n",
       "3          60       RL         78.0     9978   Pave     0      IR1   \n",
       "4         120       RL         43.0     5005   Pave     0      IR1   \n",
       "\n",
       "  LandContour Utilities LotConfig  ... ScreenPorch PoolArea PoolQC  Fence  \\\n",
       "0         Lvl    AllPub    Inside  ...         120        0      0  MnPrv   \n",
       "1         Lvl    AllPub    Corner  ...           0        0      0      0   \n",
       "2         Lvl    AllPub    Inside  ...           0        0      0  MnPrv   \n",
       "3         Lvl    AllPub    Inside  ...           0        0      0      0   \n",
       "4         HLS    AllPub    Inside  ...         144        0      0      0   \n",
       "\n",
       "  MiscFeature MiscVal  MoSold  YrSold  SaleType  SaleCondition  \n",
       "0           0       0       6    2010        WD         Normal  \n",
       "1        Gar2   12500       6    2010        WD         Normal  \n",
       "2           0       0       3    2010        WD         Normal  \n",
       "3           0       0       6    2010        WD         Normal  \n",
       "4           0       0       1    2010        WD         Normal  \n",
       "\n",
       "[5 rows x 79 columns]"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "primary-scotland",
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "import pandas as pd\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.wrappers.scikit_learn import KerasRegressor\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from keras.callbacks import ModelCheckpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "structural-president",
   "metadata": {},
   "outputs": [],
   "source": [
    "train=pd.read_csv('C:/Users/Laptop Zone/jupyterNoteBook/HousingPricePrediction/data/train.csv')\n",
    "test=pd.read_csv('C:/Users/Laptop Zone/jupyterNoteBook/HousingPricePrediction/data/test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "contained-sheriff",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.concat([train, test],axis=0, sort='False', ignore_index = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "innovative-youth",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data[data.columns.difference(['Id'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "historical-citizenship",
   "metadata": {},
   "outputs": [],
   "source": [
    "ids = test[\"Id\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "sorted-composition",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "positive-brook",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Encoding categorical data\n",
    "data = pd.get_dummies(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "collectible-stick",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2919, 312)"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "fixed-timer",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>1stFlrSF</th>\n",
       "      <th>2ndFlrSF</th>\n",
       "      <th>3SsnPorch</th>\n",
       "      <th>BedroomAbvGr</th>\n",
       "      <th>BsmtFinSF1</th>\n",
       "      <th>BsmtFinSF2</th>\n",
       "      <th>BsmtFullBath</th>\n",
       "      <th>BsmtHalfBath</th>\n",
       "      <th>BsmtUnfSF</th>\n",
       "      <th>EnclosedPorch</th>\n",
       "      <th>...</th>\n",
       "      <th>SaleType_ConLI</th>\n",
       "      <th>SaleType_ConLw</th>\n",
       "      <th>SaleType_New</th>\n",
       "      <th>SaleType_Oth</th>\n",
       "      <th>SaleType_WD</th>\n",
       "      <th>Street_Grvl</th>\n",
       "      <th>Street_Pave</th>\n",
       "      <th>Utilities_0</th>\n",
       "      <th>Utilities_AllPub</th>\n",
       "      <th>Utilities_NoSeWa</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>856</td>\n",
       "      <td>854</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>706.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>150.0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1262</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>978.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>284.0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>920</td>\n",
       "      <td>866</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>486.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>434.0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>961</td>\n",
       "      <td>756</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>216.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>540.0</td>\n",
       "      <td>272</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1145</td>\n",
       "      <td>1053</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>655.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>490.0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 312 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   1stFlrSF  2ndFlrSF  3SsnPorch  BedroomAbvGr  BsmtFinSF1  BsmtFinSF2  \\\n",
       "0       856       854          0             3       706.0         0.0   \n",
       "1      1262         0          0             3       978.0         0.0   \n",
       "2       920       866          0             3       486.0         0.0   \n",
       "3       961       756          0             3       216.0         0.0   \n",
       "4      1145      1053          0             4       655.0         0.0   \n",
       "\n",
       "   BsmtFullBath  BsmtHalfBath  BsmtUnfSF  EnclosedPorch  ...  SaleType_ConLI  \\\n",
       "0           1.0           0.0      150.0              0  ...               0   \n",
       "1           0.0           1.0      284.0              0  ...               0   \n",
       "2           1.0           0.0      434.0              0  ...               0   \n",
       "3           1.0           0.0      540.0            272  ...               0   \n",
       "4           1.0           0.0      490.0              0  ...               0   \n",
       "\n",
       "   SaleType_ConLw  SaleType_New  SaleType_Oth  SaleType_WD  Street_Grvl  \\\n",
       "0               0             0             0            1            0   \n",
       "1               0             0             0            1            0   \n",
       "2               0             0             0            1            0   \n",
       "3               0             0             0            1            0   \n",
       "4               0             0             0            1            0   \n",
       "\n",
       "   Street_Pave  Utilities_0  Utilities_AllPub  Utilities_NoSeWa  \n",
       "0            1            0                 1                 0  \n",
       "1            1            0                 1                 0  \n",
       "2            1            0                 1                 0  \n",
       "3            1            0                 1                 0  \n",
       "4            1            0                 1                 0  \n",
       "\n",
       "[5 rows x 312 columns]"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "mexican-navigation",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = data.iloc[:1460,:] #Upto 1460 rows from first\n",
    "test = data.iloc[1460:,:] # From 1461th row to last\n",
    "X_train = train[train.columns.difference(['SalePrice'])].values\n",
    "y_train = train[['SalePrice']].values\n",
    "X_test = test[test.columns.difference(['SalePrice'])].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "thirty-comfort",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature Scaling\n",
    "from sklearn.preprocessing import StandardScaler, PowerTransformer\n",
    "pt_X = PowerTransformer(method='yeo-johnson', standardize=False)\n",
    "sc_y = StandardScaler()\n",
    "sc_X = StandardScaler()\n",
    "y_train = sc_y.fit_transform(y_train)\n",
    "X_train = sc_X.fit_transform(X_train)\n",
    "X_test = sc_X.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "detected-innocent",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1460, 1)"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "occupational-howard",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1460,)"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "flat_y = y_train.flatten()\n",
    "flat_y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "roman-subsection",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold, cross_val_score\n",
    "kfolds = KFold(n_splits=10, shuffle=True, random_state=42)\n",
    "def rmsle(y, y_pred):\n",
    "    return np.sqrt(mean_squared_error(y, y_pred))\n",
    "\n",
    "def cv_rmse(model, x=X):\n",
    "    rmse = np.sqrt(-cross_val_score(model, X, y, scoring=\"neg_mean_squared_error\", cv=kfolds))\n",
    "    return (rmse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "dried-holocaust",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training...\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.004667 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n"
     ]
    }
   ],
   "source": [
    "import lightgbm as lgb\n",
    "# create dataset for lightgbm\n",
    "lgb_train = lgb.Dataset(X_train, flat_y)\n",
    "# specify your configurations as a dict\n",
    "params = {\n",
    "    'boosting_type': 'gbdt',\n",
    "    'objective': 'regression',\n",
    "    'metric': {'l2', 'l1'},\n",
    "    'num_leaves': 31,\n",
    "    'learning_rate': 0.05,\n",
    "    'feature_fraction': 0.9,\n",
    "    'bagging_fraction': 0.8,\n",
    "    'bagging_freq': 5,\n",
    "    'verbose': 0\n",
    "}\n",
    "\n",
    "print('Starting training...')\n",
    "# train\n",
    "gbm = lgb.train(params,\n",
    "                lgb_train,\n",
    "                num_boost_round=250)\n",
    "# predict\n",
    "lgbm_prediction_tr = gbm.predict(X_train, num_iteration=gbm.best_iteration)\n",
    "lgbm_prediction_te = gbm.predict(X_test, num_iteration=gbm.best_iteration)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "entitled-performance",
   "metadata": {},
   "outputs": [],
   "source": [
    "train[\"lgb\"] = lgbm_prediction_tr\n",
    "test[\"lgb\"] = lgbm_prediction_te"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "chicken-developer",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>1stFlrSF</th>\n",
       "      <th>2ndFlrSF</th>\n",
       "      <th>3SsnPorch</th>\n",
       "      <th>BedroomAbvGr</th>\n",
       "      <th>BsmtFinSF1</th>\n",
       "      <th>BsmtFinSF2</th>\n",
       "      <th>BsmtFullBath</th>\n",
       "      <th>BsmtHalfBath</th>\n",
       "      <th>BsmtUnfSF</th>\n",
       "      <th>EnclosedPorch</th>\n",
       "      <th>...</th>\n",
       "      <th>SaleType_ConLw</th>\n",
       "      <th>SaleType_New</th>\n",
       "      <th>SaleType_Oth</th>\n",
       "      <th>SaleType_WD</th>\n",
       "      <th>Street_Grvl</th>\n",
       "      <th>Street_Pave</th>\n",
       "      <th>Utilities_0</th>\n",
       "      <th>Utilities_AllPub</th>\n",
       "      <th>Utilities_NoSeWa</th>\n",
       "      <th>lgb</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>856</td>\n",
       "      <td>854</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>706.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>150.0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.318032</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1262</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>978.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>284.0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.008252</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2 rows × 313 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   1stFlrSF  2ndFlrSF  3SsnPorch  BedroomAbvGr  BsmtFinSF1  BsmtFinSF2  \\\n",
       "0       856       854          0             3       706.0         0.0   \n",
       "1      1262         0          0             3       978.0         0.0   \n",
       "\n",
       "   BsmtFullBath  BsmtHalfBath  BsmtUnfSF  EnclosedPorch  ...  SaleType_ConLw  \\\n",
       "0           1.0           0.0      150.0              0  ...               0   \n",
       "1           0.0           1.0      284.0              0  ...               0   \n",
       "\n",
       "   SaleType_New  SaleType_Oth  SaleType_WD  Street_Grvl  Street_Pave  \\\n",
       "0             0             0            1            0            1   \n",
       "1             0             0            1            0            1   \n",
       "\n",
       "   Utilities_0  Utilities_AllPub  Utilities_NoSeWa       lgb  \n",
       "0            0                 1                 0  0.318032  \n",
       "1            0                 1                 0  0.008252  \n",
       "\n",
       "[2 rows x 313 columns]"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "undefined-aaron",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = sc_y.inverse_transform(lgbm_prediction_te)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "higher-aaron",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([125112.61947507, 159941.60061715, 179800.79082196, ...,\n",
       "       147411.31676378, 115851.43746952, 210385.08884737])"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "blank-purse",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.concat([train, test],axis=0, sort='False', ignore_index = True)\n",
    "data = data[data.columns.difference(['Id'])]\n",
    "train = data.iloc[:1460,:]\n",
    "test = data.iloc[1460:,:]\n",
    "X_train = train[train.columns.difference(['SalePrice'])].values\n",
    "y_train = train[['SalePrice']].values\n",
    "X_test = test[test.columns.difference(['SalePrice'])].values\n",
    "pt_X = PowerTransformer(method='yeo-johnson', standardize=False)\n",
    "sc_y = StandardScaler()\n",
    "sc_X = StandardScaler()\n",
    "y_train = sc_y.fit_transform(y_train)\n",
    "X_train = sc_X.fit_transform(X_train)\n",
    "X_test = sc_X.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "through-occupation",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Inıtialising the ANN\n",
    "model = Sequential()\n",
    "#Adding the input layer and first hidden layer\n",
    "model.add(Dense(units =300, kernel_initializer='random_uniform', activation= 'tanh', \n",
    "                input_dim=X_train.shape[1]))\n",
    "#Add the first hidden layer\n",
    "model.add(Dense(units =300, kernel_initializer='random_uniform', activation= 'tanh'))\n",
    "#Add the second hidden layer\n",
    "\n",
    "model.add(Dense(units =10, kernel_initializer='random_uniform', activation= 'relu'))\n",
    "#The output layer\n",
    "model.add(Dense(units =1, kernel_initializer='random_uniform', activation= 'relu'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "verbal-color",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_5\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_32 (Dense)             (None, 300)               93900     \n",
      "_________________________________________________________________\n",
      "dense_33 (Dense)             (None, 300)               90300     \n",
      "_________________________________________________________________\n",
      "dense_34 (Dense)             (None, 10)                3010      \n",
      "_________________________________________________________________\n",
      "dense_35 (Dense)             (None, 1)                 11        \n",
      "=================================================================\n",
      "Total params: 187,221\n",
      "Trainable params: 187,221\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "optional-professional",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "39/43 [==========================>...] - ETA: 0s - loss: 0.0627 - mse: 0.6238\n",
      "Epoch 00001: val_loss improved from inf to 0.02198, saving model to min_vl_model.h5\n",
      "43/43 [==============================] - 1s 12ms/step - loss: 0.0620 - mse: 0.6339 - val_loss: 0.0220 - val_mse: 0.4246\n",
      "Epoch 2/200\n",
      "43/43 [==============================] - ETA: 0s - loss: 0.0255 - mse: 0.4947\n",
      "Epoch 00002: val_loss did not improve from 0.02198\n",
      "43/43 [==============================] - 0s 6ms/step - loss: 0.0255 - mse: 0.4947 - val_loss: 0.0316 - val_mse: 0.4384\n",
      "Epoch 3/200\n",
      "40/43 [==========================>...] - ETA: 0s - loss: 0.0187 - mse: 0.4453\n",
      "Epoch 00003: val_loss did not improve from 0.02198\n",
      "43/43 [==============================] - 0s 6ms/step - loss: 0.0185 - mse: 0.4354 - val_loss: 0.0335 - val_mse: 0.4635\n",
      "Epoch 4/200\n",
      "41/43 [===========================>..] - ETA: 0s - loss: 0.0164 - mse: 0.4062\n",
      "Epoch 00004: val_loss did not improve from 0.02198\n",
      "43/43 [==============================] - 0s 6ms/step - loss: 0.0173 - mse: 0.4287 - val_loss: 0.0421 - val_mse: 0.5008\n",
      "Epoch 5/200\n",
      "38/43 [=========================>....] - ETA: 0s - loss: 0.0113 - mse: 0.3870\n",
      "Epoch 00005: val_loss did not improve from 0.02198\n",
      "43/43 [==============================] - 0s 5ms/step - loss: 0.0117 - mse: 0.3935 - val_loss: 0.0375 - val_mse: 0.4666\n",
      "Epoch 6/200\n",
      "39/43 [==========================>...] - ETA: 0s - loss: 0.0121 - mse: 0.3962\n",
      "Epoch 00006: val_loss did not improve from 0.02198\n",
      "43/43 [==============================] - 0s 5ms/step - loss: 0.0117 - mse: 0.3920 - val_loss: 0.0619 - val_mse: 0.6672\n",
      "Epoch 7/200\n",
      "41/43 [===========================>..] - ETA: 0s - loss: 0.0104 - mse: 0.3788\n",
      "Epoch 00007: val_loss did not improve from 0.02198\n",
      "43/43 [==============================] - 0s 5ms/step - loss: 0.0103 - mse: 0.3762 - val_loss: 0.0547 - val_mse: 0.6039\n",
      "Epoch 8/200\n",
      "39/43 [==========================>...] - ETA: 0s - loss: 0.0088 - mse: 0.3551\n",
      "Epoch 00008: val_loss did not improve from 0.02198\n",
      "43/43 [==============================] - 0s 5ms/step - loss: 0.0091 - mse: 0.3602 - val_loss: 0.0591 - val_mse: 0.6587\n",
      "Epoch 9/200\n",
      "35/43 [=======================>......] - ETA: 0s - loss: 0.0075 - mse: 0.3403\n",
      "Epoch 00009: val_loss did not improve from 0.02198\n",
      "43/43 [==============================] - 0s 8ms/step - loss: 0.0077 - mse: 0.3502 - val_loss: 0.0690 - val_mse: 0.7813\n",
      "Epoch 10/200\n",
      "42/43 [============================>.] - ETA: 0s - loss: 0.0075 - mse: 0.3521\n",
      "Epoch 00010: val_loss did not improve from 0.02198\n",
      "43/43 [==============================] - 0s 6ms/step - loss: 0.0076 - mse: 0.3499 - val_loss: 0.0678 - val_mse: 0.7354\n",
      "Epoch 11/200\n",
      "39/43 [==========================>...] - ETA: 0s - loss: 0.0066 - mse: 0.3512\n",
      "Epoch 00011: val_loss did not improve from 0.02198\n",
      "43/43 [==============================] - 0s 5ms/step - loss: 0.0064 - mse: 0.3468 - val_loss: 0.0530 - val_mse: 0.5968\n",
      "Epoch 12/200\n",
      "36/43 [========================>.....] - ETA: 0s - loss: 0.0047 - mse: 0.3371\n",
      "Epoch 00012: val_loss did not improve from 0.02198\n",
      "43/43 [==============================] - 0s 5ms/step - loss: 0.0050 - mse: 0.3349 - val_loss: 0.0587 - val_mse: 0.6592\n",
      "Epoch 13/200\n",
      "33/43 [======================>.......] - ETA: 0s - loss: 0.0045 - mse: 0.3302\n",
      "Epoch 00013: val_loss did not improve from 0.02198\n",
      "43/43 [==============================] - 0s 6ms/step - loss: 0.0041 - mse: 0.3278 - val_loss: 0.0619 - val_mse: 0.6896\n",
      "Epoch 14/200\n",
      "37/43 [========================>.....] - ETA: 0s - loss: 0.0042 - mse: 0.3335\n",
      "Epoch 00014: val_loss did not improve from 0.02198\n",
      "43/43 [==============================] - 0s 5ms/step - loss: 0.0040 - mse: 0.3275 - val_loss: 0.0701 - val_mse: 0.7977\n",
      "Epoch 15/200\n",
      "35/43 [=======================>......] - ETA: 0s - loss: 0.0032 - mse: 0.3221\n",
      "Epoch 00015: val_loss did not improve from 0.02198\n",
      "43/43 [==============================] - 0s 5ms/step - loss: 0.0032 - mse: 0.3203 - val_loss: 0.0750 - val_mse: 0.8690\n",
      "Epoch 16/200\n",
      "38/43 [=========================>....] - ETA: 0s - loss: 0.0032 - mse: 0.3305  \n",
      "Epoch 00016: val_loss did not improve from 0.02198\n",
      "43/43 [==============================] - 0s 5ms/step - loss: 0.0031 - mse: 0.3179 - val_loss: 0.0712 - val_mse: 0.8191\n",
      "Epoch 17/200\n",
      "36/43 [========================>.....] - ETA: 0s - loss: 0.0029 - mse: 0.3219\n",
      "Epoch 00017: val_loss did not improve from 0.02198\n",
      "43/43 [==============================] - 0s 5ms/step - loss: 0.0028 - mse: 0.3157 - val_loss: 0.0839 - val_mse: 0.9966\n",
      "Epoch 18/200\n",
      "39/43 [==========================>...] - ETA: 0s - loss: 0.0029 - mse: 0.3084\n",
      "Epoch 00018: val_loss did not improve from 0.02198\n",
      "43/43 [==============================] - 0s 5ms/step - loss: 0.0030 - mse: 0.3142 - val_loss: 0.0849 - val_mse: 1.0269\n",
      "Epoch 19/200\n",
      "36/43 [========================>.....] - ETA: 0s - loss: 0.0042 - mse: 0.3194  \n",
      "Epoch 00019: val_loss did not improve from 0.02198\n",
      "43/43 [==============================] - 0s 5ms/step - loss: 0.0041 - mse: 0.3171 - val_loss: 0.0712 - val_mse: 0.8074\n",
      "Epoch 20/200\n",
      "38/43 [=========================>....] - ETA: 0s - loss: 0.0032 - mse: 0.3186\n",
      "Epoch 00020: val_loss did not improve from 0.02198\n",
      "43/43 [==============================] - 0s 6ms/step - loss: 0.0031 - mse: 0.3173 - val_loss: 0.0894 - val_mse: 1.0857\n",
      "Epoch 21/200\n",
      "34/43 [======================>.......] - ETA: 0s - loss: 0.0033 - mse: 0.3205\n",
      "Epoch 00021: val_loss did not improve from 0.02198\n",
      "43/43 [==============================] - 0s 5ms/step - loss: 0.0030 - mse: 0.3195 - val_loss: 0.0670 - val_mse: 0.7560\n",
      "Epoch 22/200\n",
      "37/43 [========================>.....] - ETA: 0s - loss: 0.0022 - mse: 0.3048\n",
      "Epoch 00022: val_loss did not improve from 0.02198\n",
      "43/43 [==============================] - 0s 5ms/step - loss: 0.0021 - mse: 0.3107 - val_loss: 0.0740 - val_mse: 0.8540\n",
      "Epoch 23/200\n",
      "38/43 [=========================>....] - ETA: 0s - loss: 0.0023 - mse: 0.3109  \n",
      "Epoch 00023: val_loss did not improve from 0.02198\n",
      "43/43 [==============================] - 0s 5ms/step - loss: 0.0023 - mse: 0.3100 - val_loss: 0.0859 - val_mse: 1.0250\n",
      "Epoch 24/200\n",
      "37/43 [========================>.....] - ETA: 0s - loss: 0.0021 - mse: 0.3154\n",
      "Epoch 00024: val_loss did not improve from 0.02198\n",
      "43/43 [==============================] - 0s 5ms/step - loss: 0.0020 - mse: 0.3111 - val_loss: 0.0798 - val_mse: 0.9378\n",
      "Epoch 25/200\n",
      "33/43 [======================>.......] - ETA: 0s - loss: 0.0018 - mse: 0.3201  \n",
      "Epoch 00025: val_loss did not improve from 0.02198\n",
      "43/43 [==============================] - 0s 5ms/step - loss: 0.0018 - mse: 0.3075 - val_loss: 0.0868 - val_mse: 1.0472\n",
      "Epoch 26/200\n",
      "37/43 [========================>.....] - ETA: 0s - loss: 0.0016 - mse: 0.3003  \n",
      "Epoch 00026: val_loss did not improve from 0.02198\n",
      "43/43 [==============================] - 0s 5ms/step - loss: 0.0016 - mse: 0.3050 - val_loss: 0.0884 - val_mse: 1.0859\n",
      "Epoch 27/200\n",
      "36/43 [========================>.....] - ETA: 0s - loss: 0.0015 - mse: 0.3101   \n",
      "Epoch 00027: val_loss did not improve from 0.02198\n",
      "43/43 [==============================] - 0s 5ms/step - loss: 0.0014 - mse: 0.3042 - val_loss: 0.0870 - val_mse: 1.0628\n",
      "Epoch 28/200\n",
      "37/43 [========================>.....] - ETA: 0s - loss: 0.0014 - mse: 0.3025  \n",
      "Epoch 00028: val_loss did not improve from 0.02198\n",
      "43/43 [==============================] - 0s 5ms/step - loss: 0.0014 - mse: 0.3039 - val_loss: 0.0906 - val_mse: 1.1343\n",
      "Epoch 29/200\n",
      "33/43 [======================>.......] - ETA: 0s - loss: 0.0016 - mse: 0.3033\n",
      "Epoch 00029: val_loss did not improve from 0.02198\n",
      "43/43 [==============================] - 0s 6ms/step - loss: 0.0016 - mse: 0.3044 - val_loss: 0.0929 - val_mse: 1.1746\n",
      "Epoch 30/200\n",
      "37/43 [========================>.....] - ETA: 0s - loss: 0.0015 - mse: 0.3026\n",
      "Epoch 00030: val_loss did not improve from 0.02198\n",
      "43/43 [==============================] - 0s 5ms/step - loss: 0.0015 - mse: 0.3033 - val_loss: 0.0922 - val_mse: 1.1554\n",
      "Epoch 31/200\n",
      "39/43 [==========================>...] - ETA: 0s - loss: 0.0015 - mse: 0.3043  \n",
      "Epoch 00031: val_loss did not improve from 0.02198\n",
      "43/43 [==============================] - 0s 6ms/step - loss: 0.0015 - mse: 0.3048 - val_loss: 0.0920 - val_mse: 1.1640\n",
      "Epoch 32/200\n",
      "32/43 [=====================>........] - ETA: 0s - loss: 0.0017 - mse: 0.2974\n",
      "Epoch 00032: val_loss did not improve from 0.02198\n",
      "43/43 [==============================] - 0s 6ms/step - loss: 0.0016 - mse: 0.3038 - val_loss: 0.0925 - val_mse: 1.1694\n",
      "Epoch 33/200\n",
      "36/43 [========================>.....] - ETA: 0s - loss: 0.0014 - mse: 0.3063\n",
      "Epoch 00033: val_loss did not improve from 0.02198\n",
      "43/43 [==============================] - 0s 5ms/step - loss: 0.0014 - mse: 0.3038 - val_loss: 0.0927 - val_mse: 1.1875\n",
      "Epoch 34/200\n",
      "41/43 [===========================>..] - ETA: 0s - loss: 0.0012 - mse: 0.3004   \n",
      "Epoch 00034: val_loss did not improve from 0.02198\n",
      "43/43 [==============================] - 0s 7ms/step - loss: 0.0011 - mse: 0.3017 - val_loss: 0.0964 - val_mse: 1.2521\n",
      "Epoch 35/200\n",
      "34/43 [======================>.......] - ETA: 0s - loss: 0.0015 - mse: 0.2963\n",
      "Epoch 00035: val_loss did not improve from 0.02198\n",
      "43/43 [==============================] - 0s 6ms/step - loss: 0.0015 - mse: 0.3034 - val_loss: 0.0936 - val_mse: 1.2051\n",
      "Epoch 36/200\n",
      "33/43 [======================>.......] - ETA: 0s - loss: 0.0015 - mse: 0.3042  \n",
      "Epoch 00036: val_loss did not improve from 0.02198\n",
      "43/43 [==============================] - 0s 6ms/step - loss: 0.0014 - mse: 0.3031 - val_loss: 0.0950 - val_mse: 1.2232\n",
      "Epoch 37/200\n",
      "39/43 [==========================>...] - ETA: 0s - loss: 0.0013 - mse: 0.3025  \n",
      "Epoch 00037: val_loss did not improve from 0.02198\n",
      "43/43 [==============================] - 0s 6ms/step - loss: 0.0013 - mse: 0.3022 - val_loss: 0.0941 - val_mse: 1.2103\n",
      "Epoch 38/200\n",
      "32/43 [=====================>........] - ETA: 0s - loss: 0.0011 - mse: 0.2975  \n",
      "Epoch 00038: val_loss did not improve from 0.02198\n",
      "43/43 [==============================] - 0s 6ms/step - loss: 0.0012 - mse: 0.3010 - val_loss: 0.1044 - val_mse: 1.4356\n",
      "Epoch 39/200\n",
      "38/43 [=========================>....] - ETA: 0s - loss: 0.0012 - mse: 0.2989  \n",
      "Epoch 00039: val_loss did not improve from 0.02198\n",
      "43/43 [==============================] - 0s 5ms/step - loss: 0.0012 - mse: 0.3014 - val_loss: 0.0971 - val_mse: 1.2826\n",
      "Epoch 40/200\n",
      "39/43 [==========================>...] - ETA: 0s - loss: 0.0011 - mse: 0.3032\n",
      "Epoch 00040: val_loss did not improve from 0.02198\n",
      "43/43 [==============================] - 0s 7ms/step - loss: 0.0012 - mse: 0.3012 - val_loss: 0.0998 - val_mse: 1.3290\n",
      "Epoch 41/200\n",
      "41/43 [===========================>..] - ETA: 0s - loss: 0.0011 - mse: 0.2956    \n",
      "Epoch 00041: val_loss did not improve from 0.02198\n",
      "43/43 [==============================] - 0s 6ms/step - loss: 0.0011 - mse: 0.3004 - val_loss: 0.1003 - val_mse: 1.3486\n",
      "Epoch 42/200\n",
      "42/43 [============================>.] - ETA: 0s - loss: 9.2706e-04 - mse: 0.2978\n",
      "Epoch 00042: val_loss did not improve from 0.02198\n",
      "43/43 [==============================] - 0s 6ms/step - loss: 9.1832e-04 - mse: 0.2993 - val_loss: 0.1033 - val_mse: 1.4115\n",
      "Epoch 43/200\n",
      "36/43 [========================>.....] - ETA: 0s - loss: 8.3108e-04 - mse: 0.3170\n",
      "Epoch 00043: val_loss did not improve from 0.02198\n",
      "43/43 [==============================] - 0s 5ms/step - loss: 9.4995e-04 - mse: 0.2991 - val_loss: 0.1025 - val_mse: 1.4059\n",
      "Epoch 44/200\n",
      "40/43 [==========================>...] - ETA: 0s - loss: 0.0010 - mse: 0.3022  \n",
      "Epoch 00044: val_loss did not improve from 0.02198\n",
      "43/43 [==============================] - 0s 7ms/step - loss: 0.0010 - mse: 0.2994 - val_loss: 0.1046 - val_mse: 1.4450\n",
      "Epoch 45/200\n",
      "34/43 [======================>.......] - ETA: 0s - loss: 8.2495e-04 - mse: 0.2920\n",
      "Epoch 00045: val_loss did not improve from 0.02198\n",
      "43/43 [==============================] - 0s 6ms/step - loss: 9.0349e-04 - mse: 0.2992 - val_loss: 0.1028 - val_mse: 1.4108\n",
      "Epoch 46/200\n",
      "43/43 [==============================] - ETA: 0s - loss: 8.9530e-04 - mse: 0.2996\n",
      "Epoch 00046: val_loss did not improve from 0.02198\n",
      "43/43 [==============================] - 0s 7ms/step - loss: 8.9530e-04 - mse: 0.2996 - val_loss: 0.0993 - val_mse: 1.3277\n",
      "Epoch 47/200\n",
      "43/43 [==============================] - ETA: 0s - loss: 0.0013 - mse: 0.3002 \n",
      "Epoch 00047: val_loss did not improve from 0.02198\n",
      "43/43 [==============================] - 0s 6ms/step - loss: 0.0013 - mse: 0.3002 - val_loss: 0.1038 - val_mse: 1.4257\n",
      "Epoch 48/200\n",
      "38/43 [=========================>....] - ETA: 0s - loss: 0.0016 - mse: 0.3008 \n",
      "Epoch 00048: val_loss did not improve from 0.02198\n",
      "43/43 [==============================] - 0s 7ms/step - loss: 0.0016 - mse: 0.3027 - val_loss: 0.0934 - val_mse: 1.2101\n",
      "Epoch 49/200\n",
      "40/43 [==========================>...] - ETA: 0s - loss: 0.0012 - mse: 0.3035   \n",
      "Epoch 00049: val_loss did not improve from 0.02198\n",
      "43/43 [==============================] - 0s 5ms/step - loss: 0.0012 - mse: 0.3009 - val_loss: 0.0961 - val_mse: 1.2675\n",
      "Epoch 50/200\n",
      "34/43 [======================>.......] - ETA: 0s - loss: 0.0012 - mse: 0.3025\n",
      "Epoch 00050: val_loss did not improve from 0.02198\n",
      "43/43 [==============================] - 0s 6ms/step - loss: 0.0012 - mse: 0.3007 - val_loss: 0.0966 - val_mse: 1.2812\n",
      "Epoch 51/200\n",
      "36/43 [========================>.....] - ETA: 0s - loss: 0.0012 - mse: 0.3090  \n",
      "Epoch 00051: val_loss did not improve from 0.02198\n",
      "43/43 [==============================] - 0s 5ms/step - loss: 0.0013 - mse: 0.3016 - val_loss: 0.0981 - val_mse: 1.3165\n",
      "Epoch 52/200\n",
      "43/43 [==============================] - ETA: 0s - loss: 0.0016 - mse: 0.3028 \n",
      "Epoch 00052: val_loss did not improve from 0.02198\n",
      "43/43 [==============================] - 0s 6ms/step - loss: 0.0016 - mse: 0.3028 - val_loss: 0.0935 - val_mse: 1.2048\n",
      "Epoch 53/200\n",
      "35/43 [=======================>......] - ETA: 0s - loss: 0.0020 - mse: 0.3001  \n",
      "Epoch 00053: val_loss did not improve from 0.02198\n",
      "43/43 [==============================] - 0s 5ms/step - loss: 0.0018 - mse: 0.3035 - val_loss: 0.0904 - val_mse: 1.1414\n",
      "Epoch 54/200\n",
      "40/43 [==========================>...] - ETA: 0s - loss: 0.0019 - mse: 0.3009  \n",
      "Epoch 00054: val_loss did not improve from 0.02198\n",
      "43/43 [==============================] - 0s 5ms/step - loss: 0.0018 - mse: 0.3018 - val_loss: 0.1013 - val_mse: 1.3728\n",
      "Epoch 55/200\n",
      "42/43 [============================>.] - ETA: 0s - loss: 0.0012 - mse: 0.3025\n",
      "Epoch 00055: val_loss did not improve from 0.02198\n",
      "43/43 [==============================] - 0s 7ms/step - loss: 0.0012 - mse: 0.3012 - val_loss: 0.0951 - val_mse: 1.2388\n",
      "Epoch 56/200\n",
      "35/43 [=======================>......] - ETA: 0s - loss: 0.0012 - mse: 0.2990   \n",
      "Epoch 00056: val_loss did not improve from 0.02198\n",
      "43/43 [==============================] - 0s 6ms/step - loss: 0.0012 - mse: 0.3015 - val_loss: 0.0986 - val_mse: 1.2805\n",
      "Epoch 57/200\n",
      "41/43 [===========================>..] - ETA: 0s - loss: 9.7612e-04 - mse: 0.3023\n",
      "Epoch 00057: val_loss did not improve from 0.02198\n",
      "43/43 [==============================] - 0s 6ms/step - loss: 9.9368e-04 - mse: 0.3004 - val_loss: 0.1004 - val_mse: 1.3502\n",
      "Epoch 58/200\n",
      "37/43 [========================>.....] - ETA: 0s - loss: 8.5230e-04 - mse: 0.3005\n",
      "Epoch 00058: val_loss did not improve from 0.02198\n",
      "43/43 [==============================] - 0s 5ms/step - loss: 9.3862e-04 - mse: 0.3000 - val_loss: 0.1020 - val_mse: 1.3901\n",
      "Epoch 59/200\n",
      "40/43 [==========================>...] - ETA: 0s - loss: 8.4946e-04 - mse: 0.3022\n",
      "Epoch 00059: val_loss did not improve from 0.02198\n",
      "43/43 [==============================] - 0s 7ms/step - loss: 8.2664e-04 - mse: 0.2994 - val_loss: 0.1034 - val_mse: 1.4302\n",
      "Epoch 60/200\n",
      "35/43 [=======================>......] - ETA: 0s - loss: 7.7778e-04 - mse: 0.3052\n",
      "Epoch 00060: val_loss did not improve from 0.02198\n",
      "43/43 [==============================] - 0s 5ms/step - loss: 7.2838e-04 - mse: 0.2985 - val_loss: 0.1041 - val_mse: 1.4388\n",
      "Epoch 61/200\n",
      "35/43 [=======================>......] - ETA: 0s - loss: 7.7480e-04 - mse: 0.3041\n",
      "Epoch 00061: val_loss did not improve from 0.02198\n",
      "43/43 [==============================] - 0s 6ms/step - loss: 7.4840e-04 - mse: 0.2981 - val_loss: 0.1058 - val_mse: 1.4772\n",
      "Epoch 62/200\n",
      "36/43 [========================>.....] - ETA: 0s - loss: 9.0872e-04 - mse: 0.2920\n",
      "Epoch 00062: val_loss did not improve from 0.02198\n",
      "43/43 [==============================] - 0s 6ms/step - loss: 8.3047e-04 - mse: 0.2990 - val_loss: 0.1089 - val_mse: 1.5682\n",
      "Epoch 63/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32/43 [=====================>........] - ETA: 0s - loss: 0.0012 - mse: 0.2869   \n",
      "Epoch 00063: val_loss did not improve from 0.02198\n",
      "43/43 [==============================] - 0s 6ms/step - loss: 0.0010 - mse: 0.2998 - val_loss: 0.1046 - val_mse: 1.4514\n",
      "Epoch 64/200\n",
      "40/43 [==========================>...] - ETA: 0s - loss: 8.6874e-04 - mse: 0.3014\n",
      "Epoch 00064: val_loss did not improve from 0.02198\n",
      "43/43 [==============================] - 0s 6ms/step - loss: 8.4676e-04 - mse: 0.2995 - val_loss: 0.1060 - val_mse: 1.4897\n",
      "Epoch 65/200\n",
      "42/43 [============================>.] - ETA: 0s - loss: 8.1878e-04 - mse: 0.2996\n",
      "Epoch 00065: val_loss did not improve from 0.02198\n",
      "43/43 [==============================] - 0s 5ms/step - loss: 8.1315e-04 - mse: 0.2984 - val_loss: 0.1073 - val_mse: 1.5260\n",
      "Epoch 66/200\n",
      "41/43 [===========================>..] - ETA: 0s - loss: 7.6682e-04 - mse: 0.3012\n",
      "Epoch 00066: val_loss did not improve from 0.02198\n",
      "43/43 [==============================] - 0s 5ms/step - loss: 7.6026e-04 - mse: 0.2981 - val_loss: 0.1097 - val_mse: 1.5756\n",
      "Epoch 67/200\n",
      "41/43 [===========================>..] - ETA: 0s - loss: 8.2309e-04 - mse: 0.2968\n",
      "Epoch 00067: val_loss did not improve from 0.02198\n",
      "43/43 [==============================] - 0s 6ms/step - loss: 8.3904e-04 - mse: 0.2988 - val_loss: 0.1095 - val_mse: 1.5781\n",
      "Epoch 68/200\n",
      "38/43 [=========================>....] - ETA: 0s - loss: 6.8715e-04 - mse: 0.3032\n",
      "Epoch 00068: val_loss did not improve from 0.02198\n",
      "43/43 [==============================] - 0s 5ms/step - loss: 7.5471e-04 - mse: 0.2983 - val_loss: 0.1080 - val_mse: 1.5464\n",
      "Epoch 69/200\n",
      "41/43 [===========================>..] - ETA: 0s - loss: 7.7287e-04 - mse: 0.2969\n",
      "Epoch 00069: val_loss did not improve from 0.02198\n",
      "43/43 [==============================] - 0s 6ms/step - loss: 7.8945e-04 - mse: 0.2981 - val_loss: 0.1121 - val_mse: 1.6405\n",
      "Epoch 70/200\n",
      "39/43 [==========================>...] - ETA: 0s - loss: 8.1177e-04 - mse: 0.2987\n",
      "Epoch 00070: val_loss did not improve from 0.02198\n",
      "43/43 [==============================] - 0s 6ms/step - loss: 7.7729e-04 - mse: 0.2982 - val_loss: 0.1110 - val_mse: 1.6301\n",
      "Epoch 71/200\n",
      "37/43 [========================>.....] - ETA: 0s - loss: 7.3066e-04 - mse: 0.2993\n",
      "Epoch 00071: val_loss did not improve from 0.02198\n",
      "43/43 [==============================] - 0s 5ms/step - loss: 7.4926e-04 - mse: 0.2982 - val_loss: 0.1099 - val_mse: 1.5952\n",
      "Epoch 72/200\n",
      "37/43 [========================>.....] - ETA: 0s - loss: 8.8268e-04 - mse: 0.2991\n",
      "Epoch 00072: val_loss did not improve from 0.02198\n",
      "43/43 [==============================] - 0s 7ms/step - loss: 9.0321e-04 - mse: 0.2993 - val_loss: 0.1132 - val_mse: 1.6837\n",
      "Epoch 73/200\n",
      "40/43 [==========================>...] - ETA: 0s - loss: 9.2792e-04 - mse: 0.2990\n",
      "Epoch 00073: val_loss did not improve from 0.02198\n",
      "43/43 [==============================] - 0s 5ms/step - loss: 9.0508e-04 - mse: 0.2988 - val_loss: 0.1115 - val_mse: 1.6467\n",
      "Epoch 74/200\n",
      "39/43 [==========================>...] - ETA: 0s - loss: 8.5319e-04 - mse: 0.2971\n",
      "Epoch 00074: val_loss did not improve from 0.02198\n",
      "43/43 [==============================] - 0s 5ms/step - loss: 8.7012e-04 - mse: 0.2986 - val_loss: 0.1095 - val_mse: 1.5892\n",
      "Epoch 75/200\n",
      "40/43 [==========================>...] - ETA: 0s - loss: 7.5402e-04 - mse: 0.3015\n",
      "Epoch 00075: val_loss did not improve from 0.02198\n",
      "43/43 [==============================] - 0s 6ms/step - loss: 7.3055e-04 - mse: 0.2983 - val_loss: 0.1070 - val_mse: 1.5080\n",
      "Epoch 76/200\n",
      "38/43 [=========================>....] - ETA: 0s - loss: 6.3760e-04 - mse: 0.3002\n",
      "Epoch 00076: val_loss did not improve from 0.02198\n",
      "43/43 [==============================] - 0s 5ms/step - loss: 7.0444e-04 - mse: 0.2980 - val_loss: 0.1080 - val_mse: 1.5379\n",
      "Epoch 77/200\n",
      "34/43 [======================>.......] - ETA: 0s - loss: 6.8677e-04 - mse: 0.2981\n",
      "Epoch 00077: val_loss did not improve from 0.02198\n",
      "43/43 [==============================] - 0s 6ms/step - loss: 7.8834e-04 - mse: 0.2982 - val_loss: 0.1120 - val_mse: 1.6576\n",
      "Epoch 78/200\n",
      "36/43 [========================>.....] - ETA: 0s - loss: 6.5329e-04 - mse: 0.3010\n",
      "Epoch 00078: val_loss did not improve from 0.02198\n",
      "43/43 [==============================] - 0s 5ms/step - loss: 8.3507e-04 - mse: 0.2984 - val_loss: 0.1131 - val_mse: 1.6944\n",
      "Epoch 79/200\n",
      "37/43 [========================>.....] - ETA: 0s - loss: 9.6601e-04 - mse: 0.2933\n",
      "Epoch 00079: val_loss did not improve from 0.02198\n",
      "43/43 [==============================] - 0s 5ms/step - loss: 0.0010 - mse: 0.3013 - val_loss: 0.1083 - val_mse: 1.5416\n",
      "Epoch 80/200\n",
      "39/43 [==========================>...] - ETA: 0s - loss: 0.0023 - mse: 0.3190  \n",
      "Epoch 00080: val_loss did not improve from 0.02198\n",
      "43/43 [==============================] - 0s 5ms/step - loss: 0.0023 - mse: 0.3168 - val_loss: 0.1186 - val_mse: 1.8199\n",
      "Epoch 81/200\n",
      "38/43 [=========================>....] - ETA: 0s - loss: 0.0021 - mse: 0.3005\n",
      "Epoch 00081: val_loss did not improve from 0.02198\n",
      "43/43 [==============================] - 0s 5ms/step - loss: 0.0022 - mse: 0.3061 - val_loss: 0.1158 - val_mse: 1.7089\n",
      "Epoch 82/200\n",
      "40/43 [==========================>...] - ETA: 0s - loss: 0.0023 - mse: 0.3059\n",
      "Epoch 00082: val_loss did not improve from 0.02198\n",
      "43/43 [==============================] - 0s 5ms/step - loss: 0.0023 - mse: 0.3061 - val_loss: 0.1107 - val_mse: 1.6145\n",
      "Epoch 83/200\n",
      "38/43 [=========================>....] - ETA: 0s - loss: 0.0017 - mse: 0.2987\n",
      "Epoch 00083: val_loss did not improve from 0.02198\n",
      "43/43 [==============================] - 0s 5ms/step - loss: 0.0018 - mse: 0.3028 - val_loss: 0.1065 - val_mse: 1.4927\n",
      "Epoch 84/200\n",
      "34/43 [======================>.......] - ETA: 0s - loss: 0.0015 - mse: 0.2960  \n",
      "Epoch 00084: val_loss did not improve from 0.02198\n",
      "43/43 [==============================] - 0s 5ms/step - loss: 0.0015 - mse: 0.3020 - val_loss: 0.1051 - val_mse: 1.4625\n",
      "Epoch 85/200\n",
      "36/43 [========================>.....] - ETA: 0s - loss: 0.0020 - mse: 0.3077    - ETA: 0s - loss: 0.0021 - mse: 0.301\n",
      "Epoch 00085: val_loss did not improve from 0.02198\n",
      "43/43 [==============================] - 0s 5ms/step - loss: 0.0018 - mse: 0.3026 - val_loss: 0.1048 - val_mse: 1.4247\n",
      "Epoch 86/200\n",
      "40/43 [==========================>...] - ETA: 0s - loss: 0.0015 - mse: 0.3049  \n",
      "Epoch 00086: val_loss did not improve from 0.02198\n",
      "43/43 [==============================] - 0s 5ms/step - loss: 0.0015 - mse: 0.3037 - val_loss: 0.1068 - val_mse: 1.4484\n",
      "Epoch 87/200\n",
      "40/43 [==========================>...] - ETA: 0s - loss: 0.0060 - mse: 0.3271208 - ETA: 0s - loss: 0.0030 - mse: 0.2716 \n",
      "Epoch 00087: val_loss did not improve from 0.02198\n",
      "43/43 [==============================] - 0s 6ms/step - loss: 0.0059 - mse: 0.3256 - val_loss: 0.0539 - val_mse: 0.6222\n",
      "Epoch 88/200\n",
      "40/43 [==========================>...] - ETA: 0s - loss: 0.0113 - mse: 0.3805\n",
      "Epoch 00088: val_loss did not improve from 0.02198\n",
      "43/43 [==============================] - 0s 7ms/step - loss: 0.0111 - mse: 0.3770 - val_loss: 0.0788 - val_mse: 0.9182\n",
      "Epoch 89/200\n",
      "35/43 [=======================>......] - ETA: 0s - loss: 0.0072 - mse: 0.3561  \n",
      "Epoch 00089: val_loss did not improve from 0.02198\n",
      "43/43 [==============================] - 0s 6ms/step - loss: 0.0069 - mse: 0.3471 - val_loss: 0.0773 - val_mse: 0.8655\n",
      "Epoch 90/200\n",
      "42/43 [============================>.] - ETA: 0s - loss: 0.0043 - mse: 0.3215\n",
      "Epoch 00090: val_loss did not improve from 0.02198\n",
      "43/43 [==============================] - 0s 6ms/step - loss: 0.0042 - mse: 0.3215 - val_loss: 0.0818 - val_mse: 0.9658\n",
      "Epoch 91/200\n",
      "42/43 [============================>.] - ETA: 0s - loss: 0.0024 - mse: 0.3131\n",
      "Epoch 00091: val_loss did not improve from 0.02198\n",
      "43/43 [==============================] - 0s 7ms/step - loss: 0.0024 - mse: 0.3117 - val_loss: 0.0890 - val_mse: 1.1065\n",
      "Epoch 92/200\n",
      "38/43 [=========================>....] - ETA: 0s - loss: 0.0020 - mse: 0.3083\n",
      "Epoch 00092: val_loss did not improve from 0.02198\n",
      "43/43 [==============================] - 0s 8ms/step - loss: 0.0019 - mse: 0.3060 - val_loss: 0.0941 - val_mse: 1.1987\n",
      "Epoch 93/200\n",
      "37/43 [========================>.....] - ETA: 0s - loss: 0.0014 - mse: 0.3008  \n",
      "Epoch 00093: val_loss did not improve from 0.02198\n",
      "43/43 [==============================] - 0s 5ms/step - loss: 0.0016 - mse: 0.3036 - val_loss: 0.1001 - val_mse: 1.3060\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 94/200\n",
      "42/43 [============================>.] - ETA: 0s - loss: 0.0016 - mse: 0.3011\n",
      "Epoch 00094: val_loss did not improve from 0.02198\n",
      "43/43 [==============================] - 0s 7ms/step - loss: 0.0016 - mse: 0.3029 - val_loss: 0.0988 - val_mse: 1.2814\n",
      "Epoch 95/200\n",
      "41/43 [===========================>..] - ETA: 0s - loss: 0.0015 - mse: 0.3005  \n",
      "Epoch 00095: val_loss did not improve from 0.02198\n",
      "43/43 [==============================] - 0s 7ms/step - loss: 0.0014 - mse: 0.3013 - val_loss: 0.1006 - val_mse: 1.3334\n",
      "Epoch 96/200\n",
      "43/43 [==============================] - ETA: 0s - loss: 0.0014 - mse: 0.3003    \n",
      "Epoch 00096: val_loss did not improve from 0.02198\n",
      "43/43 [==============================] - 0s 7ms/step - loss: 0.0014 - mse: 0.3003 - val_loss: 0.1035 - val_mse: 1.3980\n",
      "Epoch 97/200\n",
      "39/43 [==========================>...] - ETA: 0s - loss: 0.0014 - mse: 0.3044  \n",
      "Epoch 00097: val_loss did not improve from 0.02198\n",
      "43/43 [==============================] - 0s 7ms/step - loss: 0.0013 - mse: 0.2996 - val_loss: 0.1070 - val_mse: 1.4921\n",
      "Epoch 98/200\n",
      "40/43 [==========================>...] - ETA: 0s - loss: 0.0020 - mse: 0.3003  \n",
      "Epoch 00098: val_loss did not improve from 0.02198\n",
      "43/43 [==============================] - 0s 7ms/step - loss: 0.0019 - mse: 0.3053 - val_loss: 0.0952 - val_mse: 1.2080\n",
      "Epoch 99/200\n",
      "38/43 [=========================>....] - ETA: 0s - loss: 0.0014 - mse: 0.3069 \n",
      "Epoch 00099: val_loss did not improve from 0.02198\n",
      "43/43 [==============================] - 0s 8ms/step - loss: 0.0017 - mse: 0.3018 - val_loss: 0.1071 - val_mse: 1.5088\n",
      "Epoch 100/200\n",
      "36/43 [========================>.....] - ETA: 0s - loss: 0.0018 - mse: 0.3079   \n",
      "Epoch 00100: val_loss did not improve from 0.02198\n",
      "43/43 [==============================] - 0s 6ms/step - loss: 0.0018 - mse: 0.3043 - val_loss: 0.0886 - val_mse: 1.1074\n",
      "Epoch 101/200\n",
      "43/43 [==============================] - ETA: 0s - loss: 9.3045e-04 - mse: 0.3019\n",
      "Epoch 00101: val_loss did not improve from 0.02198\n",
      "43/43 [==============================] - 0s 8ms/step - loss: 9.3045e-04 - mse: 0.3019 - val_loss: 0.0919 - val_mse: 1.1730\n",
      "Epoch 102/200\n",
      "43/43 [==============================] - ETA: 0s - loss: 7.1715e-04 - mse: 0.2994\n",
      "Epoch 00102: val_loss did not improve from 0.02198\n",
      "43/43 [==============================] - 0s 6ms/step - loss: 7.1715e-04 - mse: 0.2994 - val_loss: 0.0940 - val_mse: 1.2114\n",
      "Epoch 103/200\n",
      "38/43 [=========================>....] - ETA: 0s - loss: 9.7759e-04 - mse: 0.3042\n",
      "Epoch 00103: val_loss did not improve from 0.02198\n",
      "43/43 [==============================] - 0s 7ms/step - loss: 9.6934e-04 - mse: 0.2998 - val_loss: 0.0843 - val_mse: 1.0336\n",
      "Epoch 104/200\n",
      "41/43 [===========================>..] - ETA: 0s - loss: 6.8396e-04 - mse: 0.3018\n",
      "Epoch 00104: val_loss did not improve from 0.02198\n",
      "43/43 [==============================] - 0s 6ms/step - loss: 6.7084e-04 - mse: 0.2995 - val_loss: 0.0870 - val_mse: 1.0807\n",
      "Epoch 105/200\n",
      "36/43 [========================>.....] - ETA: 0s - loss: 4.0868e-04 - mse: 0.3014\n",
      "Epoch 00105: val_loss did not improve from 0.02198\n",
      "43/43 [==============================] - 0s 6ms/step - loss: 5.4500e-04 - mse: 0.2990 - val_loss: 0.0877 - val_mse: 1.0911\n",
      "Epoch 106/200\n",
      "37/43 [========================>.....] - ETA: 0s - loss: 4.6557e-04 - mse: 0.2997\n",
      "Epoch 00106: val_loss did not improve from 0.02198\n",
      "43/43 [==============================] - 0s 7ms/step - loss: 5.2021e-04 - mse: 0.2983 - val_loss: 0.0900 - val_mse: 1.1359\n",
      "Epoch 107/200\n",
      "35/43 [=======================>......] - ETA: 0s - loss: 3.7081e-04 - mse: 0.2886\n",
      "Epoch 00107: val_loss did not improve from 0.02198\n",
      "43/43 [==============================] - 0s 6ms/step - loss: 4.2551e-04 - mse: 0.2973 - val_loss: 0.0912 - val_mse: 1.1642\n",
      "Epoch 108/200\n",
      "43/43 [==============================] - ETA: 0s - loss: 4.6167e-04 - mse: 0.2973\n",
      "Epoch 00108: val_loss did not improve from 0.02198\n",
      "43/43 [==============================] - 0s 6ms/step - loss: 4.6167e-04 - mse: 0.2973 - val_loss: 0.0913 - val_mse: 1.1656\n",
      "Epoch 109/200\n",
      "37/43 [========================>.....] - ETA: 0s - loss: 3.8443e-04 - mse: 0.3009\n",
      "Epoch 00109: val_loss did not improve from 0.02198\n",
      "43/43 [==============================] - 0s 7ms/step - loss: 4.0153e-04 - mse: 0.2970 - val_loss: 0.0936 - val_mse: 1.2121\n",
      "Epoch 110/200\n",
      "33/43 [======================>.......] - ETA: 0s - loss: 3.4038e-04 - mse: 0.3010\n",
      "Epoch 00110: val_loss did not improve from 0.02198\n",
      "43/43 [==============================] - 0s 6ms/step - loss: 3.7970e-04 - mse: 0.2967 - val_loss: 0.0944 - val_mse: 1.2324\n",
      "Epoch 111/200\n",
      "43/43 [==============================] - ETA: 0s - loss: 3.6179e-04 - mse: 0.2967\n",
      "Epoch 00111: val_loss did not improve from 0.02198\n",
      "43/43 [==============================] - 0s 6ms/step - loss: 3.6179e-04 - mse: 0.2967 - val_loss: 0.0954 - val_mse: 1.2536\n",
      "Epoch 112/200\n",
      "42/43 [============================>.] - ETA: 0s - loss: 3.6868e-04 - mse: 0.2968\n",
      "Epoch 00112: val_loss did not improve from 0.02198\n",
      "43/43 [==============================] - 0s 7ms/step - loss: 3.6556e-04 - mse: 0.2966 - val_loss: 0.0954 - val_mse: 1.2552\n",
      "Epoch 113/200\n",
      "40/43 [==========================>...] - ETA: 0s - loss: 4.1542e-04 - mse: 0.2917\n",
      "Epoch 00113: val_loss did not improve from 0.02198\n",
      "43/43 [==============================] - 0s 7ms/step - loss: 3.9618e-04 - mse: 0.2968 - val_loss: 0.0972 - val_mse: 1.2896\n",
      "Epoch 114/200\n",
      "33/43 [======================>.......] - ETA: 0s - loss: 3.8028e-04 - mse: 0.2921\n",
      "Epoch 00114: val_loss did not improve from 0.02198\n",
      "43/43 [==============================] - 0s 6ms/step - loss: 4.0121e-04 - mse: 0.2971 - val_loss: 0.0957 - val_mse: 1.2584\n",
      "Epoch 115/200\n",
      "34/43 [======================>.......] - ETA: 0s - loss: 3.4768e-04 - mse: 0.3109\n",
      "Epoch 00115: val_loss did not improve from 0.02198\n",
      "43/43 [==============================] - 0s 5ms/step - loss: 3.8560e-04 - mse: 0.2970 - val_loss: 0.0987 - val_mse: 1.3241\n",
      "Epoch 116/200\n",
      "36/43 [========================>.....] - ETA: 0s - loss: 4.3725e-04 - mse: 0.3007\n",
      "Epoch 00116: val_loss did not improve from 0.02198\n",
      "43/43 [==============================] - 0s 5ms/step - loss: 3.8426e-04 - mse: 0.2970 - val_loss: 0.0963 - val_mse: 1.2759\n",
      "Epoch 117/200\n",
      "35/43 [=======================>......] - ETA: 0s - loss: 4.2624e-04 - mse: 0.2917\n",
      "Epoch 00117: val_loss did not improve from 0.02198\n",
      "43/43 [==============================] - 0s 5ms/step - loss: 3.6981e-04 - mse: 0.2968 - val_loss: 0.0978 - val_mse: 1.3082\n",
      "Epoch 118/200\n",
      "37/43 [========================>.....] - ETA: 0s - loss: 4.1335e-04 - mse: 0.2956\n",
      "Epoch 00118: val_loss did not improve from 0.02198\n",
      "43/43 [==============================] - 0s 5ms/step - loss: 3.8605e-04 - mse: 0.2970 - val_loss: 0.0964 - val_mse: 1.2774\n",
      "Epoch 119/200\n",
      "35/43 [=======================>......] - ETA: 0s - loss: 4.3267e-04 - mse: 0.2994\n",
      "Epoch 00119: val_loss did not improve from 0.02198\n",
      "43/43 [==============================] - 0s 5ms/step - loss: 4.1273e-04 - mse: 0.2969 - val_loss: 0.0981 - val_mse: 1.3143\n",
      "Epoch 120/200\n",
      "37/43 [========================>.....] - ETA: 0s - loss: 4.2723e-04 - mse: 0.2967\n",
      "Epoch 00120: val_loss did not improve from 0.02198\n",
      "43/43 [==============================] - 0s 5ms/step - loss: 4.0842e-04 - mse: 0.2969 - val_loss: 0.0987 - val_mse: 1.3172\n",
      "Epoch 121/200\n",
      "36/43 [========================>.....] - ETA: 0s - loss: 3.9307e-04 - mse: 0.2909\n",
      "Epoch 00121: val_loss did not improve from 0.02198\n",
      "43/43 [==============================] - 0s 5ms/step - loss: 3.9772e-04 - mse: 0.2968 - val_loss: 0.0997 - val_mse: 1.3464\n",
      "Epoch 122/200\n",
      "38/43 [=========================>....] - ETA: 0s - loss: 3.8101e-04 - mse: 0.3004\n",
      "Epoch 00122: val_loss did not improve from 0.02198\n",
      "43/43 [==============================] - 0s 5ms/step - loss: 4.0240e-04 - mse: 0.2968 - val_loss: 0.0973 - val_mse: 1.2985\n",
      "Epoch 123/200\n",
      "35/43 [=======================>......] - ETA: 0s - loss: 4.7055e-04 - mse: 0.3066\n",
      "Epoch 00123: val_loss did not improve from 0.02198\n",
      "43/43 [==============================] - 0s 6ms/step - loss: 4.3005e-04 - mse: 0.2969 - val_loss: 0.0992 - val_mse: 1.3377\n",
      "Epoch 124/200\n",
      "36/43 [========================>.....] - ETA: 0s - loss: 4.4488e-04 - mse: 0.2992\n",
      "Epoch 00124: val_loss did not improve from 0.02198\n",
      "43/43 [==============================] - 0s 6ms/step - loss: 4.4226e-04 - mse: 0.2969 - val_loss: 0.0991 - val_mse: 1.3356\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 125/200\n",
      "35/43 [=======================>......] - ETA: 0s - loss: 3.3768e-04 - mse: 0.2905- ETA: 0s - loss: 2.4558e-04 - mse: 0.295\n",
      "Epoch 00125: val_loss did not improve from 0.02198\n",
      "43/43 [==============================] - 0s 5ms/step - loss: 4.1229e-04 - mse: 0.2967 - val_loss: 0.0998 - val_mse: 1.3484\n",
      "Epoch 126/200\n",
      "38/43 [=========================>....] - ETA: 0s - loss: 5.9012e-04 - mse: 0.2973\n",
      "Epoch 00126: val_loss did not improve from 0.02198\n",
      "43/43 [==============================] - 0s 5ms/step - loss: 5.9096e-04 - mse: 0.2974 - val_loss: 0.1004 - val_mse: 1.3510\n",
      "Epoch 127/200\n",
      "39/43 [==========================>...] - ETA: 0s - loss: 5.1221e-04 - mse: 0.3012\n",
      "Epoch 00127: val_loss did not improve from 0.02198\n",
      "43/43 [==============================] - 0s 5ms/step - loss: 5.0776e-04 - mse: 0.2971 - val_loss: 0.1009 - val_mse: 1.3649\n",
      "Epoch 128/200\n",
      "36/43 [========================>.....] - ETA: 0s - loss: 6.3168e-04 - mse: 0.3127\n",
      "Epoch 00128: val_loss did not improve from 0.02198\n",
      "43/43 [==============================] - 0s 7ms/step - loss: 5.9273e-04 - mse: 0.2981 - val_loss: 0.0980 - val_mse: 1.2822\n",
      "Epoch 129/200\n",
      "39/43 [==========================>...] - ETA: 0s - loss: 5.4439e-04 - mse: 0.3057\n",
      "Epoch 00129: val_loss did not improve from 0.02198\n",
      "43/43 [==============================] - 0s 5ms/step - loss: 5.7067e-04 - mse: 0.2978 - val_loss: 0.1003 - val_mse: 1.3374\n",
      "Epoch 130/200\n",
      "38/43 [=========================>....] - ETA: 0s - loss: 4.1728e-04 - mse: 0.2897\n",
      "Epoch 00130: val_loss did not improve from 0.02198\n",
      "43/43 [==============================] - 0s 5ms/step - loss: 4.4363e-04 - mse: 0.2971 - val_loss: 0.0983 - val_mse: 1.3042\n",
      "Epoch 131/200\n",
      "38/43 [=========================>....] - ETA: 0s - loss: 4.0085e-04 - mse: 0.2986\n",
      "Epoch 00131: val_loss did not improve from 0.02198\n",
      "43/43 [==============================] - 0s 5ms/step - loss: 3.7806e-04 - mse: 0.2969 - val_loss: 0.0998 - val_mse: 1.3421\n",
      "Epoch 132/200\n",
      "37/43 [========================>.....] - ETA: 0s - loss: 3.8517e-04 - mse: 0.2960\n",
      "Epoch 00132: val_loss did not improve from 0.02198\n",
      "43/43 [==============================] - 0s 5ms/step - loss: 3.6339e-04 - mse: 0.2970 - val_loss: 0.1007 - val_mse: 1.3601\n",
      "Epoch 133/200\n",
      "36/43 [========================>.....] - ETA: 0s - loss: 3.9647e-04 - mse: 0.2975\n",
      "Epoch 00133: val_loss did not improve from 0.02198\n",
      "43/43 [==============================] - 0s 5ms/step - loss: 3.5779e-04 - mse: 0.2967 - val_loss: 0.0994 - val_mse: 1.3341\n",
      "Epoch 134/200\n",
      "36/43 [========================>.....] - ETA: 0s - loss: 3.7886e-04 - mse: 0.2935\n",
      "Epoch 00134: val_loss did not improve from 0.02198\n",
      "43/43 [==============================] - 0s 5ms/step - loss: 3.4970e-04 - mse: 0.2966 - val_loss: 0.0998 - val_mse: 1.3457\n",
      "Epoch 135/200\n",
      "37/43 [========================>.....] - ETA: 0s - loss: 3.7133e-04 - mse: 0.3017\n",
      "Epoch 00135: val_loss did not improve from 0.02198\n",
      "43/43 [==============================] - 0s 5ms/step - loss: 3.4373e-04 - mse: 0.2965 - val_loss: 0.0998 - val_mse: 1.3471\n",
      "Epoch 136/200\n",
      "34/43 [======================>.......] - ETA: 0s - loss: 3.6905e-04 - mse: 0.3015\n",
      "Epoch 00136: val_loss did not improve from 0.02198\n",
      "43/43 [==============================] - 0s 5ms/step - loss: 3.1378e-04 - mse: 0.2966 - val_loss: 0.1013 - val_mse: 1.3744\n",
      "Epoch 137/200\n",
      "35/43 [=======================>......] - ETA: 0s - loss: 3.5854e-04 - mse: 0.3083\n",
      "Epoch 00137: val_loss did not improve from 0.02198\n",
      "43/43 [==============================] - 0s 5ms/step - loss: 3.1866e-04 - mse: 0.2966 - val_loss: 0.0988 - val_mse: 1.3188\n",
      "Epoch 138/200\n",
      "41/43 [===========================>..] - ETA: 0s - loss: 3.4418e-04 - mse: 0.2967\n",
      "Epoch 00138: val_loss did not improve from 0.02198\n",
      "43/43 [==============================] - 0s 6ms/step - loss: 3.3386e-04 - mse: 0.2966 - val_loss: 0.1010 - val_mse: 1.3681\n",
      "Epoch 139/200\n",
      "36/43 [========================>.....] - ETA: 0s - loss: 3.7806e-04 - mse: 0.2880\n",
      "Epoch 00139: val_loss did not improve from 0.02198\n",
      "43/43 [==============================] - 0s 5ms/step - loss: 3.7478e-04 - mse: 0.2967 - val_loss: 0.1006 - val_mse: 1.3606\n",
      "Epoch 140/200\n",
      "35/43 [=======================>......] - ETA: 0s - loss: 5.5006e-04 - mse: 0.2951\n",
      "Epoch 00140: val_loss did not improve from 0.02198\n",
      "43/43 [==============================] - 0s 5ms/step - loss: 5.0247e-04 - mse: 0.2974 - val_loss: 0.0991 - val_mse: 1.3244\n",
      "Epoch 141/200\n",
      "35/43 [=======================>......] - ETA: 0s - loss: 5.1599e-04 - mse: 0.2835\n",
      "Epoch 00141: val_loss did not improve from 0.02198\n",
      "43/43 [==============================] - 0s 5ms/step - loss: 4.6099e-04 - mse: 0.2975 - val_loss: 0.1020 - val_mse: 1.3820\n",
      "Epoch 142/200\n",
      "35/43 [=======================>......] - ETA: 0s - loss: 5.5123e-04 - mse: 0.2996\n",
      "Epoch 00142: val_loss did not improve from 0.02198\n",
      "43/43 [==============================] - 0s 5ms/step - loss: 5.5417e-04 - mse: 0.2980 - val_loss: 0.1042 - val_mse: 1.4327\n",
      "Epoch 143/200\n",
      "38/43 [=========================>....] - ETA: 0s - loss: 6.2278e-04 - mse: 0.2962\n",
      "Epoch 00143: val_loss did not improve from 0.02198\n",
      "43/43 [==============================] - 0s 5ms/step - loss: 5.8746e-04 - mse: 0.2981 - val_loss: 0.1001 - val_mse: 1.3299\n",
      "Epoch 144/200\n",
      "36/43 [========================>.....] - ETA: 0s - loss: 4.8906e-04 - mse: 0.2952\n",
      "Epoch 00144: val_loss did not improve from 0.02198\n",
      "43/43 [==============================] - 0s 5ms/step - loss: 4.4427e-04 - mse: 0.2973 - val_loss: 0.1028 - val_mse: 1.4005\n",
      "Epoch 145/200\n",
      "35/43 [=======================>......] - ETA: 0s - loss: 4.0894e-04 - mse: 0.3022\n",
      "Epoch 00145: val_loss did not improve from 0.02198\n",
      "43/43 [==============================] - 0s 5ms/step - loss: 4.0962e-04 - mse: 0.2968 - val_loss: 0.1036 - val_mse: 1.4216\n",
      "Epoch 146/200\n",
      "36/43 [========================>.....] - ETA: 0s - loss: 4.0597e-04 - mse: 0.3028\n",
      "Epoch 00146: val_loss did not improve from 0.02198\n",
      "43/43 [==============================] - 0s 5ms/step - loss: 4.0933e-04 - mse: 0.2971 - val_loss: 0.1014 - val_mse: 1.3672\n",
      "Epoch 147/200\n",
      "35/43 [=======================>......] - ETA: 0s - loss: 4.1728e-04 - mse: 0.2914\n",
      "Epoch 00147: val_loss did not improve from 0.02198\n",
      "43/43 [==============================] - 0s 5ms/step - loss: 4.0854e-04 - mse: 0.2969 - val_loss: 0.1031 - val_mse: 1.3994\n",
      "Epoch 148/200\n",
      "36/43 [========================>.....] - ETA: 0s - loss: 3.6852e-04 - mse: 0.2973\n",
      "Epoch 00148: val_loss did not improve from 0.02198\n",
      "43/43 [==============================] - 0s 5ms/step - loss: 3.9562e-04 - mse: 0.2967 - val_loss: 0.1028 - val_mse: 1.3981\n",
      "Epoch 149/200\n",
      "35/43 [=======================>......] - ETA: 0s - loss: 4.0367e-04 - mse: 0.2855\n",
      "Epoch 00149: val_loss did not improve from 0.02198\n",
      "43/43 [==============================] - 0s 6ms/step - loss: 3.5232e-04 - mse: 0.2967 - val_loss: 0.1032 - val_mse: 1.4206\n",
      "Epoch 150/200\n",
      "36/43 [========================>.....] - ETA: 0s - loss: 3.6558e-04 - mse: 0.2940\n",
      "Epoch 00150: val_loss did not improve from 0.02198\n",
      "43/43 [==============================] - 0s 5ms/step - loss: 3.6722e-04 - mse: 0.2969 - val_loss: 0.1017 - val_mse: 1.3845\n",
      "Epoch 151/200\n",
      "38/43 [=========================>....] - ETA: 0s - loss: 3.7783e-04 - mse: 0.2982\n",
      "Epoch 00151: val_loss did not improve from 0.02198\n",
      "43/43 [==============================] - 0s 5ms/step - loss: 3.4358e-04 - mse: 0.2969 - val_loss: 0.1015 - val_mse: 1.3675\n",
      "Epoch 152/200\n",
      "35/43 [=======================>......] - ETA: 0s - loss: 3.0398e-04 - mse: 0.3068\n",
      "Epoch 00152: val_loss did not improve from 0.02198\n",
      "43/43 [==============================] - 0s 6ms/step - loss: 3.2735e-04 - mse: 0.2967 - val_loss: 0.1032 - val_mse: 1.4125\n",
      "Epoch 153/200\n",
      "37/43 [========================>.....] - ETA: 0s - loss: 3.0820e-04 - mse: 0.2994\n",
      "Epoch 00153: val_loss did not improve from 0.02198\n",
      "43/43 [==============================] - 0s 5ms/step - loss: 3.1568e-04 - mse: 0.2966 - val_loss: 0.1003 - val_mse: 1.3380\n",
      "Epoch 154/200\n",
      "35/43 [=======================>......] - ETA: 0s - loss: 2.4917e-04 - mse: 0.2998\n",
      "Epoch 00154: val_loss did not improve from 0.02198\n",
      "43/43 [==============================] - 0s 6ms/step - loss: 3.9006e-04 - mse: 0.2971 - val_loss: 0.1037 - val_mse: 1.4258\n",
      "Epoch 155/200\n",
      "43/43 [==============================] - ETA: 0s - loss: 3.4726e-04 - mse: 0.2967\n",
      "Epoch 00155: val_loss did not improve from 0.02198\n",
      "43/43 [==============================] - 0s 7ms/step - loss: 3.4726e-04 - mse: 0.2967 - val_loss: 0.1056 - val_mse: 1.4623\n",
      "Epoch 156/200\n",
      "41/43 [===========================>..] - ETA: 0s - loss: 3.4910e-04 - mse: 0.2947\n",
      "Epoch 00156: val_loss did not improve from 0.02198\n",
      "43/43 [==============================] - 0s 6ms/step - loss: 3.5010e-04 - mse: 0.2965 - val_loss: 0.1042 - val_mse: 1.4319\n",
      "Epoch 157/200\n",
      "34/43 [======================>.......] - ETA: 0s - loss: 2.9930e-04 - mse: 0.2892\n",
      "Epoch 00157: val_loss did not improve from 0.02198\n",
      "43/43 [==============================] - 0s 6ms/step - loss: 3.2230e-04 - mse: 0.2964 - val_loss: 0.1049 - val_mse: 1.4580\n",
      "Epoch 158/200\n",
      "37/43 [========================>.....] - ETA: 0s - loss: 2.9193e-04 - mse: 0.2963\n",
      "Epoch 00158: val_loss did not improve from 0.02198\n",
      "43/43 [==============================] - 0s 8ms/step - loss: 2.9314e-04 - mse: 0.2963 - val_loss: 0.1026 - val_mse: 1.4043\n",
      "Epoch 159/200\n",
      "41/43 [===========================>..] - ETA: 0s - loss: 2.9510e-04 - mse: 0.2961\n",
      "Epoch 00159: val_loss did not improve from 0.02198\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 2.8881e-04 - mse: 0.2962 - val_loss: 0.1056 - val_mse: 1.4746\n",
      "Epoch 160/200\n",
      "36/43 [========================>.....] - ETA: 0s - loss: 3.2652e-04 - mse: 0.2913\n",
      "Epoch 00160: val_loss did not improve from 0.02198\n",
      "43/43 [==============================] - 0s 7ms/step - loss: 2.8548e-04 - mse: 0.2963 - val_loss: 0.1038 - val_mse: 1.4366\n",
      "Epoch 161/200\n",
      "40/43 [==========================>...] - ETA: 0s - loss: 3.2330e-04 - mse: 0.2972\n",
      "Epoch 00161: val_loss did not improve from 0.02198\n",
      "43/43 [==============================] - 0s 6ms/step - loss: 3.0978e-04 - mse: 0.2964 - val_loss: 0.1052 - val_mse: 1.4624\n",
      "Epoch 162/200\n",
      "35/43 [=======================>......] - ETA: 0s - loss: 2.8770e-04 - mse: 0.2849\n",
      "Epoch 00162: val_loss did not improve from 0.02198\n",
      "43/43 [==============================] - 0s 7ms/step - loss: 3.2842e-04 - mse: 0.2965 - val_loss: 0.1055 - val_mse: 1.4709\n",
      "Epoch 163/200\n",
      "39/43 [==========================>...] - ETA: 0s - loss: 3.2656e-04 - mse: 0.2959\n",
      "Epoch 00163: val_loss did not improve from 0.02198\n",
      "43/43 [==============================] - 0s 7ms/step - loss: 3.1796e-04 - mse: 0.2964 - val_loss: 0.1037 - val_mse: 1.4362\n",
      "Epoch 164/200\n",
      "37/43 [========================>.....] - ETA: 0s - loss: 3.4307e-04 - mse: 0.2937\n",
      "Epoch 00164: val_loss did not improve from 0.02198\n",
      "43/43 [==============================] - 0s 8ms/step - loss: 3.0619e-04 - mse: 0.2965 - val_loss: 0.1052 - val_mse: 1.4652\n",
      "Epoch 165/200\n",
      "40/43 [==========================>...] - ETA: 0s - loss: 3.0725e-04 - mse: 0.2968\n",
      "Epoch 00165: val_loss did not improve from 0.02198\n",
      "43/43 [==============================] - 0s 7ms/step - loss: 2.9287e-04 - mse: 0.2963 - val_loss: 0.1039 - val_mse: 1.4404\n",
      "Epoch 166/200\n",
      "40/43 [==========================>...] - ETA: 0s - loss: 2.0727e-04 - mse: 0.3007- ETA: 0s - loss: 2.2260e-04 - mse: 0.2\n",
      "Epoch 00166: val_loss did not improve from 0.02198\n",
      "43/43 [==============================] - 0s 7ms/step - loss: 2.8980e-04 - mse: 0.2962 - val_loss: 0.1054 - val_mse: 1.4708\n",
      "Epoch 167/200\n",
      "41/43 [===========================>..] - ETA: 0s - loss: 2.7896e-04 - mse: 0.2974\n",
      "Epoch 00167: val_loss did not improve from 0.02198\n",
      "43/43 [==============================] - 0s 6ms/step - loss: 2.8370e-04 - mse: 0.2962 - val_loss: 0.1045 - val_mse: 1.4519\n",
      "Epoch 168/200\n",
      "38/43 [=========================>....] - ETA: 0s - loss: 3.0548e-04 - mse: 0.2889\n",
      "Epoch 00168: val_loss did not improve from 0.02198\n",
      "43/43 [==============================] - 0s 7ms/step - loss: 2.8199e-04 - mse: 0.2962 - val_loss: 0.1037 - val_mse: 1.4323\n",
      "Epoch 169/200\n",
      "42/43 [============================>.] - ETA: 0s - loss: 2.9671e-04 - mse: 0.2942\n",
      "Epoch 00169: val_loss did not improve from 0.02198\n",
      "43/43 [==============================] - 0s 8ms/step - loss: 2.9400e-04 - mse: 0.2963 - val_loss: 0.1044 - val_mse: 1.4517\n",
      "Epoch 170/200\n",
      "41/43 [===========================>..] - ETA: 0s - loss: 3.0624e-04 - mse: 0.2900\n",
      "Epoch 00170: val_loss did not improve from 0.02198\n",
      "43/43 [==============================] - 0s 7ms/step - loss: 2.9677e-04 - mse: 0.2963 - val_loss: 0.1049 - val_mse: 1.4578\n",
      "Epoch 171/200\n",
      "43/43 [==============================] - ETA: 0s - loss: 3.0435e-04 - mse: 0.2963- ETA: 0s - loss: 4.2082e-04 - mse: 0.29\n",
      "Epoch 00171: val_loss did not improve from 0.02198\n",
      "43/43 [==============================] - 0s 6ms/step - loss: 3.0435e-04 - mse: 0.2963 - val_loss: 0.1037 - val_mse: 1.4334\n",
      "Epoch 172/200\n",
      "34/43 [======================>.......] - ETA: 0s - loss: 2.4097e-04 - mse: 0.2954\n",
      "Epoch 00172: val_loss did not improve from 0.02198\n",
      "43/43 [==============================] - 0s 6ms/step - loss: 3.1094e-04 - mse: 0.2964 - val_loss: 0.1060 - val_mse: 1.4927\n",
      "Epoch 173/200\n",
      "32/43 [=====================>........] - ETA: 0s - loss: 4.2577e-04 - mse: 0.2875\n",
      "Epoch 00173: val_loss did not improve from 0.02198\n",
      "43/43 [==============================] - 0s 6ms/step - loss: 3.6877e-04 - mse: 0.2970 - val_loss: 0.1072 - val_mse: 1.5169\n",
      "Epoch 174/200\n",
      "33/43 [======================>.......] - ETA: 0s - loss: 7.1490e-04 - mse: 0.3166\n",
      "Epoch 00174: val_loss did not improve from 0.02198\n",
      "43/43 [==============================] - 0s 6ms/step - loss: 6.7783e-04 - mse: 0.2994 - val_loss: 0.1086 - val_mse: 1.5513\n",
      "Epoch 175/200\n",
      "39/43 [==========================>...] - ETA: 0s - loss: 0.0011 - mse: 0.3010 \n",
      "Epoch 00175: val_loss did not improve from 0.02198\n",
      "43/43 [==============================] - 0s 7ms/step - loss: 0.0011 - mse: 0.3034 - val_loss: 0.1017 - val_mse: 1.3375\n",
      "Epoch 176/200\n",
      "38/43 [=========================>....] - ETA: 0s - loss: 0.0014 - mse: 0.3100  \n",
      "Epoch 00176: val_loss did not improve from 0.02198\n",
      "43/43 [==============================] - 0s 5ms/step - loss: 0.0015 - mse: 0.3069 - val_loss: 0.0985 - val_mse: 1.3198\n",
      "Epoch 177/200\n",
      "42/43 [============================>.] - ETA: 0s - loss: 0.0018 - mse: 0.3032\n",
      "Epoch 00177: val_loss did not improve from 0.02198\n",
      "43/43 [==============================] - 0s 7ms/step - loss: 0.0018 - mse: 0.3053 - val_loss: 0.1015 - val_mse: 1.3643\n",
      "Epoch 178/200\n",
      "32/43 [=====================>........] - ETA: 0s - loss: 0.0023 - mse: 0.3083  \n",
      "Epoch 00178: val_loss did not improve from 0.02198\n",
      "43/43 [==============================] - 0s 6ms/step - loss: 0.0023 - mse: 0.3129 - val_loss: 0.1001 - val_mse: 1.3672\n",
      "Epoch 179/200\n",
      "34/43 [======================>.......] - ETA: 0s - loss: 0.0022 - mse: 0.3060\n",
      "Epoch 00179: val_loss did not improve from 0.02198\n",
      "43/43 [==============================] - 0s 5ms/step - loss: 0.0025 - mse: 0.3095 - val_loss: 0.1029 - val_mse: 1.4307\n",
      "Epoch 180/200\n",
      "32/43 [=====================>........] - ETA: 0s - loss: 0.0031 - mse: 0.3167\n",
      "Epoch 00180: val_loss did not improve from 0.02198\n",
      "43/43 [==============================] - 0s 7ms/step - loss: 0.0026 - mse: 0.3112 - val_loss: 0.0953 - val_mse: 1.2227\n",
      "Epoch 181/200\n",
      "42/43 [============================>.] - ETA: 0s - loss: 0.0012 - mse: 0.3021   \n",
      "Epoch 00181: val_loss did not improve from 0.02198\n",
      "43/43 [==============================] - 0s 6ms/step - loss: 0.0012 - mse: 0.3019 - val_loss: 0.0977 - val_mse: 1.2871\n",
      "Epoch 182/200\n",
      "34/43 [======================>.......] - ETA: 0s - loss: 6.8740e-04 - mse: 0.3009\n",
      "Epoch 00182: val_loss did not improve from 0.02198\n",
      "43/43 [==============================] - 0s 6ms/step - loss: 7.9825e-04 - mse: 0.2996 - val_loss: 0.1025 - val_mse: 1.4031\n",
      "Epoch 183/200\n",
      "35/43 [=======================>......] - ETA: 0s - loss: 3.8520e-04 - mse: 0.2894\n",
      "Epoch 00183: val_loss did not improve from 0.02198\n",
      "43/43 [==============================] - 0s 6ms/step - loss: 4.8348e-04 - mse: 0.2974 - val_loss: 0.1051 - val_mse: 1.4524\n",
      "Epoch 184/200\n",
      "37/43 [========================>.....] - ETA: 0s - loss: 4.5841e-04 - mse: 0.2981\n",
      "Epoch 00184: val_loss did not improve from 0.02198\n",
      "43/43 [==============================] - 0s 6ms/step - loss: 4.2687e-04 - mse: 0.2971 - val_loss: 0.1023 - val_mse: 1.3929\n",
      "Epoch 185/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "35/43 [=======================>......] - ETA: 0s - loss: 4.2443e-04 - mse: 0.2938\n",
      "Epoch 00185: val_loss did not improve from 0.02198\n",
      "43/43 [==============================] - 0s 5ms/step - loss: 3.6975e-04 - mse: 0.2971 - val_loss: 0.1049 - val_mse: 1.4576\n",
      "Epoch 186/200\n",
      "38/43 [=========================>....] - ETA: 0s - loss: 3.4770e-04 - mse: 0.3051\n",
      "Epoch 00186: val_loss did not improve from 0.02198\n",
      "43/43 [==============================] - 0s 7ms/step - loss: 3.3660e-04 - mse: 0.2966 - val_loss: 0.1047 - val_mse: 1.4520\n",
      "Epoch 187/200\n",
      "39/43 [==========================>...] - ETA: 0s - loss: 1.3874e-04 - mse: 0.2996\n",
      "Epoch 00187: val_loss did not improve from 0.02198\n",
      "43/43 [==============================] - 0s 5ms/step - loss: 3.0025e-04 - mse: 0.2963 - val_loss: 0.1059 - val_mse: 1.4773\n",
      "Epoch 188/200\n",
      "38/43 [=========================>....] - ETA: 0s - loss: 2.8230e-04 - mse: 0.3003\n",
      "Epoch 00188: val_loss did not improve from 0.02198\n",
      "43/43 [==============================] - 0s 7ms/step - loss: 2.8859e-04 - mse: 0.2963 - val_loss: 0.1064 - val_mse: 1.4938\n",
      "Epoch 189/200\n",
      "34/43 [======================>.......] - ETA: 0s - loss: 2.4478e-04 - mse: 0.3043\n",
      "Epoch 00189: val_loss did not improve from 0.02198\n",
      "43/43 [==============================] - 0s 5ms/step - loss: 2.8109e-04 - mse: 0.2962 - val_loss: 0.1075 - val_mse: 1.5204\n",
      "Epoch 190/200\n",
      "40/43 [==========================>...] - ETA: 0s - loss: 2.9679e-04 - mse: 0.2934\n",
      "Epoch 00190: val_loss did not improve from 0.02198\n",
      "43/43 [==============================] - 0s 7ms/step - loss: 2.8224e-04 - mse: 0.2963 - val_loss: 0.1073 - val_mse: 1.5105\n",
      "Epoch 191/200\n",
      "42/43 [============================>.] - ETA: 0s - loss: 2.8253e-04 - mse: 0.2969\n",
      "Epoch 00191: val_loss did not improve from 0.02198\n",
      "43/43 [==============================] - 0s 6ms/step - loss: 2.8016e-04 - mse: 0.2963 - val_loss: 0.1058 - val_mse: 1.4738\n",
      "Epoch 192/200\n",
      "42/43 [============================>.] - ETA: 0s - loss: 2.7077e-04 - mse: 0.2975\n",
      "Epoch 00192: val_loss did not improve from 0.02198\n",
      "43/43 [==============================] - 0s 6ms/step - loss: 2.7377e-04 - mse: 0.2962 - val_loss: 0.1067 - val_mse: 1.4979\n",
      "Epoch 193/200\n",
      "34/43 [======================>.......] - ETA: 0s - loss: 1.9065e-04 - mse: 0.2961\n",
      "Epoch 00193: val_loss did not improve from 0.02198\n",
      "43/43 [==============================] - 0s 6ms/step - loss: 2.7011e-04 - mse: 0.2961 - val_loss: 0.1059 - val_mse: 1.4795\n",
      "Epoch 194/200\n",
      "41/43 [===========================>..] - ETA: 0s - loss: 2.7327e-04 - mse: 0.2965\n",
      "Epoch 00194: val_loss did not improve from 0.02198\n",
      "43/43 [==============================] - 0s 7ms/step - loss: 2.7042e-04 - mse: 0.2961 - val_loss: 0.1067 - val_mse: 1.5015\n",
      "Epoch 195/200\n",
      "42/43 [============================>.] - ETA: 0s - loss: 2.7504e-04 - mse: 0.2968\n",
      "Epoch 00195: val_loss did not improve from 0.02198\n",
      "43/43 [==============================] - 0s 6ms/step - loss: 2.7256e-04 - mse: 0.2961 - val_loss: 0.1064 - val_mse: 1.4906\n",
      "Epoch 196/200\n",
      "39/43 [==========================>...] - ETA: 0s - loss: 2.9401e-04 - mse: 0.2955\n",
      "Epoch 00196: val_loss did not improve from 0.02198\n",
      "43/43 [==============================] - 0s 7ms/step - loss: 2.7277e-04 - mse: 0.2961 - val_loss: 0.1069 - val_mse: 1.5099\n",
      "Epoch 197/200\n",
      "34/43 [======================>.......] - ETA: 0s - loss: 2.7599e-04 - mse: 0.2993\n",
      "Epoch 00197: val_loss did not improve from 0.02198\n",
      "43/43 [==============================] - 0s 6ms/step - loss: 2.6963e-04 - mse: 0.2961 - val_loss: 0.1064 - val_mse: 1.4972\n",
      "Epoch 198/200\n",
      "40/43 [==========================>...] - ETA: 0s - loss: 2.5983e-04 - mse: 0.2999\n",
      "Epoch 00198: val_loss did not improve from 0.02198\n",
      "43/43 [==============================] - 0s 7ms/step - loss: 2.8778e-04 - mse: 0.2963 - val_loss: 0.0957 - val_mse: 1.2563\n",
      "Epoch 199/200\n",
      "36/43 [========================>.....] - ETA: 0s - loss: 3.5062e-04 - mse: 0.2917\n",
      "Epoch 00199: val_loss did not improve from 0.02198\n",
      "43/43 [==============================] - 0s 6ms/step - loss: 3.5168e-04 - mse: 0.2966 - val_loss: 0.0944 - val_mse: 1.2372\n",
      "Epoch 200/200\n",
      "40/43 [==========================>...] - ETA: 0s - loss: 3.3851e-04 - mse: 0.2887\n",
      "Epoch 00200: val_loss did not improve from 0.02198\n",
      "43/43 [==============================] - 0s 7ms/step - loss: 3.2221e-04 - mse: 0.2964 - val_loss: 0.0964 - val_mse: 1.2757\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#Compiling the ANN\n",
    "opt = keras.optimizers.Adam(lr=0.0015, beta_1=0.9, beta_2=0.999, epsilon=None, decay=0.0, amsgrad=False)\n",
    "model.compile(optimizer=opt, loss='mean_squared_logarithmic_error', metrics=['mse'])\n",
    "#Fitting the ANN to the training set\n",
    "model_filepath = 'min_vl_model.h5'\n",
    "checkpoint = ModelCheckpoint(model_filepath, monitor = 'val_loss', verbose=1, save_best_only = True, mode='min' )\n",
    "model.fit(X_train,y_train, validation_split=0.07, batch_size=32, epochs=200, callbacks=[checkpoint])\n",
    "model.load_weights(model_filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "modified-workplace",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_6\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_36 (Dense)             (None, 300)               93900     \n",
      "_________________________________________________________________\n",
      "dropout_10 (Dropout)         (None, 300)               0         \n",
      "_________________________________________________________________\n",
      "dense_37 (Dense)             (None, 100)               30100     \n",
      "_________________________________________________________________\n",
      "dropout_11 (Dropout)         (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_38 (Dense)             (None, 80)                8080      \n",
      "_________________________________________________________________\n",
      "batch_normalization_13 (Batc (None, 80)                320       \n",
      "_________________________________________________________________\n",
      "dropout_12 (Dropout)         (None, 80)                0         \n",
      "_________________________________________________________________\n",
      "dense_39 (Dense)             (None, 50)                4050      \n",
      "_________________________________________________________________\n",
      "batch_normalization_14 (Batc (None, 50)                200       \n",
      "_________________________________________________________________\n",
      "dense_40 (Dense)             (None, 1)                 51        \n",
      "=================================================================\n",
      "Total params: 136,701\n",
      "Trainable params: 136,441\n",
      "Non-trainable params: 260\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "from keras.initializers import he_normal\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.layers import Dense, Activation \n",
    "from keras.layers import Dropout\n",
    "\n",
    "#Inıtialising the ANN\n",
    "model = Sequential()\n",
    "#Adding the input layer and first hidden layer\n",
    "model.add(Dense(units =300, kernel_initializer=he_normal(seed=None), activation= 'tanh', \n",
    "                input_dim=X_train.shape[1]))\n",
    "\n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "#Add the first hidden layer\n",
    "model.add(Dense(units =100, kernel_initializer=he_normal(seed=None), activation= 'tanh'))\n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "#Add the second hidden layer\n",
    "model.add(Dense(units =80, kernel_initializer=he_normal(seed=None), activation= 'relu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "#Add the third hidden layer\n",
    "model.add(Dense(units =50, kernel_initializer=he_normal(seed=None), activation= 'relu'))\n",
    "model.add(BatchNormalization())\n",
    "\n",
    "#The output layer\n",
    "model.add(Dense(units =1, kernel_initializer=he_normal(seed=None), activation= 'relu'))\n",
    "\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "infectious-briefs",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "40/43 [==========================>...] - ETA: 0s - loss: 0.1150 - mse: 0.8920\n",
      "Epoch 00001: val_loss improved from inf to 0.03156, saving model to min_vl_model3.h5\n",
      "43/43 [==============================] - 1s 18ms/step - loss: 0.1116 - mse: 0.8731 - val_loss: 0.0316 - val_mse: 0.4215\n",
      "Epoch 2/200\n",
      "42/43 [============================>.] - ETA: 0s - loss: 0.0541 - mse: 0.5656\n",
      "Epoch 00002: val_loss improved from 0.03156 to 0.02698, saving model to min_vl_model3.h5\n",
      "43/43 [==============================] - 1s 12ms/step - loss: 0.0545 - mse: 0.5686 - val_loss: 0.0270 - val_mse: 0.3930\n",
      "Epoch 3/200\n",
      "40/43 [==========================>...] - ETA: 0s - loss: 0.0530 - mse: 0.5782\n",
      "Epoch 00003: val_loss did not improve from 0.02698\n",
      "43/43 [==============================] - 0s 7ms/step - loss: 0.0529 - mse: 0.5682 - val_loss: 0.0387 - val_mse: 0.4575\n",
      "Epoch 4/200\n",
      "43/43 [==============================] - ETA: 0s - loss: 0.0445 - mse: 0.5096\n",
      "Epoch 00004: val_loss did not improve from 0.02698\n",
      "43/43 [==============================] - 0s 7ms/step - loss: 0.0445 - mse: 0.5096 - val_loss: 0.0344 - val_mse: 0.4004\n",
      "Epoch 5/200\n",
      "38/43 [=========================>....] - ETA: 0s - loss: 0.0377 - mse: 0.4986\n",
      "Epoch 00005: val_loss improved from 0.02698 to 0.02193, saving model to min_vl_model3.h5\n",
      "43/43 [==============================] - 0s 8ms/step - loss: 0.0376 - mse: 0.4922 - val_loss: 0.0219 - val_mse: 0.3626\n",
      "Epoch 6/200\n",
      "39/43 [==========================>...] - ETA: 0s - loss: 0.0317 - mse: 0.4615\n",
      "Epoch 00006: val_loss improved from 0.02193 to 0.02004, saving model to min_vl_model3.h5\n",
      "43/43 [==============================] - 0s 8ms/step - loss: 0.0304 - mse: 0.4574 - val_loss: 0.0200 - val_mse: 0.3686\n",
      "Epoch 7/200\n",
      "36/43 [========================>.....] - ETA: 0s - loss: 0.0312 - mse: 0.4910\n",
      "Epoch 00007: val_loss improved from 0.02004 to 0.01827, saving model to min_vl_model3.h5\n",
      "43/43 [==============================] - 0s 8ms/step - loss: 0.0313 - mse: 0.4793 - val_loss: 0.0183 - val_mse: 0.3681\n",
      "Epoch 8/200\n",
      "40/43 [==========================>...] - ETA: 0s - loss: 0.0279 - mse: 0.4616\n",
      "Epoch 00008: val_loss did not improve from 0.01827\n",
      "43/43 [==============================] - 0s 6ms/step - loss: 0.0279 - mse: 0.4600 - val_loss: 0.0201 - val_mse: 0.3678\n",
      "Epoch 9/200\n",
      "38/43 [=========================>....] - ETA: 0s - loss: 0.0251 - mse: 0.4444\n",
      "Epoch 00009: val_loss did not improve from 0.01827\n",
      "43/43 [==============================] - 0s 7ms/step - loss: 0.0253 - mse: 0.4402 - val_loss: 0.0187 - val_mse: 0.3671\n",
      "Epoch 10/200\n",
      "41/43 [===========================>..] - ETA: 0s - loss: 0.0246 - mse: 0.4371\n",
      "Epoch 00010: val_loss improved from 0.01827 to 0.01649, saving model to min_vl_model3.h5\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.0243 - mse: 0.4354 - val_loss: 0.0165 - val_mse: 0.3553\n",
      "Epoch 11/200\n",
      "38/43 [=========================>....] - ETA: 0s - loss: 0.0216 - mse: 0.4234\n",
      "Epoch 00011: val_loss improved from 0.01649 to 0.01619, saving model to min_vl_model3.h5\n",
      "43/43 [==============================] - 0s 8ms/step - loss: 0.0214 - mse: 0.4298 - val_loss: 0.0162 - val_mse: 0.3585\n",
      "Epoch 12/200\n",
      "40/43 [==========================>...] - ETA: 0s - loss: 0.0232 - mse: 0.4378\n",
      "Epoch 00012: val_loss did not improve from 0.01619\n",
      "43/43 [==============================] - 0s 6ms/step - loss: 0.0227 - mse: 0.4299 - val_loss: 0.0209 - val_mse: 0.3648\n",
      "Epoch 13/200\n",
      "38/43 [=========================>....] - ETA: 0s - loss: 0.0226 - mse: 0.4409\n",
      "Epoch 00013: val_loss did not improve from 0.01619\n",
      "43/43 [==============================] - 0s 6ms/step - loss: 0.0231 - mse: 0.4373 - val_loss: 0.0166 - val_mse: 0.3582\n",
      "Epoch 14/200\n",
      "36/43 [========================>.....] - ETA: 0s - loss: 0.0201 - mse: 0.4089\n",
      "Epoch 00014: val_loss did not improve from 0.01619\n",
      "43/43 [==============================] - 0s 7ms/step - loss: 0.0216 - mse: 0.4291 - val_loss: 0.0231 - val_mse: 0.3833\n",
      "Epoch 15/200\n",
      "36/43 [========================>.....] - ETA: 0s - loss: 0.0200 - mse: 0.4350\n",
      "Epoch 00015: val_loss did not improve from 0.01619\n",
      "43/43 [==============================] - 0s 7ms/step - loss: 0.0200 - mse: 0.4242 - val_loss: 0.0227 - val_mse: 0.3936\n",
      "Epoch 16/200\n",
      "37/43 [========================>.....] - ETA: 0s - loss: 0.0166 - mse: 0.3849\n",
      "Epoch 00016: val_loss did not improve from 0.01619\n",
      "43/43 [==============================] - 0s 7ms/step - loss: 0.0181 - mse: 0.4047 - val_loss: 0.0206 - val_mse: 0.3838\n",
      "Epoch 17/200\n",
      "43/43 [==============================] - ETA: 0s - loss: 0.0168 - mse: 0.3956\n",
      "Epoch 00017: val_loss did not improve from 0.01619\n",
      "43/43 [==============================] - 0s 7ms/step - loss: 0.0168 - mse: 0.3956 - val_loss: 0.0209 - val_mse: 0.3834\n",
      "Epoch 18/200\n",
      "38/43 [=========================>....] - ETA: 0s - loss: 0.0159 - mse: 0.3762\n",
      "Epoch 00018: val_loss did not improve from 0.01619\n",
      "43/43 [==============================] - 0s 6ms/step - loss: 0.0170 - mse: 0.4033 - val_loss: 0.0213 - val_mse: 0.3878\n",
      "Epoch 19/200\n",
      "43/43 [==============================] - ETA: 0s - loss: 0.0147 - mse: 0.3888\n",
      "Epoch 00019: val_loss did not improve from 0.01619\n",
      "43/43 [==============================] - 0s 8ms/step - loss: 0.0147 - mse: 0.3888 - val_loss: 0.0200 - val_mse: 0.3828\n",
      "Epoch 20/200\n",
      "37/43 [========================>.....] - ETA: 0s - loss: 0.0154 - mse: 0.3854\n",
      "Epoch 00020: val_loss did not improve from 0.01619\n",
      "43/43 [==============================] - 0s 7ms/step - loss: 0.0149 - mse: 0.3898 - val_loss: 0.0258 - val_mse: 0.4049\n",
      "Epoch 21/200\n",
      "37/43 [========================>.....] - ETA: 0s - loss: 0.0170 - mse: 0.4155\n",
      "Epoch 00021: val_loss did not improve from 0.01619\n",
      "43/43 [==============================] - 0s 6ms/step - loss: 0.0169 - mse: 0.4090 - val_loss: 0.0280 - val_mse: 0.4176\n",
      "Epoch 22/200\n",
      "35/43 [=======================>......] - ETA: 0s - loss: 0.0161 - mse: 0.4041\n",
      "Epoch 00022: val_loss did not improve from 0.01619\n",
      "43/43 [==============================] - 0s 7ms/step - loss: 0.0157 - mse: 0.3980 - val_loss: 0.0280 - val_mse: 0.4299\n",
      "Epoch 23/200\n",
      "42/43 [============================>.] - ETA: 0s - loss: 0.0144 - mse: 0.3879\n",
      "Epoch 00023: val_loss did not improve from 0.01619\n",
      "43/43 [==============================] - 0s 7ms/step - loss: 0.0144 - mse: 0.3887 - val_loss: 0.0312 - val_mse: 0.4489\n",
      "Epoch 24/200\n",
      "37/43 [========================>.....] - ETA: 0s - loss: 0.0147 - mse: 0.4093\n",
      "Epoch 00024: val_loss did not improve from 0.01619\n",
      "43/43 [==============================] - 0s 7ms/step - loss: 0.0145 - mse: 0.4010 - val_loss: 0.0340 - val_mse: 0.4636\n",
      "Epoch 25/200\n",
      "35/43 [=======================>......] - ETA: 0s - loss: 0.0123 - mse: 0.3765\n",
      "Epoch 00025: val_loss did not improve from 0.01619\n",
      "43/43 [==============================] - 0s 7ms/step - loss: 0.0117 - mse: 0.3811 - val_loss: 0.0360 - val_mse: 0.4838\n",
      "Epoch 26/200\n",
      "39/43 [==========================>...] - ETA: 0s - loss: 0.0141 - mse: 0.4054\n",
      "Epoch 00026: val_loss did not improve from 0.01619\n",
      "43/43 [==============================] - 0s 8ms/step - loss: 0.0139 - mse: 0.4009 - val_loss: 0.0358 - val_mse: 0.4842\n",
      "Epoch 27/200\n",
      "36/43 [========================>.....] - ETA: 0s - loss: 0.0127 - mse: 0.3945\n",
      "Epoch 00027: val_loss did not improve from 0.01619\n",
      "43/43 [==============================] - 0s 7ms/step - loss: 0.0126 - mse: 0.3828 - val_loss: 0.0386 - val_mse: 0.5160\n",
      "Epoch 28/200\n",
      "43/43 [==============================] - ETA: 0s - loss: 0.0118 - mse: 0.3810\n",
      "Epoch 00028: val_loss did not improve from 0.01619\n",
      "43/43 [==============================] - 0s 8ms/step - loss: 0.0118 - mse: 0.3810 - val_loss: 0.0414 - val_mse: 0.5230\n",
      "Epoch 29/200\n",
      "35/43 [=======================>......] - ETA: 0s - loss: 0.0108 - mse: 0.3715\n",
      "Epoch 00029: val_loss did not improve from 0.01619\n",
      "43/43 [==============================] - 0s 7ms/step - loss: 0.0109 - mse: 0.3688 - val_loss: 0.0467 - val_mse: 0.5373\n",
      "Epoch 30/200\n",
      "41/43 [===========================>..] - ETA: 0s - loss: 0.0111 - mse: 0.3709\n",
      "Epoch 00030: val_loss did not improve from 0.01619\n",
      "43/43 [==============================] - 0s 8ms/step - loss: 0.0113 - mse: 0.3699 - val_loss: 0.0503 - val_mse: 0.5823\n",
      "Epoch 31/200\n",
      "40/43 [==========================>...] - ETA: 0s - loss: 0.0104 - mse: 0.3623\n",
      "Epoch 00031: val_loss did not improve from 0.01619\n",
      "43/43 [==============================] - 0s 8ms/step - loss: 0.0103 - mse: 0.3632 - val_loss: 0.0498 - val_mse: 0.5740\n",
      "Epoch 32/200\n",
      "42/43 [============================>.] - ETA: 0s - loss: 0.0122 - mse: 0.3758\n",
      "Epoch 00032: val_loss did not improve from 0.01619\n",
      "43/43 [==============================] - 0s 8ms/step - loss: 0.0122 - mse: 0.3805 - val_loss: 0.0528 - val_mse: 0.6020\n",
      "Epoch 33/200\n",
      "36/43 [========================>.....] - ETA: 0s - loss: 0.0115 - mse: 0.3582\n",
      "Epoch 00033: val_loss did not improve from 0.01619\n",
      "43/43 [==============================] - 0s 7ms/step - loss: 0.0114 - mse: 0.3770 - val_loss: 0.0497 - val_mse: 0.5949\n",
      "Epoch 34/200\n",
      "38/43 [=========================>....] - ETA: 0s - loss: 0.0113 - mse: 0.3723\n",
      "Epoch 00034: val_loss did not improve from 0.01619\n",
      "43/43 [==============================] - 0s 8ms/step - loss: 0.0115 - mse: 0.3703 - val_loss: 0.0492 - val_mse: 0.5893\n",
      "Epoch 35/200\n",
      "42/43 [============================>.] - ETA: 0s - loss: 0.0104 - mse: 0.3716\n",
      "Epoch 00035: val_loss did not improve from 0.01619\n",
      "43/43 [==============================] - 0s 7ms/step - loss: 0.0104 - mse: 0.3707 - val_loss: 0.0473 - val_mse: 0.5607\n",
      "Epoch 36/200\n",
      "39/43 [==========================>...] - ETA: 0s - loss: 0.0102 - mse: 0.3765\n",
      "Epoch 00036: val_loss did not improve from 0.01619\n",
      "43/43 [==============================] - 0s 6ms/step - loss: 0.0100 - mse: 0.3729 - val_loss: 0.0579 - val_mse: 0.6368\n",
      "Epoch 37/200\n",
      "41/43 [===========================>..] - ETA: 0s - loss: 0.0096 - mse: 0.3633\n",
      "Epoch 00037: val_loss did not improve from 0.01619\n",
      "43/43 [==============================] - 0s 6ms/step - loss: 0.0097 - mse: 0.3647 - val_loss: 0.0574 - val_mse: 0.6417\n",
      "Epoch 38/200\n",
      "42/43 [============================>.] - ETA: 0s - loss: 0.0093 - mse: 0.3586\n",
      "Epoch 00038: val_loss did not improve from 0.01619\n",
      "43/43 [==============================] - 0s 6ms/step - loss: 0.0092 - mse: 0.3564 - val_loss: 0.0589 - val_mse: 0.6520\n",
      "Epoch 39/200\n",
      "40/43 [==========================>...] - ETA: 0s - loss: 0.0108 - mse: 0.3655\n",
      "Epoch 00039: val_loss did not improve from 0.01619\n",
      "43/43 [==============================] - 0s 8ms/step - loss: 0.0105 - mse: 0.3657 - val_loss: 0.0536 - val_mse: 0.6211\n",
      "Epoch 40/200\n",
      "41/43 [===========================>..] - ETA: 0s - loss: 0.0088 - mse: 0.3570\n",
      "Epoch 00040: val_loss did not improve from 0.01619\n",
      "43/43 [==============================] - 0s 6ms/step - loss: 0.0095 - mse: 0.3724 - val_loss: 0.0499 - val_mse: 0.5915\n",
      "Epoch 41/200\n",
      "36/43 [========================>.....] - ETA: 0s - loss: 0.0094 - mse: 0.3474\n",
      "Epoch 00041: val_loss did not improve from 0.01619\n",
      "43/43 [==============================] - 0s 7ms/step - loss: 0.0091 - mse: 0.3609 - val_loss: 0.0530 - val_mse: 0.6189\n",
      "Epoch 42/200\n",
      "40/43 [==========================>...] - ETA: 0s - loss: 0.0086 - mse: 0.3659\n",
      "Epoch 00042: val_loss did not improve from 0.01619\n",
      "43/43 [==============================] - 0s 7ms/step - loss: 0.0087 - mse: 0.3674 - val_loss: 0.0527 - val_mse: 0.6037\n",
      "Epoch 43/200\n",
      "41/43 [===========================>..] - ETA: 0s - loss: 0.0077 - mse: 0.3589\n",
      "Epoch 00043: val_loss did not improve from 0.01619\n",
      "43/43 [==============================] - 0s 6ms/step - loss: 0.0078 - mse: 0.3571 - val_loss: 0.0583 - val_mse: 0.6588\n",
      "Epoch 44/200\n",
      "39/43 [==========================>...] - ETA: 0s - loss: 0.0069 - mse: 0.3471\n",
      "Epoch 00044: val_loss did not improve from 0.01619\n",
      "43/43 [==============================] - 0s 6ms/step - loss: 0.0071 - mse: 0.3461 - val_loss: 0.0565 - val_mse: 0.6321\n",
      "Epoch 45/200\n",
      "40/43 [==========================>...] - ETA: 0s - loss: 0.0074 - mse: 0.3443\n",
      "Epoch 00045: val_loss did not improve from 0.01619\n",
      "43/43 [==============================] - 0s 6ms/step - loss: 0.0077 - mse: 0.3519 - val_loss: 0.0583 - val_mse: 0.6431\n",
      "Epoch 46/200\n",
      "39/43 [==========================>...] - ETA: 0s - loss: 0.0090 - mse: 0.3610\n",
      "Epoch 00046: val_loss did not improve from 0.01619\n",
      "43/43 [==============================] - 0s 6ms/step - loss: 0.0092 - mse: 0.3625 - val_loss: 0.0616 - val_mse: 0.6764\n",
      "Epoch 47/200\n",
      "40/43 [==========================>...] - ETA: 0s - loss: 0.0095 - mse: 0.3641\n",
      "Epoch 00047: val_loss did not improve from 0.01619\n",
      "43/43 [==============================] - 0s 6ms/step - loss: 0.0095 - mse: 0.3637 - val_loss: 0.0603 - val_mse: 0.6952\n",
      "Epoch 48/200\n",
      "39/43 [==========================>...] - ETA: 0s - loss: 0.0090 - mse: 0.3637\n",
      "Epoch 00048: val_loss did not improve from 0.01619\n",
      "43/43 [==============================] - 0s 8ms/step - loss: 0.0087 - mse: 0.3624 - val_loss: 0.0629 - val_mse: 0.7234\n",
      "Epoch 49/200\n",
      "43/43 [==============================] - ETA: 0s - loss: 0.0075 - mse: 0.3515\n",
      "Epoch 00049: val_loss did not improve from 0.01619\n",
      "43/43 [==============================] - 0s 7ms/step - loss: 0.0075 - mse: 0.3515 - val_loss: 0.0619 - val_mse: 0.7175\n",
      "Epoch 50/200\n",
      "41/43 [===========================>..] - ETA: 0s - loss: 0.0073 - mse: 0.3538\n",
      "Epoch 00050: val_loss did not improve from 0.01619\n",
      "43/43 [==============================] - 0s 8ms/step - loss: 0.0073 - mse: 0.3544 - val_loss: 0.0601 - val_mse: 0.7095\n",
      "Epoch 51/200\n",
      "35/43 [=======================>......] - ETA: 0s - loss: 0.0079 - mse: 0.3559\n",
      "Epoch 00051: val_loss did not improve from 0.01619\n",
      "43/43 [==============================] - 0s 7ms/step - loss: 0.0084 - mse: 0.3531 - val_loss: 0.0573 - val_mse: 0.6811\n",
      "Epoch 52/200\n",
      "38/43 [=========================>....] - ETA: 0s - loss: 0.0068 - mse: 0.3372\n",
      "Epoch 00052: val_loss did not improve from 0.01619\n",
      "43/43 [==============================] - 0s 6ms/step - loss: 0.0073 - mse: 0.3493 - val_loss: 0.0539 - val_mse: 0.6539\n",
      "Epoch 53/200\n",
      "39/43 [==========================>...] - ETA: 0s - loss: 0.0064 - mse: 0.3449\n",
      "Epoch 00053: val_loss did not improve from 0.01619\n",
      "43/43 [==============================] - 0s 8ms/step - loss: 0.0063 - mse: 0.3390 - val_loss: 0.0564 - val_mse: 0.6718\n",
      "Epoch 54/200\n",
      "41/43 [===========================>..] - ETA: 0s - loss: 0.0070 - mse: 0.3498\n",
      "Epoch 00054: val_loss did not improve from 0.01619\n",
      "43/43 [==============================] - 0s 8ms/step - loss: 0.0071 - mse: 0.3521 - val_loss: 0.0608 - val_mse: 0.7261\n",
      "Epoch 55/200\n",
      "37/43 [========================>.....] - ETA: 0s - loss: 0.0077 - mse: 0.3506\n",
      "Epoch 00055: val_loss did not improve from 0.01619\n",
      "43/43 [==============================] - 0s 6ms/step - loss: 0.0077 - mse: 0.3545 - val_loss: 0.0590 - val_mse: 0.7132\n",
      "Epoch 56/200\n",
      "38/43 [=========================>....] - ETA: 0s - loss: 0.0067 - mse: 0.3545\n",
      "Epoch 00056: val_loss did not improve from 0.01619\n",
      "43/43 [==============================] - 0s 8ms/step - loss: 0.0068 - mse: 0.3491 - val_loss: 0.0561 - val_mse: 0.6890\n",
      "Epoch 57/200\n",
      "40/43 [==========================>...] - ETA: 0s - loss: 0.0079 - mse: 0.3564\n",
      "Epoch 00057: val_loss did not improve from 0.01619\n",
      "43/43 [==============================] - 0s 8ms/step - loss: 0.0078 - mse: 0.3524 - val_loss: 0.0572 - val_mse: 0.6854\n",
      "Epoch 58/200\n",
      "35/43 [=======================>......] - ETA: 0s - loss: 0.0073 - mse: 0.3470\n",
      "Epoch 00058: val_loss did not improve from 0.01619\n",
      "43/43 [==============================] - 0s 7ms/step - loss: 0.0069 - mse: 0.3453 - val_loss: 0.0564 - val_mse: 0.6779\n",
      "Epoch 59/200\n",
      "35/43 [=======================>......] - ETA: 0s - loss: 0.0069 - mse: 0.3564\n",
      "Epoch 00059: val_loss did not improve from 0.01619\n",
      "43/43 [==============================] - 0s 7ms/step - loss: 0.0074 - mse: 0.3544 - val_loss: 0.0648 - val_mse: 0.7564\n",
      "Epoch 60/200\n",
      "37/43 [========================>.....] - ETA: 0s - loss: 0.0069 - mse: 0.3528\n",
      "Epoch 00060: val_loss did not improve from 0.01619\n",
      "43/43 [==============================] - 0s 6ms/step - loss: 0.0070 - mse: 0.3492 - val_loss: 0.0594 - val_mse: 0.7181\n",
      "Epoch 61/200\n",
      "41/43 [===========================>..] - ETA: 0s - loss: 0.0065 - mse: 0.3447\n",
      "Epoch 00061: val_loss did not improve from 0.01619\n",
      "43/43 [==============================] - 0s 8ms/step - loss: 0.0069 - mse: 0.3518 - val_loss: 0.0583 - val_mse: 0.7055\n",
      "Epoch 62/200\n",
      "41/43 [===========================>..] - ETA: 0s - loss: 0.0066 - mse: 0.3510\n",
      "Epoch 00062: val_loss did not improve from 0.01619\n",
      "43/43 [==============================] - 0s 7ms/step - loss: 0.0066 - mse: 0.3557 - val_loss: 0.0622 - val_mse: 0.7390\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 63/200\n",
      "38/43 [=========================>....] - ETA: 0s - loss: 0.0061 - mse: 0.3406\n",
      "Epoch 00063: val_loss did not improve from 0.01619\n",
      "43/43 [==============================] - 0s 7ms/step - loss: 0.0063 - mse: 0.3436 - val_loss: 0.0642 - val_mse: 0.7794\n",
      "Epoch 64/200\n",
      "40/43 [==========================>...] - ETA: 0s - loss: 0.0067 - mse: 0.3491\n",
      "Epoch 00064: val_loss did not improve from 0.01619\n",
      "43/43 [==============================] - 0s 8ms/step - loss: 0.0067 - mse: 0.3462 - val_loss: 0.0593 - val_mse: 0.7291\n",
      "Epoch 65/200\n",
      "40/43 [==========================>...] - ETA: 0s - loss: 0.0066 - mse: 0.3526\n",
      "Epoch 00065: val_loss did not improve from 0.01619\n",
      "43/43 [==============================] - 0s 6ms/step - loss: 0.0065 - mse: 0.3486 - val_loss: 0.0605 - val_mse: 0.7444\n",
      "Epoch 66/200\n",
      "42/43 [============================>.] - ETA: 0s - loss: 0.0057 - mse: 0.3466\n",
      "Epoch 00066: val_loss did not improve from 0.01619\n",
      "43/43 [==============================] - 0s 8ms/step - loss: 0.0056 - mse: 0.3469 - val_loss: 0.0596 - val_mse: 0.7331\n",
      "Epoch 67/200\n",
      "35/43 [=======================>......] - ETA: 0s - loss: 0.0066 - mse: 0.3466\n",
      "Epoch 00067: val_loss did not improve from 0.01619\n",
      "43/43 [==============================] - 0s 7ms/step - loss: 0.0061 - mse: 0.3446 - val_loss: 0.0588 - val_mse: 0.7221\n",
      "Epoch 68/200\n",
      "39/43 [==========================>...] - ETA: 0s - loss: 0.0062 - mse: 0.3521\n",
      "Epoch 00068: val_loss did not improve from 0.01619\n",
      "43/43 [==============================] - 0s 8ms/step - loss: 0.0065 - mse: 0.3493 - val_loss: 0.0593 - val_mse: 0.7216\n",
      "Epoch 69/200\n",
      "33/43 [======================>.......] - ETA: 0s - loss: 0.0064 - mse: 0.3445\n",
      "Epoch 00069: val_loss did not improve from 0.01619\n",
      "43/43 [==============================] - 0s 7ms/step - loss: 0.0067 - mse: 0.3488 - val_loss: 0.0590 - val_mse: 0.7184\n",
      "Epoch 70/200\n",
      "41/43 [===========================>..] - ETA: 0s - loss: 0.0056 - mse: 0.3385\n",
      "Epoch 00070: val_loss did not improve from 0.01619\n",
      "43/43 [==============================] - 0s 9ms/step - loss: 0.0056 - mse: 0.3392 - val_loss: 0.0597 - val_mse: 0.7340\n",
      "Epoch 71/200\n",
      "39/43 [==========================>...] - ETA: 0s - loss: 0.0057 - mse: 0.3373\n",
      "Epoch 00071: val_loss did not improve from 0.01619\n",
      "43/43 [==============================] - 0s 6ms/step - loss: 0.0058 - mse: 0.3348 - val_loss: 0.0581 - val_mse: 0.7015\n",
      "Epoch 72/200\n",
      "41/43 [===========================>..] - ETA: 0s - loss: 0.0069 - mse: 0.3506\n",
      "Epoch 00072: val_loss did not improve from 0.01619\n",
      "43/43 [==============================] - 0s 8ms/step - loss: 0.0069 - mse: 0.3508 - val_loss: 0.0556 - val_mse: 0.6896\n",
      "Epoch 73/200\n",
      "38/43 [=========================>....] - ETA: 0s - loss: 0.0063 - mse: 0.3443\n",
      "Epoch 00073: val_loss did not improve from 0.01619\n",
      "43/43 [==============================] - 0s 6ms/step - loss: 0.0061 - mse: 0.3496 - val_loss: 0.0611 - val_mse: 0.7692\n",
      "Epoch 74/200\n",
      "40/43 [==========================>...] - ETA: 0s - loss: 0.0059 - mse: 0.3419\n",
      "Epoch 00074: val_loss did not improve from 0.01619\n",
      "43/43 [==============================] - 0s 8ms/step - loss: 0.0058 - mse: 0.3422 - val_loss: 0.0601 - val_mse: 0.7517\n",
      "Epoch 75/200\n",
      "40/43 [==========================>...] - ETA: 0s - loss: 0.0052 - mse: 0.3403\n",
      "Epoch 00075: val_loss did not improve from 0.01619\n",
      "43/43 [==============================] - 0s 8ms/step - loss: 0.0051 - mse: 0.3373 - val_loss: 0.0631 - val_mse: 0.7822\n",
      "Epoch 76/200\n",
      "39/43 [==========================>...] - ETA: 0s - loss: 0.0050 - mse: 0.3342\n",
      "Epoch 00076: val_loss did not improve from 0.01619\n",
      "43/43 [==============================] - 0s 8ms/step - loss: 0.0051 - mse: 0.3313 - val_loss: 0.0688 - val_mse: 0.8712\n",
      "Epoch 77/200\n",
      "40/43 [==========================>...] - ETA: 0s - loss: 0.0051 - mse: 0.3322\n",
      "Epoch 00077: val_loss did not improve from 0.01619\n",
      "43/43 [==============================] - 0s 8ms/step - loss: 0.0056 - mse: 0.3441 - val_loss: 0.0624 - val_mse: 0.7793\n",
      "Epoch 78/200\n",
      "40/43 [==========================>...] - ETA: 0s - loss: 0.0063 - mse: 0.3463\n",
      "Epoch 00078: val_loss did not improve from 0.01619\n",
      "43/43 [==============================] - 0s 8ms/step - loss: 0.0066 - mse: 0.3480 - val_loss: 0.0593 - val_mse: 0.7226\n",
      "Epoch 79/200\n",
      "40/43 [==========================>...] - ETA: 0s - loss: 0.0060 - mse: 0.3382\n",
      "Epoch 00079: val_loss did not improve from 0.01619\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.0060 - mse: 0.3426 - val_loss: 0.0601 - val_mse: 0.7439\n",
      "Epoch 80/200\n",
      "40/43 [==========================>...] - ETA: 0s - loss: 0.0055 - mse: 0.3401\n",
      "Epoch 00080: val_loss did not improve from 0.01619\n",
      "43/43 [==============================] - 0s 8ms/step - loss: 0.0055 - mse: 0.3374 - val_loss: 0.0617 - val_mse: 0.7532\n",
      "Epoch 81/200\n",
      "38/43 [=========================>....] - ETA: 0s - loss: 0.0058 - mse: 0.3495\n",
      "Epoch 00081: val_loss did not improve from 0.01619\n",
      "43/43 [==============================] - 0s 8ms/step - loss: 0.0057 - mse: 0.3440 - val_loss: 0.0637 - val_mse: 0.7715\n",
      "Epoch 82/200\n",
      "40/43 [==========================>...] - ETA: 0s - loss: 0.0058 - mse: 0.3376\n",
      "Epoch 00082: val_loss did not improve from 0.01619\n",
      "43/43 [==============================] - 0s 9ms/step - loss: 0.0059 - mse: 0.3397 - val_loss: 0.0649 - val_mse: 0.7751\n",
      "Epoch 83/200\n",
      "38/43 [=========================>....] - ETA: 0s - loss: 0.0060 - mse: 0.3355\n",
      "Epoch 00083: val_loss did not improve from 0.01619\n",
      "43/43 [==============================] - 0s 9ms/step - loss: 0.0059 - mse: 0.3414 - val_loss: 0.0685 - val_mse: 0.8103\n",
      "Epoch 84/200\n",
      "40/43 [==========================>...] - ETA: 0s - loss: 0.0046 - mse: 0.3306\n",
      "Epoch 00084: val_loss did not improve from 0.01619\n",
      "43/43 [==============================] - 0s 8ms/step - loss: 0.0047 - mse: 0.3338 - val_loss: 0.0638 - val_mse: 0.7406\n",
      "Epoch 85/200\n",
      "43/43 [==============================] - ETA: 0s - loss: 0.0051 - mse: 0.3418\n",
      "Epoch 00085: val_loss did not improve from 0.01619\n",
      "43/43 [==============================] - 0s 8ms/step - loss: 0.0051 - mse: 0.3418 - val_loss: 0.0655 - val_mse: 0.7794\n",
      "Epoch 86/200\n",
      "38/43 [=========================>....] - ETA: 0s - loss: 0.0044 - mse: 0.3136\n",
      "Epoch 00086: val_loss did not improve from 0.01619\n",
      "43/43 [==============================] - 0s 8ms/step - loss: 0.0046 - mse: 0.3348 - val_loss: 0.0644 - val_mse: 0.7783\n",
      "Epoch 87/200\n",
      "43/43 [==============================] - ETA: 0s - loss: 0.0057 - mse: 0.3337\n",
      "Epoch 00087: val_loss did not improve from 0.01619\n",
      "43/43 [==============================] - 0s 7ms/step - loss: 0.0057 - mse: 0.3337 - val_loss: 0.0675 - val_mse: 0.8084\n",
      "Epoch 88/200\n",
      "38/43 [=========================>....] - ETA: 0s - loss: 0.0056 - mse: 0.3349\n",
      "Epoch 00088: val_loss did not improve from 0.01619\n",
      "43/43 [==============================] - 0s 8ms/step - loss: 0.0057 - mse: 0.3455 - val_loss: 0.0590 - val_mse: 0.6948\n",
      "Epoch 89/200\n",
      "38/43 [=========================>....] - ETA: 0s - loss: 0.0059 - mse: 0.3351\n",
      "Epoch 00089: val_loss did not improve from 0.01619\n",
      "43/43 [==============================] - 0s 8ms/step - loss: 0.0059 - mse: 0.3376 - val_loss: 0.0689 - val_mse: 0.8147\n",
      "Epoch 90/200\n",
      "39/43 [==========================>...] - ETA: 0s - loss: 0.0051 - mse: 0.3381\n",
      "Epoch 00090: val_loss did not improve from 0.01619\n",
      "43/43 [==============================] - 0s 6ms/step - loss: 0.0050 - mse: 0.3355 - val_loss: 0.0668 - val_mse: 0.8059\n",
      "Epoch 91/200\n",
      "41/43 [===========================>..] - ETA: 0s - loss: 0.0044 - mse: 0.3225\n",
      "Epoch 00091: val_loss did not improve from 0.01619\n",
      "43/43 [==============================] - 0s 6ms/step - loss: 0.0043 - mse: 0.3274 - val_loss: 0.0666 - val_mse: 0.7957\n",
      "Epoch 92/200\n",
      "39/43 [==========================>...] - ETA: 0s - loss: 0.0047 - mse: 0.3361\n",
      "Epoch 00092: val_loss did not improve from 0.01619\n",
      "43/43 [==============================] - 0s 6ms/step - loss: 0.0048 - mse: 0.3359 - val_loss: 0.0683 - val_mse: 0.8168\n",
      "Epoch 93/200\n",
      "40/43 [==========================>...] - ETA: 0s - loss: 0.0049 - mse: 0.3312\n",
      "Epoch 00093: val_loss did not improve from 0.01619\n",
      "43/43 [==============================] - 0s 6ms/step - loss: 0.0051 - mse: 0.3385 - val_loss: 0.0683 - val_mse: 0.7886\n",
      "Epoch 94/200\n",
      "40/43 [==========================>...] - ETA: 0s - loss: 0.0045 - mse: 0.3290\n",
      "Epoch 00094: val_loss did not improve from 0.01619\n",
      "43/43 [==============================] - 0s 6ms/step - loss: 0.0045 - mse: 0.3332 - val_loss: 0.0678 - val_mse: 0.7692\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 95/200\n",
      "41/43 [===========================>..] - ETA: 0s - loss: 0.0048 - mse: 0.3356\n",
      "Epoch 00095: val_loss did not improve from 0.01619\n",
      "43/43 [==============================] - 0s 6ms/step - loss: 0.0048 - mse: 0.3387 - val_loss: 0.0677 - val_mse: 0.7791\n",
      "Epoch 96/200\n",
      "40/43 [==========================>...] - ETA: 0s - loss: 0.0046 - mse: 0.3328\n",
      "Epoch 00096: val_loss did not improve from 0.01619\n",
      "43/43 [==============================] - 0s 6ms/step - loss: 0.0046 - mse: 0.3339 - val_loss: 0.0694 - val_mse: 0.8265\n",
      "Epoch 97/200\n",
      "40/43 [==========================>...] - ETA: 0s - loss: 0.0043 - mse: 0.3327\n",
      "Epoch 00097: val_loss did not improve from 0.01619\n",
      "43/43 [==============================] - 0s 8ms/step - loss: 0.0043 - mse: 0.3297 - val_loss: 0.0731 - val_mse: 0.8826\n",
      "Epoch 98/200\n",
      "41/43 [===========================>..] - ETA: 0s - loss: 0.0041 - mse: 0.3281\n",
      "Epoch 00098: val_loss did not improve from 0.01619\n",
      "43/43 [==============================] - 0s 8ms/step - loss: 0.0041 - mse: 0.3298 - val_loss: 0.0710 - val_mse: 0.8604\n",
      "Epoch 99/200\n",
      "40/43 [==========================>...] - ETA: 0s - loss: 0.0050 - mse: 0.3312\n",
      "Epoch 00099: val_loss did not improve from 0.01619\n",
      "43/43 [==============================] - 0s 6ms/step - loss: 0.0050 - mse: 0.3270 - val_loss: 0.0667 - val_mse: 0.7679\n",
      "Epoch 100/200\n",
      "38/43 [=========================>....] - ETA: 0s - loss: 0.0047 - mse: 0.3369\n",
      "Epoch 00100: val_loss did not improve from 0.01619\n",
      "43/43 [==============================] - 0s 8ms/step - loss: 0.0046 - mse: 0.3339 - val_loss: 0.0703 - val_mse: 0.8319\n",
      "Epoch 101/200\n",
      "41/43 [===========================>..] - ETA: 0s - loss: 0.0050 - mse: 0.3290\n",
      "Epoch 00101: val_loss did not improve from 0.01619\n",
      "43/43 [==============================] - 0s 6ms/step - loss: 0.0050 - mse: 0.3355 - val_loss: 0.0654 - val_mse: 0.7802\n",
      "Epoch 102/200\n",
      "38/43 [=========================>....] - ETA: 0s - loss: 0.0044 - mse: 0.3381\n",
      "Epoch 00102: val_loss did not improve from 0.01619\n",
      "43/43 [==============================] - 0s 6ms/step - loss: 0.0044 - mse: 0.3350 - val_loss: 0.0717 - val_mse: 0.8618\n",
      "Epoch 103/200\n",
      "37/43 [========================>.....] - ETA: 0s - loss: 0.0047 - mse: 0.3308\n",
      "Epoch 00103: val_loss did not improve from 0.01619\n",
      "43/43 [==============================] - 0s 6ms/step - loss: 0.0047 - mse: 0.3276 - val_loss: 0.0740 - val_mse: 0.8212\n",
      "Epoch 104/200\n",
      "38/43 [=========================>....] - ETA: 0s - loss: 0.0052 - mse: 0.3458\n",
      "Epoch 00104: val_loss did not improve from 0.01619\n",
      "43/43 [==============================] - 0s 6ms/step - loss: 0.0050 - mse: 0.3372 - val_loss: 0.0757 - val_mse: 0.8645\n",
      "Epoch 105/200\n",
      "39/43 [==========================>...] - ETA: 0s - loss: 0.0046 - mse: 0.3426\n",
      "Epoch 00105: val_loss did not improve from 0.01619\n",
      "43/43 [==============================] - 0s 6ms/step - loss: 0.0045 - mse: 0.3354 - val_loss: 0.0771 - val_mse: 0.8793\n",
      "Epoch 106/200\n",
      "38/43 [=========================>....] - ETA: 0s - loss: 0.0041 - mse: 0.3332\n",
      "Epoch 00106: val_loss did not improve from 0.01619\n",
      "43/43 [==============================] - 0s 6ms/step - loss: 0.0040 - mse: 0.3318 - val_loss: 0.0772 - val_mse: 0.8993\n",
      "Epoch 107/200\n",
      "37/43 [========================>.....] - ETA: 0s - loss: 0.0041 - mse: 0.3288\n",
      "Epoch 00107: val_loss did not improve from 0.01619\n",
      "43/43 [==============================] - 0s 6ms/step - loss: 0.0038 - mse: 0.3277 - val_loss: 0.0715 - val_mse: 0.8478\n",
      "Epoch 108/200\n",
      "42/43 [============================>.] - ETA: 0s - loss: 0.0044 - mse: 0.3246\n",
      "Epoch 00108: val_loss did not improve from 0.01619\n",
      "43/43 [==============================] - 0s 8ms/step - loss: 0.0044 - mse: 0.3252 - val_loss: 0.0776 - val_mse: 0.9591\n",
      "Epoch 109/200\n",
      "41/43 [===========================>..] - ETA: 0s - loss: 0.0042 - mse: 0.3283\n",
      "Epoch 00109: val_loss did not improve from 0.01619\n",
      "43/43 [==============================] - 0s 7ms/step - loss: 0.0043 - mse: 0.3284 - val_loss: 0.0741 - val_mse: 0.9155\n",
      "Epoch 110/200\n",
      "39/43 [==========================>...] - ETA: 0s - loss: 0.0036 - mse: 0.3239\n",
      "Epoch 00110: val_loss did not improve from 0.01619\n",
      "43/43 [==============================] - 0s 6ms/step - loss: 0.0038 - mse: 0.3277 - val_loss: 0.0801 - val_mse: 1.0094\n",
      "Epoch 111/200\n",
      "38/43 [=========================>....] - ETA: 0s - loss: 0.0043 - mse: 0.3236\n",
      "Epoch 00111: val_loss did not improve from 0.01619\n",
      "43/43 [==============================] - 0s 6ms/step - loss: 0.0043 - mse: 0.3281 - val_loss: 0.0761 - val_mse: 0.9260\n",
      "Epoch 112/200\n",
      "40/43 [==========================>...] - ETA: 0s - loss: 0.0043 - mse: 0.3179\n",
      "Epoch 00112: val_loss did not improve from 0.01619\n",
      "43/43 [==============================] - 0s 6ms/step - loss: 0.0045 - mse: 0.3310 - val_loss: 0.0786 - val_mse: 0.9629\n",
      "Epoch 113/200\n",
      "38/43 [=========================>....] - ETA: 0s - loss: 0.0042 - mse: 0.3349\n",
      "Epoch 00113: val_loss did not improve from 0.01619\n",
      "43/43 [==============================] - 0s 6ms/step - loss: 0.0041 - mse: 0.3313 - val_loss: 0.0869 - val_mse: 1.0607\n",
      "Epoch 114/200\n",
      "38/43 [=========================>....] - ETA: 0s - loss: 0.0041 - mse: 0.3366\n",
      "Epoch 00114: val_loss did not improve from 0.01619\n",
      "43/43 [==============================] - 0s 8ms/step - loss: 0.0040 - mse: 0.3263 - val_loss: 0.0811 - val_mse: 0.9675\n",
      "Epoch 115/200\n",
      "38/43 [=========================>....] - ETA: 0s - loss: 0.0038 - mse: 0.3354\n",
      "Epoch 00115: val_loss did not improve from 0.01619\n",
      "43/43 [==============================] - 0s 6ms/step - loss: 0.0039 - mse: 0.3248 - val_loss: 0.0828 - val_mse: 0.9902\n",
      "Epoch 116/200\n",
      "39/43 [==========================>...] - ETA: 0s - loss: 0.0048 - mse: 0.3442\n",
      "Epoch 00116: val_loss did not improve from 0.01619\n",
      "43/43 [==============================] - 0s 6ms/step - loss: 0.0047 - mse: 0.3404 - val_loss: 0.0883 - val_mse: 1.0707\n",
      "Epoch 117/200\n",
      "39/43 [==========================>...] - ETA: 0s - loss: 0.0043 - mse: 0.3221\n",
      "Epoch 00117: val_loss did not improve from 0.01619\n",
      "43/43 [==============================] - 0s 8ms/step - loss: 0.0044 - mse: 0.3250 - val_loss: 0.0792 - val_mse: 0.9303\n",
      "Epoch 118/200\n",
      "38/43 [=========================>....] - ETA: 0s - loss: 0.0045 - mse: 0.3376\n",
      "Epoch 00118: val_loss did not improve from 0.01619\n",
      "43/43 [==============================] - 0s 6ms/step - loss: 0.0044 - mse: 0.3309 - val_loss: 0.0831 - val_mse: 0.9827\n",
      "Epoch 119/200\n",
      "39/43 [==========================>...] - ETA: 0s - loss: 0.0037 - mse: 0.3188\n",
      "Epoch 00119: val_loss did not improve from 0.01619\n",
      "43/43 [==============================] - 0s 7ms/step - loss: 0.0036 - mse: 0.3215 - val_loss: 0.0864 - val_mse: 1.0303\n",
      "Epoch 120/200\n",
      "38/43 [=========================>....] - ETA: 0s - loss: 0.0040 - mse: 0.3253\n",
      "Epoch 00120: val_loss did not improve from 0.01619\n",
      "43/43 [==============================] - 0s 8ms/step - loss: 0.0041 - mse: 0.3280 - val_loss: 0.0831 - val_mse: 1.0156\n",
      "Epoch 121/200\n",
      "38/43 [=========================>....] - ETA: 0s - loss: 0.0039 - mse: 0.3210\n",
      "Epoch 00121: val_loss did not improve from 0.01619\n",
      "43/43 [==============================] - 0s 6ms/step - loss: 0.0039 - mse: 0.3274 - val_loss: 0.0836 - val_mse: 1.0470\n",
      "Epoch 122/200\n",
      "42/43 [============================>.] - ETA: 0s - loss: 0.0040 - mse: 0.3262\n",
      "Epoch 00122: val_loss did not improve from 0.01619\n",
      "43/43 [==============================] - 0s 8ms/step - loss: 0.0039 - mse: 0.3249 - val_loss: 0.0751 - val_mse: 0.9044\n",
      "Epoch 123/200\n",
      "37/43 [========================>.....] - ETA: 0s - loss: 0.0041 - mse: 0.3211\n",
      "Epoch 00123: val_loss did not improve from 0.01619\n",
      "43/43 [==============================] - 0s 6ms/step - loss: 0.0041 - mse: 0.3306 - val_loss: 0.0738 - val_mse: 0.8790\n",
      "Epoch 124/200\n",
      "37/43 [========================>.....] - ETA: 0s - loss: 0.0040 - mse: 0.3322\n",
      "Epoch 00124: val_loss did not improve from 0.01619\n",
      "43/43 [==============================] - 0s 7ms/step - loss: 0.0040 - mse: 0.3320 - val_loss: 0.0774 - val_mse: 0.9488\n",
      "Epoch 125/200\n",
      "38/43 [=========================>....] - ETA: 0s - loss: 0.0042 - mse: 0.3262\n",
      "Epoch 00125: val_loss did not improve from 0.01619\n",
      "43/43 [==============================] - 0s 8ms/step - loss: 0.0041 - mse: 0.3251 - val_loss: 0.0775 - val_mse: 0.9616\n",
      "Epoch 126/200\n",
      "35/43 [=======================>......] - ETA: 0s - loss: 0.0038 - mse: 0.3185\n",
      "Epoch 00126: val_loss did not improve from 0.01619\n",
      "43/43 [==============================] - 0s 7ms/step - loss: 0.0036 - mse: 0.3188 - val_loss: 0.0833 - val_mse: 1.0374\n",
      "Epoch 127/200\n",
      "39/43 [==========================>...] - ETA: 0s - loss: 0.0047 - mse: 0.3314\n",
      "Epoch 00127: val_loss did not improve from 0.01619\n",
      "43/43 [==============================] - 0s 8ms/step - loss: 0.0046 - mse: 0.3285 - val_loss: 0.0797 - val_mse: 0.9995\n",
      "Epoch 128/200\n",
      "39/43 [==========================>...] - ETA: 0s - loss: 0.0045 - mse: 0.3297\n",
      "Epoch 00128: val_loss did not improve from 0.01619\n",
      "43/43 [==============================] - 0s 7ms/step - loss: 0.0045 - mse: 0.3307 - val_loss: 0.0794 - val_mse: 0.9915\n",
      "Epoch 129/200\n",
      "39/43 [==========================>...] - ETA: 0s - loss: 0.0043 - mse: 0.3273\n",
      "Epoch 00129: val_loss did not improve from 0.01619\n",
      "43/43 [==============================] - 0s 8ms/step - loss: 0.0044 - mse: 0.3288 - val_loss: 0.0851 - val_mse: 1.0562\n",
      "Epoch 130/200\n",
      "42/43 [============================>.] - ETA: 0s - loss: 0.0037 - mse: 0.3239\n",
      "Epoch 00130: val_loss did not improve from 0.01619\n",
      "43/43 [==============================] - 0s 7ms/step - loss: 0.0037 - mse: 0.3249 - val_loss: 0.0787 - val_mse: 0.9721\n",
      "Epoch 131/200\n",
      "38/43 [=========================>....] - ETA: 0s - loss: 0.0030 - mse: 0.3161\n",
      "Epoch 00131: val_loss did not improve from 0.01619\n",
      "43/43 [==============================] - 0s 8ms/step - loss: 0.0030 - mse: 0.3190 - val_loss: 0.0749 - val_mse: 0.9222\n",
      "Epoch 132/200\n",
      "36/43 [========================>.....] - ETA: 0s - loss: 0.0047 - mse: 0.3296\n",
      "Epoch 00132: val_loss did not improve from 0.01619\n",
      "43/43 [==============================] - 0s 7ms/step - loss: 0.0046 - mse: 0.3254 - val_loss: 0.0697 - val_mse: 0.8445\n",
      "Epoch 133/200\n",
      "36/43 [========================>.....] - ETA: 0s - loss: 0.0043 - mse: 0.3284\n",
      "Epoch 00133: val_loss did not improve from 0.01619\n",
      "43/43 [==============================] - 0s 8ms/step - loss: 0.0041 - mse: 0.3304 - val_loss: 0.0731 - val_mse: 0.8798\n",
      "Epoch 134/200\n",
      "35/43 [=======================>......] - ETA: 0s - loss: 0.0035 - mse: 0.3315\n",
      "Epoch 00134: val_loss did not improve from 0.01619\n",
      "43/43 [==============================] - 0s 8ms/step - loss: 0.0034 - mse: 0.3230 - val_loss: 0.0853 - val_mse: 1.0955\n",
      "Epoch 135/200\n",
      "37/43 [========================>.....] - ETA: 0s - loss: 0.0036 - mse: 0.3294\n",
      "Epoch 00135: val_loss did not improve from 0.01619\n",
      "43/43 [==============================] - 0s 8ms/step - loss: 0.0038 - mse: 0.3252 - val_loss: 0.0825 - val_mse: 1.0403\n",
      "Epoch 136/200\n",
      "35/43 [=======================>......] - ETA: 0s - loss: 0.0036 - mse: 0.3245 \n",
      "Epoch 00136: val_loss did not improve from 0.01619\n",
      "43/43 [==============================] - 0s 7ms/step - loss: 0.0035 - mse: 0.3285 - val_loss: 0.0808 - val_mse: 1.0014\n",
      "Epoch 137/200\n",
      "38/43 [=========================>....] - ETA: 0s - loss: 0.0031 - mse: 0.3305 \n",
      "Epoch 00137: val_loss did not improve from 0.01619\n",
      "43/43 [==============================] - 0s 6ms/step - loss: 0.0031 - mse: 0.3242 - val_loss: 0.0833 - val_mse: 1.0644\n",
      "Epoch 138/200\n",
      "40/43 [==========================>...] - ETA: 0s - loss: 0.0037 - mse: 0.3283\n",
      "Epoch 00138: val_loss did not improve from 0.01619\n",
      "43/43 [==============================] - 0s 6ms/step - loss: 0.0037 - mse: 0.3250 - val_loss: 0.0836 - val_mse: 1.0556\n",
      "Epoch 139/200\n",
      "40/43 [==========================>...] - ETA: 0s - loss: 0.0030 - mse: 0.3232\n",
      "Epoch 00139: val_loss did not improve from 0.01619\n",
      "43/43 [==============================] - 0s 8ms/step - loss: 0.0031 - mse: 0.3216 - val_loss: 0.0793 - val_mse: 0.9946\n",
      "Epoch 140/200\n",
      "35/43 [=======================>......] - ETA: 0s - loss: 0.0034 - mse: 0.3238\n",
      "Epoch 00140: val_loss did not improve from 0.01619\n",
      "43/43 [==============================] - 0s 7ms/step - loss: 0.0037 - mse: 0.3224 - val_loss: 0.0829 - val_mse: 1.0482\n",
      "Epoch 141/200\n",
      "36/43 [========================>.....] - ETA: 0s - loss: 0.0039 - mse: 0.3221\n",
      "Epoch 00141: val_loss did not improve from 0.01619\n",
      "43/43 [==============================] - 0s 6ms/step - loss: 0.0041 - mse: 0.3241 - val_loss: 0.0919 - val_mse: 1.1853\n",
      "Epoch 142/200\n",
      "39/43 [==========================>...] - ETA: 0s - loss: 0.0044 - mse: 0.3376\n",
      "Epoch 00142: val_loss did not improve from 0.01619\n",
      "43/43 [==============================] - 0s 6ms/step - loss: 0.0043 - mse: 0.3327 - val_loss: 0.0869 - val_mse: 1.0780\n",
      "Epoch 143/200\n",
      "40/43 [==========================>...] - ETA: 0s - loss: 0.0039 - mse: 0.3192\n",
      "Epoch 00143: val_loss did not improve from 0.01619\n",
      "43/43 [==============================] - 0s 6ms/step - loss: 0.0039 - mse: 0.3227 - val_loss: 0.0839 - val_mse: 1.0672\n",
      "Epoch 144/200\n",
      "38/43 [=========================>....] - ETA: 0s - loss: 0.0035 - mse: 0.3198\n",
      "Epoch 00144: val_loss did not improve from 0.01619\n",
      "43/43 [==============================] - 0s 6ms/step - loss: 0.0035 - mse: 0.3240 - val_loss: 0.0820 - val_mse: 1.0424\n",
      "Epoch 145/200\n",
      "41/43 [===========================>..] - ETA: 0s - loss: 0.0030 - mse: 0.3191\n",
      "Epoch 00145: val_loss did not improve from 0.01619\n",
      "43/43 [==============================] - 0s 6ms/step - loss: 0.0031 - mse: 0.3209 - val_loss: 0.0877 - val_mse: 1.1562\n",
      "Epoch 146/200\n",
      "39/43 [==========================>...] - ETA: 0s - loss: 0.0031 - mse: 0.3164\n",
      "Epoch 00146: val_loss did not improve from 0.01619\n",
      "43/43 [==============================] - 0s 8ms/step - loss: 0.0033 - mse: 0.3207 - val_loss: 0.0900 - val_mse: 1.2037\n",
      "Epoch 147/200\n",
      "39/43 [==========================>...] - ETA: 0s - loss: 0.0032 - mse: 0.3231\n",
      "Epoch 00147: val_loss did not improve from 0.01619\n",
      "43/43 [==============================] - 0s 6ms/step - loss: 0.0033 - mse: 0.3194 - val_loss: 0.0871 - val_mse: 1.1600\n",
      "Epoch 148/200\n",
      "41/43 [===========================>..] - ETA: 0s - loss: 0.0032 - mse: 0.3186\n",
      "Epoch 00148: val_loss did not improve from 0.01619\n",
      "43/43 [==============================] - 0s 7ms/step - loss: 0.0033 - mse: 0.3189 - val_loss: 0.0832 - val_mse: 1.0613\n",
      "Epoch 149/200\n",
      "37/43 [========================>.....] - ETA: 0s - loss: 0.0032 - mse: 0.3156\n",
      "Epoch 00149: val_loss did not improve from 0.01619\n",
      "43/43 [==============================] - 0s 6ms/step - loss: 0.0033 - mse: 0.3234 - val_loss: 0.0901 - val_mse: 1.1668\n",
      "Epoch 150/200\n",
      "37/43 [========================>.....] - ETA: 0s - loss: 0.0034 - mse: 0.3263\n",
      "Epoch 00150: val_loss did not improve from 0.01619\n",
      "43/43 [==============================] - 0s 6ms/step - loss: 0.0035 - mse: 0.3235 - val_loss: 0.0848 - val_mse: 1.0874\n",
      "Epoch 151/200\n",
      "35/43 [=======================>......] - ETA: 0s - loss: 0.0030 - mse: 0.3132\n",
      "Epoch 00151: val_loss did not improve from 0.01619\n",
      "43/43 [==============================] - 0s 7ms/step - loss: 0.0030 - mse: 0.3181 - val_loss: 0.0927 - val_mse: 1.2320\n",
      "Epoch 152/200\n",
      "39/43 [==========================>...] - ETA: 0s - loss: 0.0037 - mse: 0.3189\n",
      "Epoch 00152: val_loss did not improve from 0.01619\n",
      "43/43 [==============================] - 0s 6ms/step - loss: 0.0037 - mse: 0.3194 - val_loss: 0.0922 - val_mse: 1.1919\n",
      "Epoch 153/200\n",
      "41/43 [===========================>..] - ETA: 0s - loss: 0.0039 - mse: 0.3278\n",
      "Epoch 00153: val_loss did not improve from 0.01619\n",
      "43/43 [==============================] - 0s 8ms/step - loss: 0.0041 - mse: 0.3246 - val_loss: 0.0810 - val_mse: 1.0071\n",
      "Epoch 154/200\n",
      "41/43 [===========================>..] - ETA: 0s - loss: 0.0032 - mse: 0.3221\n",
      "Epoch 00154: val_loss did not improve from 0.01619\n",
      "43/43 [==============================] - 0s 7ms/step - loss: 0.0032 - mse: 0.3258 - val_loss: 0.0848 - val_mse: 1.0424\n",
      "Epoch 155/200\n",
      "37/43 [========================>.....] - ETA: 0s - loss: 0.0032 - mse: 0.3221\n",
      "Epoch 00155: val_loss did not improve from 0.01619\n",
      "43/43 [==============================] - 0s 7ms/step - loss: 0.0033 - mse: 0.3235 - val_loss: 0.0850 - val_mse: 1.0413\n",
      "Epoch 156/200\n",
      "40/43 [==========================>...] - ETA: 0s - loss: 0.0029 - mse: 0.3232\n",
      "Epoch 00156: val_loss did not improve from 0.01619\n",
      "43/43 [==============================] - 0s 8ms/step - loss: 0.0029 - mse: 0.3204 - val_loss: 0.0880 - val_mse: 1.0823\n",
      "Epoch 157/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "40/43 [==========================>...] - ETA: 0s - loss: 0.0028 - mse: 0.3214\n",
      "Epoch 00157: val_loss did not improve from 0.01619\n",
      "43/43 [==============================] - 0s 6ms/step - loss: 0.0028 - mse: 0.3215 - val_loss: 0.0873 - val_mse: 1.0841\n",
      "Epoch 158/200\n",
      "40/43 [==========================>...] - ETA: 0s - loss: 0.0028 - mse: 0.3251\n",
      "Epoch 00158: val_loss did not improve from 0.01619\n",
      "43/43 [==============================] - 0s 6ms/step - loss: 0.0029 - mse: 0.3191 - val_loss: 0.0780 - val_mse: 0.9322\n",
      "Epoch 159/200\n",
      "42/43 [============================>.] - ETA: 0s - loss: 0.0034 - mse: 0.3208\n",
      "Epoch 00159: val_loss did not improve from 0.01619\n",
      "43/43 [==============================] - 0s 7ms/step - loss: 0.0034 - mse: 0.3203 - val_loss: 0.0794 - val_mse: 0.9273\n",
      "Epoch 160/200\n",
      "38/43 [=========================>....] - ETA: 0s - loss: 0.0031 - mse: 0.3240- ETA: 0s - loss: 0.0037 - mse: 0.3\n",
      "Epoch 00160: val_loss did not improve from 0.01619\n",
      "43/43 [==============================] - 0s 6ms/step - loss: 0.0030 - mse: 0.3221 - val_loss: 0.0808 - val_mse: 0.9592\n",
      "Epoch 161/200\n",
      "37/43 [========================>.....] - ETA: 0s - loss: 0.0027 - mse: 0.3268\n",
      "Epoch 00161: val_loss did not improve from 0.01619\n",
      "43/43 [==============================] - 0s 7ms/step - loss: 0.0027 - mse: 0.3165 - val_loss: 0.0871 - val_mse: 1.0875\n",
      "Epoch 162/200\n",
      "41/43 [===========================>..] - ETA: 0s - loss: 0.0028 - mse: 0.3142- ETA: 0s - loss: 0.0045 - mse: 0\n",
      "Epoch 00162: val_loss did not improve from 0.01619\n",
      "43/43 [==============================] - 0s 9ms/step - loss: 0.0028 - mse: 0.3148 - val_loss: 0.0891 - val_mse: 1.1173\n",
      "Epoch 163/200\n",
      "42/43 [============================>.] - ETA: 0s - loss: 0.0032 - mse: 0.3277\n",
      "Epoch 00163: val_loss did not improve from 0.01619\n",
      "43/43 [==============================] - 0s 7ms/step - loss: 0.0031 - mse: 0.3270 - val_loss: 0.0735 - val_mse: 0.8507\n",
      "Epoch 164/200\n",
      "39/43 [==========================>...] - ETA: 0s - loss: 0.0033 - mse: 0.3209 \n",
      "Epoch 00164: val_loss did not improve from 0.01619\n",
      "43/43 [==============================] - 0s 6ms/step - loss: 0.0033 - mse: 0.3207 - val_loss: 0.0797 - val_mse: 0.9601\n",
      "Epoch 165/200\n",
      "41/43 [===========================>..] - ETA: 0s - loss: 0.0030 - mse: 0.3244\n",
      "Epoch 00165: val_loss did not improve from 0.01619\n",
      "43/43 [==============================] - 0s 7ms/step - loss: 0.0030 - mse: 0.3264 - val_loss: 0.0832 - val_mse: 0.9917\n",
      "Epoch 166/200\n",
      "39/43 [==========================>...] - ETA: 0s - loss: 0.0032 - mse: 0.3262\n",
      "Epoch 00166: val_loss did not improve from 0.01619\n",
      "43/43 [==============================] - 0s 6ms/step - loss: 0.0031 - mse: 0.3190 - val_loss: 0.0864 - val_mse: 1.0724\n",
      "Epoch 167/200\n",
      "37/43 [========================>.....] - ETA: 0s - loss: 0.0029 - mse: 0.3180\n",
      "Epoch 00167: val_loss did not improve from 0.01619\n",
      "43/43 [==============================] - 0s 6ms/step - loss: 0.0029 - mse: 0.3180 - val_loss: 0.0859 - val_mse: 1.0673\n",
      "Epoch 168/200\n",
      "39/43 [==========================>...] - ETA: 0s - loss: 0.0028 - mse: 0.3221\n",
      "Epoch 00168: val_loss did not improve from 0.01619\n",
      "43/43 [==============================] - 0s 6ms/step - loss: 0.0029 - mse: 0.3203 - val_loss: 0.0860 - val_mse: 1.0749\n",
      "Epoch 169/200\n",
      "40/43 [==========================>...] - ETA: 0s - loss: 0.0024 - mse: 0.3116\n",
      "Epoch 00169: val_loss did not improve from 0.01619\n",
      "43/43 [==============================] - 0s 6ms/step - loss: 0.0025 - mse: 0.3138 - val_loss: 0.0975 - val_mse: 1.2699\n",
      "Epoch 170/200\n",
      "39/43 [==========================>...] - ETA: 0s - loss: 0.0024 - mse: 0.3222\n",
      "Epoch 00170: val_loss did not improve from 0.01619\n",
      "43/43 [==============================] - 0s 6ms/step - loss: 0.0024 - mse: 0.3182 - val_loss: 0.0948 - val_mse: 1.2254\n",
      "Epoch 171/200\n",
      "34/43 [======================>.......] - ETA: 0s - loss: 0.0027 - mse: 0.3182\n",
      "Epoch 00171: val_loss did not improve from 0.01619\n",
      "43/43 [==============================] - 0s 7ms/step - loss: 0.0026 - mse: 0.3158 - val_loss: 0.0983 - val_mse: 1.2909\n",
      "Epoch 172/200\n",
      "37/43 [========================>.....] - ETA: 0s - loss: 0.0028 - mse: 0.3218\n",
      "Epoch 00172: val_loss did not improve from 0.01619\n",
      "43/43 [==============================] - 0s 6ms/step - loss: 0.0028 - mse: 0.3215 - val_loss: 0.0867 - val_mse: 1.0507\n",
      "Epoch 173/200\n",
      "41/43 [===========================>..] - ETA: 0s - loss: 0.0032 - mse: 0.3279\n",
      "Epoch 00173: val_loss did not improve from 0.01619\n",
      "43/43 [==============================] - 0s 6ms/step - loss: 0.0031 - mse: 0.3228 - val_loss: 0.0904 - val_mse: 1.1070\n",
      "Epoch 174/200\n",
      "37/43 [========================>.....] - ETA: 0s - loss: 0.0030 - mse: 0.3215\n",
      "Epoch 00174: val_loss did not improve from 0.01619\n",
      "43/43 [==============================] - 0s 7ms/step - loss: 0.0030 - mse: 0.3198 - val_loss: 0.0864 - val_mse: 1.0435\n",
      "Epoch 175/200\n",
      "36/43 [========================>.....] - ETA: 0s - loss: 0.0029 - mse: 0.3124\n",
      "Epoch 00175: val_loss did not improve from 0.01619\n",
      "43/43 [==============================] - 0s 6ms/step - loss: 0.0030 - mse: 0.3177 - val_loss: 0.0924 - val_mse: 1.1525\n",
      "Epoch 176/200\n",
      "38/43 [=========================>....] - ETA: 0s - loss: 0.0036 - mse: 0.3191 \n",
      "Epoch 00176: val_loss did not improve from 0.01619\n",
      "43/43 [==============================] - 0s 6ms/step - loss: 0.0036 - mse: 0.3180 - val_loss: 0.0878 - val_mse: 1.0622\n",
      "Epoch 177/200\n",
      "42/43 [============================>.] - ETA: 0s - loss: 0.0029 - mse: 0.3192\n",
      "Epoch 00177: val_loss did not improve from 0.01619\n",
      "43/43 [==============================] - 0s 7ms/step - loss: 0.0028 - mse: 0.3198 - val_loss: 0.1002 - val_mse: 1.2977\n",
      "Epoch 178/200\n",
      "43/43 [==============================] - ETA: 0s - loss: 0.0027 - mse: 0.3193\n",
      "Epoch 00178: val_loss did not improve from 0.01619\n",
      "43/43 [==============================] - 0s 7ms/step - loss: 0.0027 - mse: 0.3193 - val_loss: 0.0938 - val_mse: 1.1561\n",
      "Epoch 179/200\n",
      "36/43 [========================>.....] - ETA: 0s - loss: 0.0027 - mse: 0.3085\n",
      "Epoch 00179: val_loss did not improve from 0.01619\n",
      "43/43 [==============================] - 0s 7ms/step - loss: 0.0028 - mse: 0.3140 - val_loss: 0.0933 - val_mse: 1.1527\n",
      "Epoch 180/200\n",
      "35/43 [=======================>......] - ETA: 0s - loss: 0.0032 - mse: 0.3220\n",
      "Epoch 00180: val_loss did not improve from 0.01619\n",
      "43/43 [==============================] - 0s 7ms/step - loss: 0.0031 - mse: 0.3220 - val_loss: 0.0969 - val_mse: 1.2417\n",
      "Epoch 181/200\n",
      "41/43 [===========================>..] - ETA: 0s - loss: 0.0026 - mse: 0.3153\n",
      "Epoch 00181: val_loss did not improve from 0.01619\n",
      "43/43 [==============================] - 0s 8ms/step - loss: 0.0026 - mse: 0.3171 - val_loss: 0.0983 - val_mse: 1.2973\n",
      "Epoch 182/200\n",
      "43/43 [==============================] - ETA: 0s - loss: 0.0025 - mse: 0.3141\n",
      "Epoch 00182: val_loss did not improve from 0.01619\n",
      "43/43 [==============================] - 0s 8ms/step - loss: 0.0025 - mse: 0.3141 - val_loss: 0.0975 - val_mse: 1.2563\n",
      "Epoch 183/200\n",
      "37/43 [========================>.....] - ETA: 0s - loss: 0.0021 - mse: 0.3086\n",
      "Epoch 00183: val_loss did not improve from 0.01619\n",
      "43/43 [==============================] - 0s 7ms/step - loss: 0.0023 - mse: 0.3151 - val_loss: 0.0981 - val_mse: 1.2637\n",
      "Epoch 184/200\n",
      "36/43 [========================>.....] - ETA: 0s - loss: 0.0025 - mse: 0.3162    - ETA: 0s - loss: 0.0020 - mse: 0.30\n",
      "Epoch 00184: val_loss did not improve from 0.01619\n",
      "43/43 [==============================] - 0s 7ms/step - loss: 0.0027 - mse: 0.3127 - val_loss: 0.0987 - val_mse: 1.2742\n",
      "Epoch 185/200\n",
      "35/43 [=======================>......] - ETA: 0s - loss: 0.0031 - mse: 0.3116\n",
      "Epoch 00185: val_loss did not improve from 0.01619\n",
      "43/43 [==============================] - 0s 7ms/step - loss: 0.0038 - mse: 0.3224 - val_loss: 0.0908 - val_mse: 1.1466\n",
      "Epoch 186/200\n",
      "37/43 [========================>.....] - ETA: 0s - loss: 0.0043 - mse: 0.3261\n",
      "Epoch 00186: val_loss did not improve from 0.01619\n",
      "43/43 [==============================] - 0s 7ms/step - loss: 0.0042 - mse: 0.3314 - val_loss: 0.0806 - val_mse: 0.9720\n",
      "Epoch 187/200\n",
      "42/43 [============================>.] - ETA: 0s - loss: 0.0029 - mse: 0.3138\n",
      "Epoch 00187: val_loss did not improve from 0.01619\n",
      "43/43 [==============================] - 0s 7ms/step - loss: 0.0030 - mse: 0.3142 - val_loss: 0.0826 - val_mse: 1.0143\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 188/200\n",
      "37/43 [========================>.....] - ETA: 0s - loss: 0.0029 - mse: 0.3186\n",
      "Epoch 00188: val_loss did not improve from 0.01619\n",
      "43/43 [==============================] - 0s 7ms/step - loss: 0.0028 - mse: 0.3153 - val_loss: 0.0860 - val_mse: 1.0834\n",
      "Epoch 189/200\n",
      "39/43 [==========================>...] - ETA: 0s - loss: 0.0026 - mse: 0.3148\n",
      "Epoch 00189: val_loss did not improve from 0.01619\n",
      "43/43 [==============================] - 0s 6ms/step - loss: 0.0025 - mse: 0.3141 - val_loss: 0.0809 - val_mse: 0.9910\n",
      "Epoch 190/200\n",
      "39/43 [==========================>...] - ETA: 0s - loss: 0.0030 - mse: 0.3184\n",
      "Epoch 00190: val_loss did not improve from 0.01619\n",
      "43/43 [==============================] - 0s 6ms/step - loss: 0.0031 - mse: 0.3202 - val_loss: 0.0941 - val_mse: 1.2279\n",
      "Epoch 191/200\n",
      "37/43 [========================>.....] - ETA: 0s - loss: 0.0028 - mse: 0.3081\n",
      "Epoch 00191: val_loss did not improve from 0.01619\n",
      "43/43 [==============================] - 0s 7ms/step - loss: 0.0029 - mse: 0.3227 - val_loss: 0.0801 - val_mse: 0.9895\n",
      "Epoch 192/200\n",
      "34/43 [======================>.......] - ETA: 0s - loss: 0.0024 - mse: 0.3271\n",
      "Epoch 00192: val_loss did not improve from 0.01619\n",
      "43/43 [==============================] - 0s 7ms/step - loss: 0.0024 - mse: 0.3177 - val_loss: 0.0847 - val_mse: 1.0540\n",
      "Epoch 193/200\n",
      "38/43 [=========================>....] - ETA: 0s - loss: 0.0032 - mse: 0.3180\n",
      "Epoch 00193: val_loss did not improve from 0.01619\n",
      "43/43 [==============================] - 0s 8ms/step - loss: 0.0031 - mse: 0.3196 - val_loss: 0.0857 - val_mse: 1.0587\n",
      "Epoch 194/200\n",
      "39/43 [==========================>...] - ETA: 0s - loss: 0.0023 - mse: 0.3148\n",
      "Epoch 00194: val_loss did not improve from 0.01619\n",
      "43/43 [==============================] - 0s 7ms/step - loss: 0.0024 - mse: 0.3136 - val_loss: 0.0929 - val_mse: 1.2112\n",
      "Epoch 195/200\n",
      "40/43 [==========================>...] - ETA: 0s - loss: 0.0025 - mse: 0.3151\n",
      "Epoch 00195: val_loss did not improve from 0.01619\n",
      "43/43 [==============================] - 0s 8ms/step - loss: 0.0024 - mse: 0.3142 - val_loss: 0.0861 - val_mse: 1.0751\n",
      "Epoch 196/200\n",
      "41/43 [===========================>..] - ETA: 0s - loss: 0.0024 - mse: 0.3096 \n",
      "Epoch 00196: val_loss did not improve from 0.01619\n",
      "43/43 [==============================] - 0s 6ms/step - loss: 0.0024 - mse: 0.3129 - val_loss: 0.0866 - val_mse: 1.0896\n",
      "Epoch 197/200\n",
      "39/43 [==========================>...] - ETA: 0s - loss: 0.0025 - mse: 0.3114\n",
      "Epoch 00197: val_loss did not improve from 0.01619\n",
      "43/43 [==============================] - 0s 8ms/step - loss: 0.0026 - mse: 0.3123 - val_loss: 0.0966 - val_mse: 1.2765\n",
      "Epoch 198/200\n",
      "38/43 [=========================>....] - ETA: 0s - loss: 0.0028 - mse: 0.3165\n",
      "Epoch 00198: val_loss did not improve from 0.01619\n",
      "43/43 [==============================] - 0s 7ms/step - loss: 0.0027 - mse: 0.3171 - val_loss: 0.0881 - val_mse: 1.1027\n",
      "Epoch 199/200\n",
      "40/43 [==========================>...] - ETA: 0s - loss: 0.0023 - mse: 0.3094    - ETA: 0s - loss: 0.0026 - mse: 0.3\n",
      "Epoch 00199: val_loss did not improve from 0.01619\n",
      "43/43 [==============================] - 0s 8ms/step - loss: 0.0024 - mse: 0.3132 - val_loss: 0.0869 - val_mse: 1.0763\n",
      "Epoch 200/200\n",
      "36/43 [========================>.....] - ETA: 0s - loss: 0.0028 - mse: 0.3189\n",
      "Epoch 00200: val_loss did not improve from 0.01619\n",
      "43/43 [==============================] - 0s 7ms/step - loss: 0.0027 - mse: 0.3157 - val_loss: 0.0848 - val_mse: 1.0350\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#Compiling the ANN\n",
    "opt = keras.optimizers.Adam(lr=0.0015, beta_1=0.9, beta_2=0.999, epsilon=None, decay=0.0, amsgrad=False)\n",
    "model.compile(optimizer=opt, loss='mean_squared_logarithmic_error', metrics=['mse'])\n",
    "#Fitting the ANN to the training set\n",
    "model_filepath = 'min_vl_model3.h5'\n",
    "checkpoint = ModelCheckpoint(model_filepath, monitor = 'val_loss', verbose=1, save_best_only = True, mode='min' )\n",
    "model.fit(X_train,y_train, validation_split=0.07, batch_size=32, epochs=200, callbacks=[checkpoint])\n",
    "model.load_weights(model_filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "crazy-league",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_7\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_41 (Dense)             (None, 150)               46950     \n",
      "_________________________________________________________________\n",
      "dense_42 (Dense)             (None, 100)               15100     \n",
      "_________________________________________________________________\n",
      "dropout_13 (Dropout)         (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_43 (Dense)             (None, 80)                8080      \n",
      "_________________________________________________________________\n",
      "batch_normalization_15 (Batc (None, 80)                320       \n",
      "_________________________________________________________________\n",
      "dropout_14 (Dropout)         (None, 80)                0         \n",
      "_________________________________________________________________\n",
      "dense_44 (Dense)             (None, 50)                4050      \n",
      "_________________________________________________________________\n",
      "dense_45 (Dense)             (None, 30)                1530      \n",
      "_________________________________________________________________\n",
      "batch_normalization_16 (Batc (None, 30)                120       \n",
      "_________________________________________________________________\n",
      "dense_46 (Dense)             (None, 10)                310       \n",
      "_________________________________________________________________\n",
      "batch_normalization_17 (Batc (None, 10)                40        \n",
      "_________________________________________________________________\n",
      "dense_47 (Dense)             (None, 1)                 11        \n",
      "=================================================================\n",
      "Total params: 76,511\n",
      "Trainable params: 76,271\n",
      "Non-trainable params: 240\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "from keras.initializers import he_normal\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "\n",
    "#Inıtialising the ANN\n",
    "model = Sequential()\n",
    "#Adding the input layer and first hidden layer\n",
    "model.add(Dense(units =150, kernel_initializer=he_normal(seed=None), activation= 'tanh', \n",
    "                input_dim=X_train.shape[1]))\n",
    "\n",
    "\n",
    "\n",
    "#Add the first hidden layer\n",
    "model.add(Dense(units =100, kernel_initializer=he_normal(seed=None), activation= 'tanh'))\n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "#Add the second hidden layer\n",
    "model.add(Dense(units =80, kernel_initializer=he_normal(seed=None), activation= 'tanh'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "#Add the third hidden layer\n",
    "model.add(Dense(units =50, kernel_initializer=he_normal(seed=None), activation= 'tanh'))\n",
    "\n",
    "\n",
    "#Add the fourth hidden layer\n",
    "model.add(Dense(units =30, kernel_initializer=he_normal(seed=None), activation= 'relu'))\n",
    "model.add(BatchNormalization())\n",
    "\n",
    "#Add the fifth hidden layer\n",
    "model.add(Dense(units =10, kernel_initializer=he_normal(seed=None), activation= 'relu'))\n",
    "model.add(BatchNormalization())\n",
    "\n",
    "#The output layer\n",
    "model.add(Dense(units =1, kernel_initializer=he_normal(seed=None), activation= 'relu'))\n",
    "\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "pointed-repair",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "34/43 [======================>.......] - ETA: 0s - loss: 0.1075 - mse: 0.8046\n",
      "Epoch 00001: val_loss improved from inf to 0.05953, saving model to min_vl_model4.h5\n",
      "43/43 [==============================] - 1s 26ms/step - loss: 0.1030 - mse: 0.7749 - val_loss: 0.0595 - val_mse: 0.5864\n",
      "Epoch 2/200\n",
      "36/43 [========================>.....] - ETA: 0s - loss: 0.0570 - mse: 0.6127\n",
      "Epoch 00002: val_loss improved from 0.05953 to 0.04509, saving model to min_vl_model4.h5\n",
      "43/43 [==============================] - 0s 11ms/step - loss: 0.0570 - mse: 0.5912 - val_loss: 0.0451 - val_mse: 0.4634\n",
      "Epoch 3/200\n",
      "36/43 [========================>.....] - ETA: 0s - loss: 0.0468 - mse: 0.5530\n",
      "Epoch 00003: val_loss improved from 0.04509 to 0.04270, saving model to min_vl_model4.h5\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.0449 - mse: 0.5411 - val_loss: 0.0427 - val_mse: 0.4794\n",
      "Epoch 4/200\n",
      "37/43 [========================>.....] - ETA: 0s - loss: 0.0344 - mse: 0.4754\n",
      "Epoch 00004: val_loss did not improve from 0.04270\n",
      "43/43 [==============================] - 0s 7ms/step - loss: 0.0366 - mse: 0.5016 - val_loss: 0.0594 - val_mse: 0.5743\n",
      "Epoch 5/200\n",
      "37/43 [========================>.....] - ETA: 0s - loss: 0.0319 - mse: 0.4617\n",
      "Epoch 00005: val_loss improved from 0.04270 to 0.03990, saving model to min_vl_model4.h5\n",
      "43/43 [==============================] - 0s 11ms/step - loss: 0.0325 - mse: 0.4932 - val_loss: 0.0399 - val_mse: 0.4544\n",
      "Epoch 6/200\n",
      "40/43 [==========================>...] - ETA: 0s - loss: 0.0293 - mse: 0.4820\n",
      "Epoch 00006: val_loss improved from 0.03990 to 0.03452, saving model to min_vl_model4.h5\n",
      "43/43 [==============================] - 0s 11ms/step - loss: 0.0296 - mse: 0.4787 - val_loss: 0.0345 - val_mse: 0.4552\n",
      "Epoch 7/200\n",
      "41/43 [===========================>..] - ETA: 0s - loss: 0.0234 - mse: 0.4552\n",
      "Epoch 00007: val_loss did not improve from 0.03452\n",
      "43/43 [==============================] - 0s 7ms/step - loss: 0.0235 - mse: 0.4539 - val_loss: 0.0356 - val_mse: 0.4459\n",
      "Epoch 8/200\n",
      "35/43 [=======================>......] - ETA: 0s - loss: 0.0217 - mse: 0.4601\n",
      "Epoch 00008: val_loss did not improve from 0.03452\n",
      "43/43 [==============================] - 0s 7ms/step - loss: 0.0222 - mse: 0.4466 - val_loss: 0.0482 - val_mse: 0.5405\n",
      "Epoch 9/200\n",
      "38/43 [=========================>....] - ETA: 0s - loss: 0.0221 - mse: 0.4321\n",
      "Epoch 00009: val_loss improved from 0.03452 to 0.03005, saving model to min_vl_model4.h5\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.0231 - mse: 0.4476 - val_loss: 0.0300 - val_mse: 0.4182\n",
      "Epoch 10/200\n",
      "38/43 [=========================>....] - ETA: 0s - loss: 0.0197 - mse: 0.4371\n",
      "Epoch 00010: val_loss did not improve from 0.03005\n",
      "43/43 [==============================] - 0s 7ms/step - loss: 0.0202 - mse: 0.4404 - val_loss: 0.0350 - val_mse: 0.4573\n",
      "Epoch 11/200\n",
      "38/43 [=========================>....] - ETA: 0s - loss: 0.0224 - mse: 0.4497\n",
      "Epoch 00011: val_loss improved from 0.03005 to 0.02790, saving model to min_vl_model4.h5\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.0223 - mse: 0.4477 - val_loss: 0.0279 - val_mse: 0.4293\n",
      "Epoch 12/200\n",
      "38/43 [=========================>....] - ETA: 0s - loss: 0.0200 - mse: 0.4389\n",
      "Epoch 00012: val_loss did not improve from 0.02790\n",
      "43/43 [==============================] - 0s 7ms/step - loss: 0.0192 - mse: 0.4250 - val_loss: 0.0344 - val_mse: 0.4615\n",
      "Epoch 13/200\n",
      "39/43 [==========================>...] - ETA: 0s - loss: 0.0181 - mse: 0.4227\n",
      "Epoch 00013: val_loss did not improve from 0.02790\n",
      "43/43 [==============================] - 0s 7ms/step - loss: 0.0179 - mse: 0.4179 - val_loss: 0.0297 - val_mse: 0.4442\n",
      "Epoch 14/200\n",
      "36/43 [========================>.....] - ETA: 0s - loss: 0.0194 - mse: 0.4499\n",
      "Epoch 00014: val_loss did not improve from 0.02790\n",
      "43/43 [==============================] - 0s 7ms/step - loss: 0.0184 - mse: 0.4332 - val_loss: 0.0338 - val_mse: 0.4613\n",
      "Epoch 15/200\n",
      "38/43 [=========================>....] - ETA: 0s - loss: 0.0162 - mse: 0.4277\n",
      "Epoch 00015: val_loss did not improve from 0.02790\n",
      "43/43 [==============================] - 0s 7ms/step - loss: 0.0163 - mse: 0.4173 - val_loss: 0.0357 - val_mse: 0.4741\n",
      "Epoch 16/200\n",
      "37/43 [========================>.....] - ETA: 0s - loss: 0.0158 - mse: 0.4228\n",
      "Epoch 00016: val_loss did not improve from 0.02790\n",
      "43/43 [==============================] - 0s 7ms/step - loss: 0.0153 - mse: 0.4152 - val_loss: 0.0307 - val_mse: 0.4573\n",
      "Epoch 17/200\n",
      "39/43 [==========================>...] - ETA: 0s - loss: 0.0148 - mse: 0.4092\n",
      "Epoch 00017: val_loss did not improve from 0.02790\n",
      "43/43 [==============================] - 0s 7ms/step - loss: 0.0149 - mse: 0.4127 - val_loss: 0.0350 - val_mse: 0.4803\n",
      "Epoch 18/200\n",
      "41/43 [===========================>..] - ETA: 0s - loss: 0.0160 - mse: 0.4213\n",
      "Epoch 00018: val_loss did not improve from 0.02790\n",
      "43/43 [==============================] - 0s 7ms/step - loss: 0.0169 - mse: 0.4249 - val_loss: 0.0355 - val_mse: 0.4816\n",
      "Epoch 19/200\n",
      "40/43 [==========================>...] - ETA: 0s - loss: 0.0146 - mse: 0.4068\n",
      "Epoch 00019: val_loss did not improve from 0.02790\n",
      "43/43 [==============================] - 0s 7ms/step - loss: 0.0152 - mse: 0.4046 - val_loss: 0.0366 - val_mse: 0.4827\n",
      "Epoch 20/200\n",
      "36/43 [========================>.....] - ETA: 0s - loss: 0.0186 - mse: 0.4349\n",
      "Epoch 00020: val_loss did not improve from 0.02790\n",
      "43/43 [==============================] - 0s 7ms/step - loss: 0.0179 - mse: 0.4198 - val_loss: 0.0403 - val_mse: 0.5051\n",
      "Epoch 21/200\n",
      "38/43 [=========================>....] - ETA: 0s - loss: 0.0148 - mse: 0.4220\n",
      "Epoch 00021: val_loss did not improve from 0.02790\n",
      "43/43 [==============================] - 0s 7ms/step - loss: 0.0150 - mse: 0.4168 - val_loss: 0.0388 - val_mse: 0.5014\n",
      "Epoch 22/200\n",
      "42/43 [============================>.] - ETA: 0s - loss: 0.0123 - mse: 0.3936\n",
      "Epoch 00022: val_loss did not improve from 0.02790\n",
      "43/43 [==============================] - 0s 8ms/step - loss: 0.0124 - mse: 0.3944 - val_loss: 0.0404 - val_mse: 0.5139\n",
      "Epoch 23/200\n",
      "40/43 [==========================>...] - ETA: 0s - loss: 0.0136 - mse: 0.3985\n",
      "Epoch 00023: val_loss did not improve from 0.02790\n",
      "43/43 [==============================] - 0s 8ms/step - loss: 0.0142 - mse: 0.4062 - val_loss: 0.0432 - val_mse: 0.5390\n",
      "Epoch 24/200\n",
      "37/43 [========================>.....] - ETA: 0s - loss: 0.0116 - mse: 0.3921\n",
      "Epoch 00024: val_loss did not improve from 0.02790\n",
      "43/43 [==============================] - 0s 7ms/step - loss: 0.0121 - mse: 0.3977 - val_loss: 0.0474 - val_mse: 0.5746\n",
      "Epoch 25/200\n",
      "39/43 [==========================>...] - ETA: 0s - loss: 0.0118 - mse: 0.3915\n",
      "Epoch 00025: val_loss did not improve from 0.02790\n",
      "43/43 [==============================] - 0s 7ms/step - loss: 0.0121 - mse: 0.3906 - val_loss: 0.0436 - val_mse: 0.5626\n",
      "Epoch 26/200\n",
      "39/43 [==========================>...] - ETA: 0s - loss: 0.0109 - mse: 0.3866\n",
      "Epoch 00026: val_loss did not improve from 0.02790\n",
      "43/43 [==============================] - 0s 7ms/step - loss: 0.0108 - mse: 0.3820 - val_loss: 0.0427 - val_mse: 0.5453\n",
      "Epoch 27/200\n",
      "40/43 [==========================>...] - ETA: 0s - loss: 0.0105 - mse: 0.3805\n",
      "Epoch 00027: val_loss did not improve from 0.02790\n",
      "43/43 [==============================] - 0s 7ms/step - loss: 0.0110 - mse: 0.3885 - val_loss: 0.0433 - val_mse: 0.5413\n",
      "Epoch 28/200\n",
      "38/43 [=========================>....] - ETA: 0s - loss: 0.0119 - mse: 0.3943\n",
      "Epoch 00028: val_loss did not improve from 0.02790\n",
      "43/43 [==============================] - 0s 7ms/step - loss: 0.0118 - mse: 0.3906 - val_loss: 0.0441 - val_mse: 0.5458\n",
      "Epoch 29/200\n",
      "37/43 [========================>.....] - ETA: 0s - loss: 0.0117 - mse: 0.3868\n",
      "Epoch 00029: val_loss did not improve from 0.02790\n",
      "43/43 [==============================] - 0s 7ms/step - loss: 0.0122 - mse: 0.3901 - val_loss: 0.0458 - val_mse: 0.5435\n",
      "Epoch 30/200\n",
      "38/43 [=========================>....] - ETA: 0s - loss: 0.0117 - mse: 0.4042\n",
      "Epoch 00030: val_loss did not improve from 0.02790\n",
      "43/43 [==============================] - 0s 7ms/step - loss: 0.0116 - mse: 0.3945 - val_loss: 0.0438 - val_mse: 0.5286\n",
      "Epoch 31/200\n",
      "37/43 [========================>.....] - ETA: 0s - loss: 0.0090 - mse: 0.3764\n",
      "Epoch 00031: val_loss did not improve from 0.02790\n",
      "43/43 [==============================] - 0s 7ms/step - loss: 0.0092 - mse: 0.3801 - val_loss: 0.0441 - val_mse: 0.5306\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 32/200\n",
      "36/43 [========================>.....] - ETA: 0s - loss: 0.0103 - mse: 0.3865\n",
      "Epoch 00032: val_loss did not improve from 0.02790\n",
      "43/43 [==============================] - 0s 8ms/step - loss: 0.0104 - mse: 0.3798 - val_loss: 0.0468 - val_mse: 0.5380\n",
      "Epoch 33/200\n",
      "38/43 [=========================>....] - ETA: 0s - loss: 0.0107 - mse: 0.4040\n",
      "Epoch 00033: val_loss did not improve from 0.02790\n",
      "43/43 [==============================] - 0s 7ms/step - loss: 0.0107 - mse: 0.3937 - val_loss: 0.0488 - val_mse: 0.5596\n",
      "Epoch 34/200\n",
      "38/43 [=========================>....] - ETA: 0s - loss: 0.0113 - mse: 0.3897\n",
      "Epoch 00034: val_loss did not improve from 0.02790\n",
      "43/43 [==============================] - 0s 8ms/step - loss: 0.0110 - mse: 0.3868 - val_loss: 0.0543 - val_mse: 0.6242\n",
      "Epoch 35/200\n",
      "40/43 [==========================>...] - ETA: 0s - loss: 0.0083 - mse: 0.3722\n",
      "Epoch 00035: val_loss did not improve from 0.02790\n",
      "43/43 [==============================] - 0s 7ms/step - loss: 0.0084 - mse: 0.3696 - val_loss: 0.0530 - val_mse: 0.6015\n",
      "Epoch 36/200\n",
      "37/43 [========================>.....] - ETA: 0s - loss: 0.0099 - mse: 0.3881\n",
      "Epoch 00036: val_loss did not improve from 0.02790\n",
      "43/43 [==============================] - 0s 9ms/step - loss: 0.0099 - mse: 0.3757 - val_loss: 0.0519 - val_mse: 0.5997\n",
      "Epoch 37/200\n",
      "36/43 [========================>.....] - ETA: 0s - loss: 0.0101 - mse: 0.3462\n",
      "Epoch 00037: val_loss did not improve from 0.02790\n",
      "43/43 [==============================] - 0s 8ms/step - loss: 0.0112 - mse: 0.3898 - val_loss: 0.0404 - val_mse: 0.5142\n",
      "Epoch 38/200\n",
      "37/43 [========================>.....] - ETA: 0s - loss: 0.0095 - mse: 0.3747\n",
      "Epoch 00038: val_loss did not improve from 0.02790\n",
      "43/43 [==============================] - 0s 7ms/step - loss: 0.0093 - mse: 0.3749 - val_loss: 0.0395 - val_mse: 0.5076\n",
      "Epoch 39/200\n",
      "42/43 [============================>.] - ETA: 0s - loss: 0.0093 - mse: 0.3723\n",
      "Epoch 00039: val_loss did not improve from 0.02790\n",
      "43/43 [==============================] - 0s 7ms/step - loss: 0.0093 - mse: 0.3703 - val_loss: 0.0427 - val_mse: 0.5336\n",
      "Epoch 40/200\n",
      "42/43 [============================>.] - ETA: 0s - loss: 0.0094 - mse: 0.3747\n",
      "Epoch 00040: val_loss did not improve from 0.02790\n",
      "43/43 [==============================] - 0s 8ms/step - loss: 0.0094 - mse: 0.3737 - val_loss: 0.0465 - val_mse: 0.5777\n",
      "Epoch 41/200\n",
      "40/43 [==========================>...] - ETA: 0s - loss: 0.0080 - mse: 0.3709\n",
      "Epoch 00041: val_loss did not improve from 0.02790\n",
      "43/43 [==============================] - 0s 6ms/step - loss: 0.0084 - mse: 0.3703 - val_loss: 0.0404 - val_mse: 0.5282\n",
      "Epoch 42/200\n",
      "43/43 [==============================] - ETA: 0s - loss: 0.0091 - mse: 0.3696\n",
      "Epoch 00042: val_loss did not improve from 0.02790\n",
      "43/43 [==============================] - 0s 6ms/step - loss: 0.0091 - mse: 0.3696 - val_loss: 0.0423 - val_mse: 0.5265\n",
      "Epoch 43/200\n",
      "40/43 [==========================>...] - ETA: 0s - loss: 0.0090 - mse: 0.3671\n",
      "Epoch 00043: val_loss did not improve from 0.02790\n",
      "43/43 [==============================] - 0s 6ms/step - loss: 0.0091 - mse: 0.3780 - val_loss: 0.0470 - val_mse: 0.5616\n",
      "Epoch 44/200\n",
      "42/43 [============================>.] - ETA: 0s - loss: 0.0090 - mse: 0.3669\n",
      "Epoch 00044: val_loss did not improve from 0.02790\n",
      "43/43 [==============================] - 0s 6ms/step - loss: 0.0091 - mse: 0.3661 - val_loss: 0.0449 - val_mse: 0.5568\n",
      "Epoch 45/200\n",
      "42/43 [============================>.] - ETA: 0s - loss: 0.0097 - mse: 0.3726\n",
      "Epoch 00045: val_loss did not improve from 0.02790\n",
      "43/43 [==============================] - 0s 6ms/step - loss: 0.0098 - mse: 0.3751 - val_loss: 0.0441 - val_mse: 0.5413\n",
      "Epoch 46/200\n",
      "39/43 [==========================>...] - ETA: 0s - loss: 0.0090 - mse: 0.3685\n",
      "Epoch 00046: val_loss did not improve from 0.02790\n",
      "43/43 [==============================] - 0s 6ms/step - loss: 0.0088 - mse: 0.3741 - val_loss: 0.0439 - val_mse: 0.5763\n",
      "Epoch 47/200\n",
      "39/43 [==========================>...] - ETA: 0s - loss: 0.0103 - mse: 0.3860\n",
      "Epoch 00047: val_loss did not improve from 0.02790\n",
      "43/43 [==============================] - 0s 7ms/step - loss: 0.0104 - mse: 0.3774 - val_loss: 0.0478 - val_mse: 0.6201\n",
      "Epoch 48/200\n",
      "42/43 [============================>.] - ETA: 0s - loss: 0.0079 - mse: 0.3605\n",
      "Epoch 00048: val_loss did not improve from 0.02790\n",
      "43/43 [==============================] - 0s 7ms/step - loss: 0.0079 - mse: 0.3600 - val_loss: 0.0421 - val_mse: 0.5501\n",
      "Epoch 49/200\n",
      "42/43 [============================>.] - ETA: 0s - loss: 0.0079 - mse: 0.3635\n",
      "Epoch 00049: val_loss did not improve from 0.02790\n",
      "43/43 [==============================] - 0s 6ms/step - loss: 0.0079 - mse: 0.3656 - val_loss: 0.0441 - val_mse: 0.5388\n",
      "Epoch 50/200\n",
      "40/43 [==========================>...] - ETA: 0s - loss: 0.0085 - mse: 0.3698\n",
      "Epoch 00050: val_loss did not improve from 0.02790\n",
      "43/43 [==============================] - 0s 6ms/step - loss: 0.0084 - mse: 0.3706 - val_loss: 0.0468 - val_mse: 0.5633\n",
      "Epoch 51/200\n",
      "39/43 [==========================>...] - ETA: 0s - loss: 0.0074 - mse: 0.3613\n",
      "Epoch 00051: val_loss did not improve from 0.02790\n",
      "43/43 [==============================] - 0s 7ms/step - loss: 0.0073 - mse: 0.3633 - val_loss: 0.0485 - val_mse: 0.6042\n",
      "Epoch 52/200\n",
      "39/43 [==========================>...] - ETA: 0s - loss: 0.0076 - mse: 0.3544\n",
      "Epoch 00052: val_loss did not improve from 0.02790\n",
      "43/43 [==============================] - 0s 7ms/step - loss: 0.0077 - mse: 0.3588 - val_loss: 0.0481 - val_mse: 0.6000\n",
      "Epoch 53/200\n",
      "41/43 [===========================>..] - ETA: 0s - loss: 0.0074 - mse: 0.3529\n",
      "Epoch 00053: val_loss did not improve from 0.02790\n",
      "43/43 [==============================] - 0s 8ms/step - loss: 0.0074 - mse: 0.3518 - val_loss: 0.0452 - val_mse: 0.5954\n",
      "Epoch 54/200\n",
      "36/43 [========================>.....] - ETA: 0s - loss: 0.0064 - mse: 0.3406\n",
      "Epoch 00054: val_loss did not improve from 0.02790\n",
      "43/43 [==============================] - 0s 7ms/step - loss: 0.0073 - mse: 0.3545 - val_loss: 0.0415 - val_mse: 0.5491\n",
      "Epoch 55/200\n",
      "32/43 [=====================>........] - ETA: 0s - loss: 0.0073 - mse: 0.3512\n",
      "Epoch 00055: val_loss did not improve from 0.02790\n",
      "43/43 [==============================] - 0s 6ms/step - loss: 0.0074 - mse: 0.3566 - val_loss: 0.0421 - val_mse: 0.5641\n",
      "Epoch 56/200\n",
      "39/43 [==========================>...] - ETA: 0s - loss: 0.0064 - mse: 0.3494\n",
      "Epoch 00056: val_loss did not improve from 0.02790\n",
      "43/43 [==============================] - 0s 7ms/step - loss: 0.0068 - mse: 0.3536 - val_loss: 0.0488 - val_mse: 0.6012\n",
      "Epoch 57/200\n",
      "36/43 [========================>.....] - ETA: 0s - loss: 0.0063 - mse: 0.3464- ETA: 0s - loss: 0.0063 - mse: 0.36\n",
      "Epoch 00057: val_loss did not improve from 0.02790\n",
      "43/43 [==============================] - 0s 7ms/step - loss: 0.0065 - mse: 0.3502 - val_loss: 0.0416 - val_mse: 0.5384\n",
      "Epoch 58/200\n",
      "36/43 [========================>.....] - ETA: 0s - loss: 0.0065 - mse: 0.3632\n",
      "Epoch 00058: val_loss did not improve from 0.02790\n",
      "43/43 [==============================] - 0s 7ms/step - loss: 0.0065 - mse: 0.3543 - val_loss: 0.0438 - val_mse: 0.5640\n",
      "Epoch 59/200\n",
      "43/43 [==============================] - ETA: 0s - loss: 0.0062 - mse: 0.3501\n",
      "Epoch 00059: val_loss did not improve from 0.02790\n",
      "43/43 [==============================] - 0s 6ms/step - loss: 0.0062 - mse: 0.3501 - val_loss: 0.0450 - val_mse: 0.5762\n",
      "Epoch 60/200\n",
      "42/43 [============================>.] - ETA: 0s - loss: 0.0065 - mse: 0.3525\n",
      "Epoch 00060: val_loss did not improve from 0.02790\n",
      "43/43 [==============================] - 0s 6ms/step - loss: 0.0065 - mse: 0.3521 - val_loss: 0.0464 - val_mse: 0.5997\n",
      "Epoch 61/200\n",
      "37/43 [========================>.....] - ETA: 0s - loss: 0.0052 - mse: 0.3433\n",
      "Epoch 00061: val_loss did not improve from 0.02790\n",
      "43/43 [==============================] - 0s 7ms/step - loss: 0.0052 - mse: 0.3434 - val_loss: 0.0439 - val_mse: 0.5532\n",
      "Epoch 62/200\n",
      "41/43 [===========================>..] - ETA: 0s - loss: 0.0056 - mse: 0.3482\n",
      "Epoch 00062: val_loss did not improve from 0.02790\n",
      "43/43 [==============================] - 0s 6ms/step - loss: 0.0057 - mse: 0.3497 - val_loss: 0.0536 - val_mse: 0.6294\n",
      "Epoch 63/200\n",
      "37/43 [========================>.....] - ETA: 0s - loss: 0.0052 - mse: 0.3317\n",
      "Epoch 00063: val_loss did not improve from 0.02790\n",
      "43/43 [==============================] - 0s 7ms/step - loss: 0.0053 - mse: 0.3410 - val_loss: 0.0449 - val_mse: 0.5684\n",
      "Epoch 64/200\n",
      "38/43 [=========================>....] - ETA: 0s - loss: 0.0083 - mse: 0.3457\n",
      "Epoch 00064: val_loss did not improve from 0.02790\n",
      "43/43 [==============================] - 0s 7ms/step - loss: 0.0084 - mse: 0.3586 - val_loss: 0.0560 - val_mse: 0.6588\n",
      "Epoch 65/200\n",
      "41/43 [===========================>..] - ETA: 0s - loss: 0.0060 - mse: 0.3486\n",
      "Epoch 00065: val_loss did not improve from 0.02790\n",
      "43/43 [==============================] - 0s 8ms/step - loss: 0.0059 - mse: 0.3450 - val_loss: 0.0540 - val_mse: 0.6409\n",
      "Epoch 66/200\n",
      "35/43 [=======================>......] - ETA: 0s - loss: 0.0068 - mse: 0.3617- ETA: 0s - loss: 0.0071 - mse: 0.38\n",
      "Epoch 00066: val_loss did not improve from 0.02790\n",
      "43/43 [==============================] - 0s 7ms/step - loss: 0.0064 - mse: 0.3501 - val_loss: 0.0504 - val_mse: 0.5988\n",
      "Epoch 67/200\n",
      "40/43 [==========================>...] - ETA: 0s - loss: 0.0060 - mse: 0.3542\n",
      "Epoch 00067: val_loss did not improve from 0.02790\n",
      "43/43 [==============================] - 0s 8ms/step - loss: 0.0060 - mse: 0.3486 - val_loss: 0.0543 - val_mse: 0.6301\n",
      "Epoch 68/200\n",
      "40/43 [==========================>...] - ETA: 0s - loss: 0.0063 - mse: 0.3466\n",
      "Epoch 00068: val_loss did not improve from 0.02790\n",
      "43/43 [==============================] - 0s 6ms/step - loss: 0.0066 - mse: 0.3461 - val_loss: 0.0530 - val_mse: 0.6424\n",
      "Epoch 69/200\n",
      "41/43 [===========================>..] - ETA: 0s - loss: 0.0053 - mse: 0.3322\n",
      "Epoch 00069: val_loss did not improve from 0.02790\n",
      "43/43 [==============================] - 0s 8ms/step - loss: 0.0055 - mse: 0.3410 - val_loss: 0.0568 - val_mse: 0.6547\n",
      "Epoch 70/200\n",
      "36/43 [========================>.....] - ETA: 0s - loss: 0.0059 - mse: 0.3406\n",
      "Epoch 00070: val_loss did not improve from 0.02790\n",
      "43/43 [==============================] - 0s 7ms/step - loss: 0.0060 - mse: 0.3438 - val_loss: 0.0564 - val_mse: 0.6156\n",
      "Epoch 71/200\n",
      "41/43 [===========================>..] - ETA: 0s - loss: 0.0073 - mse: 0.3512\n",
      "Epoch 00071: val_loss did not improve from 0.02790\n",
      "43/43 [==============================] - 0s 8ms/step - loss: 0.0071 - mse: 0.3516 - val_loss: 0.0687 - val_mse: 0.7697\n",
      "Epoch 72/200\n",
      "36/43 [========================>.....] - ETA: 0s - loss: 0.0053 - mse: 0.3404\n",
      "Epoch 00072: val_loss did not improve from 0.02790\n",
      "43/43 [==============================] - 0s 7ms/step - loss: 0.0063 - mse: 0.3570 - val_loss: 0.0672 - val_mse: 0.7743\n",
      "Epoch 73/200\n",
      "37/43 [========================>.....] - ETA: 0s - loss: 0.0069 - mse: 0.3488\n",
      "Epoch 00073: val_loss did not improve from 0.02790\n",
      "43/43 [==============================] - 0s 7ms/step - loss: 0.0068 - mse: 0.3516 - val_loss: 0.0614 - val_mse: 0.7040\n",
      "Epoch 74/200\n",
      "42/43 [============================>.] - ETA: 0s - loss: 0.0062 - mse: 0.3571\n",
      "Epoch 00074: val_loss did not improve from 0.02790\n",
      "43/43 [==============================] - 0s 7ms/step - loss: 0.0063 - mse: 0.3562 - val_loss: 0.0494 - val_mse: 0.5855\n",
      "Epoch 75/200\n",
      "37/43 [========================>.....] - ETA: 0s - loss: 0.0075 - mse: 0.3640\n",
      "Epoch 00075: val_loss did not improve from 0.02790\n",
      "43/43 [==============================] - 0s 7ms/step - loss: 0.0071 - mse: 0.3540 - val_loss: 0.0540 - val_mse: 0.6408\n",
      "Epoch 76/200\n",
      "36/43 [========================>.....] - ETA: 0s - loss: 0.0058 - mse: 0.3423\n",
      "Epoch 00076: val_loss did not improve from 0.02790\n",
      "43/43 [==============================] - 0s 8ms/step - loss: 0.0058 - mse: 0.3430 - val_loss: 0.0556 - val_mse: 0.6435\n",
      "Epoch 77/200\n",
      "39/43 [==========================>...] - ETA: 0s - loss: 0.0053 - mse: 0.3478\n",
      "Epoch 00077: val_loss did not improve from 0.02790\n",
      "43/43 [==============================] - 0s 7ms/step - loss: 0.0054 - mse: 0.3387 - val_loss: 0.0500 - val_mse: 0.5947\n",
      "Epoch 78/200\n",
      "39/43 [==========================>...] - ETA: 0s - loss: 0.0045 - mse: 0.3364\n",
      "Epoch 00078: val_loss did not improve from 0.02790\n",
      "43/43 [==============================] - 0s 7ms/step - loss: 0.0045 - mse: 0.3357 - val_loss: 0.0469 - val_mse: 0.5542\n",
      "Epoch 79/200\n",
      "41/43 [===========================>..] - ETA: 0s - loss: 0.0056 - mse: 0.3526\n",
      "Epoch 00079: val_loss did not improve from 0.02790\n",
      "43/43 [==============================] - 0s 8ms/step - loss: 0.0055 - mse: 0.3502 - val_loss: 0.0536 - val_mse: 0.6060\n",
      "Epoch 80/200\n",
      "36/43 [========================>.....] - ETA: 0s - loss: 0.0045 - mse: 0.3346\n",
      "Epoch 00080: val_loss did not improve from 0.02790\n",
      "43/43 [==============================] - 0s 7ms/step - loss: 0.0044 - mse: 0.3327 - val_loss: 0.0552 - val_mse: 0.6156\n",
      "Epoch 81/200\n",
      "42/43 [============================>.] - ETA: 0s - loss: 0.0046 - mse: 0.3378\n",
      "Epoch 00081: val_loss did not improve from 0.02790\n",
      "43/43 [==============================] - 0s 8ms/step - loss: 0.0046 - mse: 0.3386 - val_loss: 0.0555 - val_mse: 0.6315\n",
      "Epoch 82/200\n",
      "36/43 [========================>.....] - ETA: 0s - loss: 0.0048 - mse: 0.3300\n",
      "Epoch 00082: val_loss did not improve from 0.02790\n",
      "43/43 [==============================] - 0s 7ms/step - loss: 0.0048 - mse: 0.3328 - val_loss: 0.0578 - val_mse: 0.6599\n",
      "Epoch 83/200\n",
      "36/43 [========================>.....] - ETA: 0s - loss: 0.0043 - mse: 0.3348\n",
      "Epoch 00083: val_loss did not improve from 0.02790\n",
      "43/43 [==============================] - 0s 8ms/step - loss: 0.0044 - mse: 0.3345 - val_loss: 0.0558 - val_mse: 0.6346\n",
      "Epoch 84/200\n",
      "40/43 [==========================>...] - ETA: 0s - loss: 0.0050 - mse: 0.3456\n",
      "Epoch 00084: val_loss did not improve from 0.02790\n",
      "43/43 [==============================] - 0s 7ms/step - loss: 0.0051 - mse: 0.3402 - val_loss: 0.0542 - val_mse: 0.6135\n",
      "Epoch 85/200\n",
      "36/43 [========================>.....] - ETA: 0s - loss: 0.0046 - mse: 0.3233\n",
      "Epoch 00085: val_loss did not improve from 0.02790\n",
      "43/43 [==============================] - 0s 7ms/step - loss: 0.0049 - mse: 0.3379 - val_loss: 0.0532 - val_mse: 0.6072\n",
      "Epoch 86/200\n",
      "43/43 [==============================] - ETA: 0s - loss: 0.0045 - mse: 0.3369\n",
      "Epoch 00086: val_loss did not improve from 0.02790\n",
      "43/43 [==============================] - 0s 8ms/step - loss: 0.0045 - mse: 0.3369 - val_loss: 0.0529 - val_mse: 0.6136\n",
      "Epoch 87/200\n",
      "41/43 [===========================>..] - ETA: 0s - loss: 0.0046 - mse: 0.3357\n",
      "Epoch 00087: val_loss did not improve from 0.02790\n",
      "43/43 [==============================] - 0s 7ms/step - loss: 0.0045 - mse: 0.3379 - val_loss: 0.0555 - val_mse: 0.6428\n",
      "Epoch 88/200\n",
      "42/43 [============================>.] - ETA: 0s - loss: 0.0042 - mse: 0.3276\n",
      "Epoch 00088: val_loss did not improve from 0.02790\n",
      "43/43 [==============================] - 0s 8ms/step - loss: 0.0042 - mse: 0.3290 - val_loss: 0.0527 - val_mse: 0.6227\n",
      "Epoch 89/200\n",
      "39/43 [==========================>...] - ETA: 0s - loss: 0.0042 - mse: 0.3454\n",
      "Epoch 00089: val_loss did not improve from 0.02790\n",
      "43/43 [==============================] - 0s 7ms/step - loss: 0.0044 - mse: 0.3349 - val_loss: 0.0591 - val_mse: 0.6761\n",
      "Epoch 90/200\n",
      "38/43 [=========================>....] - ETA: 0s - loss: 0.0062 - mse: 0.3250\n",
      "Epoch 00090: val_loss did not improve from 0.02790\n",
      "43/43 [==============================] - 0s 7ms/step - loss: 0.0064 - mse: 0.3357 - val_loss: 0.0580 - val_mse: 0.6455\n",
      "Epoch 91/200\n",
      "38/43 [=========================>....] - ETA: 0s - loss: 0.0071 - mse: 0.3445\n",
      "Epoch 00091: val_loss did not improve from 0.02790\n",
      "43/43 [==============================] - 0s 7ms/step - loss: 0.0069 - mse: 0.3485 - val_loss: 0.0731 - val_mse: 0.8515\n",
      "Epoch 92/200\n",
      "39/43 [==========================>...] - ETA: 0s - loss: 0.0063 - mse: 0.3438\n",
      "Epoch 00092: val_loss did not improve from 0.02790\n",
      "43/43 [==============================] - 0s 7ms/step - loss: 0.0062 - mse: 0.3415 - val_loss: 0.0803 - val_mse: 0.9487\n",
      "Epoch 93/200\n",
      "38/43 [=========================>....] - ETA: 0s - loss: 0.0059 - mse: 0.3387\n",
      "Epoch 00093: val_loss did not improve from 0.02790\n",
      "43/43 [==============================] - 0s 7ms/step - loss: 0.0057 - mse: 0.3362 - val_loss: 0.0675 - val_mse: 0.7833\n",
      "Epoch 94/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "43/43 [==============================] - ETA: 0s - loss: 0.0046 - mse: 0.3319\n",
      "Epoch 00094: val_loss did not improve from 0.02790\n",
      "43/43 [==============================] - 0s 7ms/step - loss: 0.0046 - mse: 0.3319 - val_loss: 0.0773 - val_mse: 0.9134\n",
      "Epoch 95/200\n",
      "40/43 [==========================>...] - ETA: 0s - loss: 0.0051 - mse: 0.3365\n",
      "Epoch 00095: val_loss did not improve from 0.02790\n",
      "43/43 [==============================] - 0s 6ms/step - loss: 0.0050 - mse: 0.3366 - val_loss: 0.0710 - val_mse: 0.8474\n",
      "Epoch 96/200\n",
      "42/43 [============================>.] - ETA: 0s - loss: 0.0047 - mse: 0.3326\n",
      "Epoch 00096: val_loss did not improve from 0.02790\n",
      "43/43 [==============================] - 0s 8ms/step - loss: 0.0047 - mse: 0.3336 - val_loss: 0.0625 - val_mse: 0.7198\n",
      "Epoch 97/200\n",
      "39/43 [==========================>...] - ETA: 0s - loss: 0.0038 - mse: 0.3337\n",
      "Epoch 00097: val_loss did not improve from 0.02790\n",
      "43/43 [==============================] - 0s 6ms/step - loss: 0.0038 - mse: 0.3331 - val_loss: 0.0636 - val_mse: 0.7335\n",
      "Epoch 98/200\n",
      "43/43 [==============================] - ETA: 0s - loss: 0.0045 - mse: 0.3384\n",
      "Epoch 00098: val_loss did not improve from 0.02790\n",
      "43/43 [==============================] - 0s 8ms/step - loss: 0.0045 - mse: 0.3384 - val_loss: 0.0708 - val_mse: 0.8308\n",
      "Epoch 99/200\n",
      "40/43 [==========================>...] - ETA: 0s - loss: 0.0048 - mse: 0.3391\n",
      "Epoch 00099: val_loss did not improve from 0.02790\n",
      "43/43 [==============================] - 0s 6ms/step - loss: 0.0047 - mse: 0.3381 - val_loss: 0.0703 - val_mse: 0.8216\n",
      "Epoch 100/200\n",
      "40/43 [==========================>...] - ETA: 0s - loss: 0.0048 - mse: 0.3346\n",
      "Epoch 00100: val_loss did not improve from 0.02790\n",
      "43/43 [==============================] - 0s 6ms/step - loss: 0.0047 - mse: 0.3377 - val_loss: 0.0672 - val_mse: 0.7954\n",
      "Epoch 101/200\n",
      "41/43 [===========================>..] - ETA: 0s - loss: 0.0049 - mse: 0.3324\n",
      "Epoch 00101: val_loss did not improve from 0.02790\n",
      "43/43 [==============================] - 0s 6ms/step - loss: 0.0048 - mse: 0.3359 - val_loss: 0.0701 - val_mse: 0.8087\n",
      "Epoch 102/200\n",
      "40/43 [==========================>...] - ETA: 0s - loss: 0.0044 - mse: 0.3347\n",
      "Epoch 00102: val_loss did not improve from 0.02790\n",
      "43/43 [==============================] - 0s 6ms/step - loss: 0.0045 - mse: 0.3378 - val_loss: 0.0680 - val_mse: 0.7859\n",
      "Epoch 103/200\n",
      "41/43 [===========================>..] - ETA: 0s - loss: 0.0047 - mse: 0.3417\n",
      "Epoch 00103: val_loss did not improve from 0.02790\n",
      "43/43 [==============================] - 0s 6ms/step - loss: 0.0049 - mse: 0.3391 - val_loss: 0.0605 - val_mse: 0.6985\n",
      "Epoch 104/200\n",
      "41/43 [===========================>..] - ETA: 0s - loss: 0.0046 - mse: 0.3315\n",
      "Epoch 00104: val_loss did not improve from 0.02790\n",
      "43/43 [==============================] - 0s 6ms/step - loss: 0.0046 - mse: 0.3331 - val_loss: 0.0620 - val_mse: 0.7275\n",
      "Epoch 105/200\n",
      "33/43 [======================>.......] - ETA: 0s - loss: 0.0045 - mse: 0.3392\n",
      "Epoch 00105: val_loss did not improve from 0.02790\n",
      "43/43 [==============================] - 0s 6ms/step - loss: 0.0042 - mse: 0.3340 - val_loss: 0.0672 - val_mse: 0.7783\n",
      "Epoch 106/200\n",
      "36/43 [========================>.....] - ETA: 0s - loss: 0.0040 - mse: 0.3266\n",
      "Epoch 00106: val_loss did not improve from 0.02790\n",
      "43/43 [==============================] - 0s 7ms/step - loss: 0.0044 - mse: 0.3350 - val_loss: 0.0578 - val_mse: 0.6813\n",
      "Epoch 107/200\n",
      "37/43 [========================>.....] - ETA: 0s - loss: 0.0034 - mse: 0.3178\n",
      "Epoch 00107: val_loss did not improve from 0.02790\n",
      "43/43 [==============================] - 0s 7ms/step - loss: 0.0038 - mse: 0.3321 - val_loss: 0.0637 - val_mse: 0.7344\n",
      "Epoch 108/200\n",
      "41/43 [===========================>..] - ETA: 0s - loss: 0.0039 - mse: 0.3360\n",
      "Epoch 00108: val_loss did not improve from 0.02790\n",
      "43/43 [==============================] - 0s 6ms/step - loss: 0.0040 - mse: 0.3326 - val_loss: 0.0660 - val_mse: 0.7754\n",
      "Epoch 109/200\n",
      "40/43 [==========================>...] - ETA: 0s - loss: 0.0041 - mse: 0.3373\n",
      "Epoch 00109: val_loss did not improve from 0.02790\n",
      "43/43 [==============================] - 0s 6ms/step - loss: 0.0041 - mse: 0.3307 - val_loss: 0.0619 - val_mse: 0.7654\n",
      "Epoch 110/200\n",
      "35/43 [=======================>......] - ETA: 0s - loss: 0.0045 - mse: 0.3311\n",
      "Epoch 00110: val_loss did not improve from 0.02790\n",
      "43/43 [==============================] - 0s 7ms/step - loss: 0.0046 - mse: 0.3331 - val_loss: 0.0583 - val_mse: 0.7095\n",
      "Epoch 111/200\n",
      "39/43 [==========================>...] - ETA: 0s - loss: 0.0042 - mse: 0.3336\n",
      "Epoch 00111: val_loss did not improve from 0.02790\n",
      "43/43 [==============================] - 0s 6ms/step - loss: 0.0042 - mse: 0.3292 - val_loss: 0.0681 - val_mse: 0.8332\n",
      "Epoch 112/200\n",
      "37/43 [========================>.....] - ETA: 0s - loss: 0.0040 - mse: 0.3375\n",
      "Epoch 00112: val_loss did not improve from 0.02790\n",
      "43/43 [==============================] - 0s 7ms/step - loss: 0.0040 - mse: 0.3286 - val_loss: 0.0641 - val_mse: 0.7652\n",
      "Epoch 113/200\n",
      "35/43 [=======================>......] - ETA: 0s - loss: 0.0035 - mse: 0.3366\n",
      "Epoch 00113: val_loss did not improve from 0.02790\n",
      "43/43 [==============================] - 0s 7ms/step - loss: 0.0035 - mse: 0.3262 - val_loss: 0.0600 - val_mse: 0.7078\n",
      "Epoch 114/200\n",
      "40/43 [==========================>...] - ETA: 0s - loss: 0.0054 - mse: 0.3324\n",
      "Epoch 00114: val_loss did not improve from 0.02790\n",
      "43/43 [==============================] - 0s 6ms/step - loss: 0.0056 - mse: 0.3350 - val_loss: 0.0637 - val_mse: 0.7965\n",
      "Epoch 115/200\n",
      "42/43 [============================>.] - ETA: 0s - loss: 0.0051 - mse: 0.3387\n",
      "Epoch 00115: val_loss did not improve from 0.02790\n",
      "43/43 [==============================] - 0s 8ms/step - loss: 0.0052 - mse: 0.3372 - val_loss: 0.0594 - val_mse: 0.7307\n",
      "Epoch 116/200\n",
      "42/43 [============================>.] - ETA: 0s - loss: 0.0049 - mse: 0.3374\n",
      "Epoch 00116: val_loss did not improve from 0.02790\n",
      "43/43 [==============================] - 0s 6ms/step - loss: 0.0049 - mse: 0.3387 - val_loss: 0.0495 - val_mse: 0.6183\n",
      "Epoch 117/200\n",
      "42/43 [============================>.] - ETA: 0s - loss: 0.0036 - mse: 0.3289\n",
      "Epoch 00117: val_loss did not improve from 0.02790\n",
      "43/43 [==============================] - 0s 6ms/step - loss: 0.0038 - mse: 0.3298 - val_loss: 0.0563 - val_mse: 0.7852\n",
      "Epoch 118/200\n",
      "40/43 [==========================>...] - ETA: 0s - loss: 0.0048 - mse: 0.3349\n",
      "Epoch 00118: val_loss did not improve from 0.02790\n",
      "43/43 [==============================] - 0s 6ms/step - loss: 0.0048 - mse: 0.3342 - val_loss: 0.0560 - val_mse: 0.7407\n",
      "Epoch 119/200\n",
      "40/43 [==========================>...] - ETA: 0s - loss: 0.0035 - mse: 0.3270\n",
      "Epoch 00119: val_loss did not improve from 0.02790\n",
      "43/43 [==============================] - 0s 6ms/step - loss: 0.0041 - mse: 0.3294 - val_loss: 0.0524 - val_mse: 0.6945\n",
      "Epoch 120/200\n",
      "42/43 [============================>.] - ETA: 0s - loss: 0.0032 - mse: 0.3215\n",
      "Epoch 00120: val_loss did not improve from 0.02790\n",
      "43/43 [==============================] - 0s 6ms/step - loss: 0.0032 - mse: 0.3235 - val_loss: 0.0631 - val_mse: 0.7887\n",
      "Epoch 121/200\n",
      "40/43 [==========================>...] - ETA: 0s - loss: 0.0039 - mse: 0.3327\n",
      "Epoch 00121: val_loss did not improve from 0.02790\n",
      "43/43 [==============================] - 0s 6ms/step - loss: 0.0039 - mse: 0.3324 - val_loss: 0.0566 - val_mse: 0.6902\n",
      "Epoch 122/200\n",
      "40/43 [==========================>...] - ETA: 0s - loss: 0.0042 - mse: 0.3279\n",
      "Epoch 00122: val_loss did not improve from 0.02790\n",
      "43/43 [==============================] - 0s 6ms/step - loss: 0.0043 - mse: 0.3315 - val_loss: 0.0493 - val_mse: 0.6524\n",
      "Epoch 123/200\n",
      "42/43 [============================>.] - ETA: 0s - loss: 0.0041 - mse: 0.3285\n",
      "Epoch 00123: val_loss did not improve from 0.02790\n",
      "43/43 [==============================] - 0s 6ms/step - loss: 0.0041 - mse: 0.3279 - val_loss: 0.0468 - val_mse: 0.5813\n",
      "Epoch 124/200\n",
      "40/43 [==========================>...] - ETA: 0s - loss: 0.0038 - mse: 0.3309\n",
      "Epoch 00124: val_loss did not improve from 0.02790\n",
      "43/43 [==============================] - 0s 6ms/step - loss: 0.0037 - mse: 0.3297 - val_loss: 0.0621 - val_mse: 0.6935\n",
      "Epoch 125/200\n",
      "35/43 [=======================>......] - ETA: 0s - loss: 0.0030 - mse: 0.3285\n",
      "Epoch 00125: val_loss did not improve from 0.02790\n",
      "43/43 [==============================] - 0s 7ms/step - loss: 0.0034 - mse: 0.3317 - val_loss: 0.0599 - val_mse: 0.6735\n",
      "Epoch 126/200\n",
      "42/43 [============================>.] - ETA: 0s - loss: 0.0029 - mse: 0.3179\n",
      "Epoch 00126: val_loss did not improve from 0.02790\n",
      "43/43 [==============================] - 0s 6ms/step - loss: 0.0029 - mse: 0.3179 - val_loss: 0.0756 - val_mse: 0.8108\n",
      "Epoch 127/200\n",
      "33/43 [======================>.......] - ETA: 0s - loss: 0.0030 - mse: 0.3177\n",
      "Epoch 00127: val_loss did not improve from 0.02790\n",
      "43/43 [==============================] - 0s 6ms/step - loss: 0.0030 - mse: 0.3204 - val_loss: 0.0636 - val_mse: 0.6916\n",
      "Epoch 128/200\n",
      "40/43 [==========================>...] - ETA: 0s - loss: 0.0035 - mse: 0.3326\n",
      "Epoch 00128: val_loss did not improve from 0.02790\n",
      "43/43 [==============================] - 0s 7ms/step - loss: 0.0036 - mse: 0.3263 - val_loss: 0.0489 - val_mse: 0.5906\n",
      "Epoch 129/200\n",
      "40/43 [==========================>...] - ETA: 0s - loss: 0.0036 - mse: 0.3209\n",
      "Epoch 00129: val_loss did not improve from 0.02790\n",
      "43/43 [==============================] - 0s 6ms/step - loss: 0.0036 - mse: 0.3198 - val_loss: 0.0307 - val_mse: 0.4721\n",
      "Epoch 130/200\n",
      "39/43 [==========================>...] - ETA: 0s - loss: 0.0029 - mse: 0.3211\n",
      "Epoch 00130: val_loss did not improve from 0.02790\n",
      "43/43 [==============================] - 0s 8ms/step - loss: 0.0028 - mse: 0.3188 - val_loss: 0.0440 - val_mse: 0.5982\n",
      "Epoch 131/200\n",
      "40/43 [==========================>...] - ETA: 0s - loss: 0.0030 - mse: 0.3207\n",
      "Epoch 00131: val_loss did not improve from 0.02790\n",
      "43/43 [==============================] - 0s 6ms/step - loss: 0.0030 - mse: 0.3235 - val_loss: 0.0393 - val_mse: 0.5519\n",
      "Epoch 132/200\n",
      "39/43 [==========================>...] - ETA: 0s - loss: 0.0029 - mse: 0.3228\n",
      "Epoch 00132: val_loss did not improve from 0.02790\n",
      "43/43 [==============================] - 0s 6ms/step - loss: 0.0029 - mse: 0.3191 - val_loss: 0.0502 - val_mse: 0.6974\n",
      "Epoch 133/200\n",
      "36/43 [========================>.....] - ETA: 0s - loss: 0.0027 - mse: 0.3319\n",
      "Epoch 00133: val_loss did not improve from 0.02790\n",
      "43/43 [==============================] - 0s 7ms/step - loss: 0.0029 - mse: 0.3235 - val_loss: 0.0502 - val_mse: 0.6992\n",
      "Epoch 134/200\n",
      "40/43 [==========================>...] - ETA: 0s - loss: 0.0029 - mse: 0.3169\n",
      "Epoch 00134: val_loss did not improve from 0.02790\n",
      "43/43 [==============================] - 0s 6ms/step - loss: 0.0029 - mse: 0.3195 - val_loss: 0.0687 - val_mse: 0.8027\n",
      "Epoch 135/200\n",
      "39/43 [==========================>...] - ETA: 0s - loss: 0.0032 - mse: 0.3230\n",
      "Epoch 00135: val_loss did not improve from 0.02790\n",
      "43/43 [==============================] - 0s 7ms/step - loss: 0.0031 - mse: 0.3253 - val_loss: 0.0609 - val_mse: 0.7388\n",
      "Epoch 136/200\n",
      "37/43 [========================>.....] - ETA: 0s - loss: 0.0031 - mse: 0.3303\n",
      "Epoch 00136: val_loss did not improve from 0.02790\n",
      "43/43 [==============================] - 0s 7ms/step - loss: 0.0031 - mse: 0.3297 - val_loss: 0.0621 - val_mse: 0.7161\n",
      "Epoch 137/200\n",
      "40/43 [==========================>...] - ETA: 0s - loss: 0.0032 - mse: 0.3302\n",
      "Epoch 00137: val_loss did not improve from 0.02790\n",
      "43/43 [==============================] - 0s 8ms/step - loss: 0.0032 - mse: 0.3289 - val_loss: 0.0631 - val_mse: 0.7916\n",
      "Epoch 138/200\n",
      "38/43 [=========================>....] - ETA: 0s - loss: 0.0028 - mse: 0.3226\n",
      "Epoch 00138: val_loss did not improve from 0.02790\n",
      "43/43 [==============================] - 0s 7ms/step - loss: 0.0026 - mse: 0.3189 - val_loss: 0.0522 - val_mse: 0.6613\n",
      "Epoch 139/200\n",
      "41/43 [===========================>..] - ETA: 0s - loss: 0.0029 - mse: 0.3203\n",
      "Epoch 00139: val_loss did not improve from 0.02790\n",
      "43/43 [==============================] - 0s 8ms/step - loss: 0.0029 - mse: 0.3193 - val_loss: 0.0590 - val_mse: 0.7507\n",
      "Epoch 140/200\n",
      "37/43 [========================>.....] - ETA: 0s - loss: 0.0030 - mse: 0.3210 \n",
      "Epoch 00140: val_loss did not improve from 0.02790\n",
      "43/43 [==============================] - 0s 7ms/step - loss: 0.0029 - mse: 0.3212 - val_loss: 0.0566 - val_mse: 0.7278\n",
      "Epoch 141/200\n",
      "39/43 [==========================>...] - ETA: 0s - loss: 0.0029 - mse: 0.3174\n",
      "Epoch 00141: val_loss did not improve from 0.02790\n",
      "43/43 [==============================] - 0s 7ms/step - loss: 0.0028 - mse: 0.3169 - val_loss: 0.0625 - val_mse: 0.7704\n",
      "Epoch 142/200\n",
      "35/43 [=======================>......] - ETA: 0s - loss: 0.0027 - mse: 0.3178\n",
      "Epoch 00142: val_loss did not improve from 0.02790\n",
      "43/43 [==============================] - 0s 7ms/step - loss: 0.0028 - mse: 0.3154 - val_loss: 0.0609 - val_mse: 0.7466\n",
      "Epoch 143/200\n",
      "36/43 [========================>.....] - ETA: 0s - loss: 0.0021 - mse: 0.3030\n",
      "Epoch 00143: val_loss did not improve from 0.02790\n",
      "43/43 [==============================] - 0s 7ms/step - loss: 0.0023 - mse: 0.3176 - val_loss: 0.0611 - val_mse: 0.7321\n",
      "Epoch 144/200\n",
      "39/43 [==========================>...] - ETA: 0s - loss: 0.0027 - mse: 0.3185\n",
      "Epoch 00144: val_loss did not improve from 0.02790\n",
      "43/43 [==============================] - 0s 6ms/step - loss: 0.0029 - mse: 0.3220 - val_loss: 0.0611 - val_mse: 0.7463\n",
      "Epoch 145/200\n",
      "42/43 [============================>.] - ETA: 0s - loss: 0.0026 - mse: 0.3158\n",
      "Epoch 00145: val_loss did not improve from 0.02790\n",
      "43/43 [==============================] - 0s 8ms/step - loss: 0.0027 - mse: 0.3157 - val_loss: 0.0498 - val_mse: 0.5790\n",
      "Epoch 146/200\n",
      "41/43 [===========================>..] - ETA: 0s - loss: 0.0028 - mse: 0.3225 \n",
      "Epoch 00146: val_loss did not improve from 0.02790\n",
      "43/43 [==============================] - 0s 6ms/step - loss: 0.0028 - mse: 0.3212 - val_loss: 0.0583 - val_mse: 0.6999\n",
      "Epoch 147/200\n",
      "36/43 [========================>.....] - ETA: 0s - loss: 0.0024 - mse: 0.3104\n",
      "Epoch 00147: val_loss did not improve from 0.02790\n",
      "43/43 [==============================] - 0s 7ms/step - loss: 0.0024 - mse: 0.3140 - val_loss: 0.0591 - val_mse: 0.7296\n",
      "Epoch 148/200\n",
      "40/43 [==========================>...] - ETA: 0s - loss: 0.0028 - mse: 0.3082\n",
      "Epoch 00148: val_loss did not improve from 0.02790\n",
      "43/43 [==============================] - 0s 6ms/step - loss: 0.0028 - mse: 0.3162 - val_loss: 0.0607 - val_mse: 0.7545\n",
      "Epoch 149/200\n",
      "39/43 [==========================>...] - ETA: 0s - loss: 0.0029 - mse: 0.3223\n",
      "Epoch 00149: val_loss did not improve from 0.02790\n",
      "43/43 [==============================] - 0s 6ms/step - loss: 0.0030 - mse: 0.3207 - val_loss: 0.0507 - val_mse: 0.6022\n",
      "Epoch 150/200\n",
      "36/43 [========================>.....] - ETA: 0s - loss: 0.0021 - mse: 0.3228 \n",
      "Epoch 00150: val_loss did not improve from 0.02790\n",
      "43/43 [==============================] - 0s 7ms/step - loss: 0.0024 - mse: 0.3130 - val_loss: 0.0617 - val_mse: 0.7435\n",
      "Epoch 151/200\n",
      "37/43 [========================>.....] - ETA: 0s - loss: 0.0029 - mse: 0.3193\n",
      "Epoch 00151: val_loss did not improve from 0.02790\n",
      "43/43 [==============================] - 0s 7ms/step - loss: 0.0027 - mse: 0.3184 - val_loss: 0.0415 - val_mse: 0.5244\n",
      "Epoch 152/200\n",
      "38/43 [=========================>....] - ETA: 0s - loss: 0.0023 - mse: 0.3173\n",
      "Epoch 00152: val_loss did not improve from 0.02790\n",
      "43/43 [==============================] - 0s 7ms/step - loss: 0.0024 - mse: 0.3160 - val_loss: 0.0410 - val_mse: 0.5185\n",
      "Epoch 153/200\n",
      "38/43 [=========================>....] - ETA: 0s - loss: 0.0030 - mse: 0.3224\n",
      "Epoch 00153: val_loss did not improve from 0.02790\n",
      "43/43 [==============================] - 0s 7ms/step - loss: 0.0031 - mse: 0.3197 - val_loss: 0.0468 - val_mse: 0.5845\n",
      "Epoch 154/200\n",
      "35/43 [=======================>......] - ETA: 0s - loss: 0.0044 - mse: 0.3237\n",
      "Epoch 00154: val_loss did not improve from 0.02790\n",
      "43/43 [==============================] - 0s 8ms/step - loss: 0.0042 - mse: 0.3214 - val_loss: 0.0483 - val_mse: 0.5530\n",
      "Epoch 155/200\n",
      "42/43 [============================>.] - ETA: 0s - loss: 0.0036 - mse: 0.3257\n",
      "Epoch 00155: val_loss did not improve from 0.02790\n",
      "43/43 [==============================] - 0s 8ms/step - loss: 0.0037 - mse: 0.3239 - val_loss: 0.0508 - val_mse: 0.5801\n",
      "Epoch 156/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "41/43 [===========================>..] - ETA: 0s - loss: 0.0028 - mse: 0.3195\n",
      "Epoch 00156: val_loss did not improve from 0.02790\n",
      "43/43 [==============================] - 0s 8ms/step - loss: 0.0028 - mse: 0.3186 - val_loss: 0.0423 - val_mse: 0.5190\n",
      "Epoch 157/200\n",
      "40/43 [==========================>...] - ETA: 0s - loss: 0.0027 - mse: 0.3264\n",
      "Epoch 00157: val_loss did not improve from 0.02790\n",
      "43/43 [==============================] - 0s 8ms/step - loss: 0.0026 - mse: 0.3211 - val_loss: 0.0454 - val_mse: 0.5342\n",
      "Epoch 158/200\n",
      "37/43 [========================>.....] - ETA: 0s - loss: 0.0025 - mse: 0.3225\n",
      "Epoch 00158: val_loss did not improve from 0.02790\n",
      "43/43 [==============================] - 0s 7ms/step - loss: 0.0026 - mse: 0.3214 - val_loss: 0.0611 - val_mse: 0.7279\n",
      "Epoch 159/200\n",
      "42/43 [============================>.] - ETA: 0s - loss: 0.0043 - mse: 0.3300\n",
      "Epoch 00159: val_loss did not improve from 0.02790\n",
      "43/43 [==============================] - 0s 8ms/step - loss: 0.0043 - mse: 0.3321 - val_loss: 0.0336 - val_mse: 0.4633\n",
      "Epoch 160/200\n",
      "39/43 [==========================>...] - ETA: 0s - loss: 0.0034 - mse: 0.3241\n",
      "Epoch 00160: val_loss did not improve from 0.02790\n",
      "43/43 [==============================] - 0s 7ms/step - loss: 0.0034 - mse: 0.3212 - val_loss: 0.0390 - val_mse: 0.4885\n",
      "Epoch 161/200\n",
      "42/43 [============================>.] - ETA: 0s - loss: 0.0027 - mse: 0.3217 \n",
      "Epoch 00161: val_loss did not improve from 0.02790\n",
      "43/43 [==============================] - 0s 6ms/step - loss: 0.0026 - mse: 0.3216 - val_loss: 0.0396 - val_mse: 0.4953\n",
      "Epoch 162/200\n",
      "41/43 [===========================>..] - ETA: 0s - loss: 0.0028 - mse: 0.3255\n",
      "Epoch 00162: val_loss did not improve from 0.02790\n",
      "43/43 [==============================] - 0s 6ms/step - loss: 0.0028 - mse: 0.3240 - val_loss: 0.0434 - val_mse: 0.5139\n",
      "Epoch 163/200\n",
      "40/43 [==========================>...] - ETA: 0s - loss: 0.0030 - mse: 0.3181\n",
      "Epoch 00163: val_loss did not improve from 0.02790\n",
      "43/43 [==============================] - 0s 7ms/step - loss: 0.0029 - mse: 0.3207 - val_loss: 0.0518 - val_mse: 0.6210\n",
      "Epoch 164/200\n",
      "36/43 [========================>.....] - ETA: 0s - loss: 0.0029 - mse: 0.3235\n",
      "Epoch 00164: val_loss did not improve from 0.02790\n",
      "43/43 [==============================] - 0s 7ms/step - loss: 0.0028 - mse: 0.3251 - val_loss: 0.0439 - val_mse: 0.5272\n",
      "Epoch 165/200\n",
      "38/43 [=========================>....] - ETA: 0s - loss: 0.0029 - mse: 0.3312\n",
      "Epoch 00165: val_loss did not improve from 0.02790\n",
      "43/43 [==============================] - 0s 6ms/step - loss: 0.0031 - mse: 0.3262 - val_loss: 0.0595 - val_mse: 0.6230\n",
      "Epoch 166/200\n",
      "42/43 [============================>.] - ETA: 0s - loss: 0.0025 - mse: 0.3200\n",
      "Epoch 00166: val_loss did not improve from 0.02790\n",
      "43/43 [==============================] - 0s 7ms/step - loss: 0.0026 - mse: 0.3198 - val_loss: 0.0500 - val_mse: 0.5831\n",
      "Epoch 167/200\n",
      "40/43 [==========================>...] - ETA: 0s - loss: 0.0022 - mse: 0.3096\n",
      "Epoch 00167: val_loss did not improve from 0.02790\n",
      "43/43 [==============================] - 0s 6ms/step - loss: 0.0022 - mse: 0.3126 - val_loss: 0.0544 - val_mse: 0.6128\n",
      "Epoch 168/200\n",
      "37/43 [========================>.....] - ETA: 0s - loss: 0.0022 - mse: 0.3170\n",
      "Epoch 00168: val_loss did not improve from 0.02790\n",
      "43/43 [==============================] - 0s 7ms/step - loss: 0.0022 - mse: 0.3145 - val_loss: 0.0529 - val_mse: 0.5994\n",
      "Epoch 169/200\n",
      "38/43 [=========================>....] - ETA: 0s - loss: 0.0024 - mse: 0.3249\n",
      "Epoch 00169: val_loss did not improve from 0.02790\n",
      "43/43 [==============================] - 0s 7ms/step - loss: 0.0024 - mse: 0.3184 - val_loss: 0.0477 - val_mse: 0.5680\n",
      "Epoch 170/200\n",
      "40/43 [==========================>...] - ETA: 0s - loss: 0.0026 - mse: 0.3186\n",
      "Epoch 00170: val_loss did not improve from 0.02790\n",
      "43/43 [==============================] - 0s 8ms/step - loss: 0.0025 - mse: 0.3143 - val_loss: 0.0461 - val_mse: 0.5491\n",
      "Epoch 171/200\n",
      "38/43 [=========================>....] - ETA: 0s - loss: 0.0022 - mse: 0.3125\n",
      "Epoch 00171: val_loss did not improve from 0.02790\n",
      "43/43 [==============================] - 0s 8ms/step - loss: 0.0022 - mse: 0.3124 - val_loss: 0.0452 - val_mse: 0.5478\n",
      "Epoch 172/200\n",
      "42/43 [============================>.] - ETA: 0s - loss: 0.0021 - mse: 0.3161\n",
      "Epoch 00172: val_loss did not improve from 0.02790\n",
      "43/43 [==============================] - 0s 9ms/step - loss: 0.0021 - mse: 0.3155 - val_loss: 0.0434 - val_mse: 0.5331\n",
      "Epoch 173/200\n",
      "35/43 [=======================>......] - ETA: 0s - loss: 0.0019 - mse: 0.3238\n",
      "Epoch 00173: val_loss did not improve from 0.02790\n",
      "43/43 [==============================] - 0s 7ms/step - loss: 0.0019 - mse: 0.3118 - val_loss: 0.0463 - val_mse: 0.5397\n",
      "Epoch 174/200\n",
      "41/43 [===========================>..] - ETA: 0s - loss: 0.0028 - mse: 0.3171\n",
      "Epoch 00174: val_loss did not improve from 0.02790\n",
      "43/43 [==============================] - 0s 8ms/step - loss: 0.0030 - mse: 0.3224 - val_loss: 0.0371 - val_mse: 0.5099\n",
      "Epoch 175/200\n",
      "35/43 [=======================>......] - ETA: 0s - loss: 0.0024 - mse: 0.3179\n",
      "Epoch 00175: val_loss did not improve from 0.02790\n",
      "43/43 [==============================] - 0s 7ms/step - loss: 0.0024 - mse: 0.3182 - val_loss: 0.0409 - val_mse: 0.5274\n",
      "Epoch 176/200\n",
      "40/43 [==========================>...] - ETA: 0s - loss: 0.0022 - mse: 0.3143\n",
      "Epoch 00176: val_loss did not improve from 0.02790\n",
      "43/43 [==============================] - 0s 8ms/step - loss: 0.0021 - mse: 0.3139 - val_loss: 0.0362 - val_mse: 0.4906\n",
      "Epoch 177/200\n",
      "39/43 [==========================>...] - ETA: 0s - loss: 0.0017 - mse: 0.3083\n",
      "Epoch 00177: val_loss did not improve from 0.02790\n",
      "43/43 [==============================] - 0s 8ms/step - loss: 0.0017 - mse: 0.3101 - val_loss: 0.0454 - val_mse: 0.5429\n",
      "Epoch 178/200\n",
      "41/43 [===========================>..] - ETA: 0s - loss: 0.0020 - mse: 0.3127\n",
      "Epoch 00178: val_loss did not improve from 0.02790\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.0020 - mse: 0.3124 - val_loss: 0.0399 - val_mse: 0.5297\n",
      "Epoch 179/200\n",
      "35/43 [=======================>......] - ETA: 0s - loss: 0.0024 - mse: 0.3218\n",
      "Epoch 00179: val_loss did not improve from 0.02790\n",
      "43/43 [==============================] - 0s 7ms/step - loss: 0.0024 - mse: 0.3179 - val_loss: 0.0430 - val_mse: 0.5567\n",
      "Epoch 180/200\n",
      "39/43 [==========================>...] - ETA: 0s - loss: 0.0024 - mse: 0.3124\n",
      "Epoch 00180: val_loss did not improve from 0.02790\n",
      "43/43 [==============================] - 0s 6ms/step - loss: 0.0023 - mse: 0.3153 - val_loss: 0.0470 - val_mse: 0.5823\n",
      "Epoch 181/200\n",
      "39/43 [==========================>...] - ETA: 0s - loss: 0.0021 - mse: 0.3107 \n",
      "Epoch 00181: val_loss did not improve from 0.02790\n",
      "43/43 [==============================] - 0s 6ms/step - loss: 0.0021 - mse: 0.3145 - val_loss: 0.0461 - val_mse: 0.6012\n",
      "Epoch 182/200\n",
      "41/43 [===========================>..] - ETA: 0s - loss: 0.0017 - mse: 0.3112\n",
      "Epoch 00182: val_loss did not improve from 0.02790\n",
      "43/43 [==============================] - 0s 6ms/step - loss: 0.0018 - mse: 0.3129 - val_loss: 0.0419 - val_mse: 0.5357\n",
      "Epoch 183/200\n",
      "37/43 [========================>.....] - ETA: 0s - loss: 0.0020 - mse: 0.3207\n",
      "Epoch 00183: val_loss did not improve from 0.02790\n",
      "43/43 [==============================] - 0s 8ms/step - loss: 0.0020 - mse: 0.3130 - val_loss: 0.0403 - val_mse: 0.5449\n",
      "Epoch 184/200\n",
      "35/43 [=======================>......] - ETA: 0s - loss: 0.0022 - mse: 0.3269 \n",
      "Epoch 00184: val_loss did not improve from 0.02790\n",
      "43/43 [==============================] - 0s 7ms/step - loss: 0.0022 - mse: 0.3170 - val_loss: 0.0379 - val_mse: 0.5112\n",
      "Epoch 185/200\n",
      "41/43 [===========================>..] - ETA: 0s - loss: 0.0020 - mse: 0.3147\n",
      "Epoch 00185: val_loss did not improve from 0.02790\n",
      "43/43 [==============================] - 0s 7ms/step - loss: 0.0019 - mse: 0.3150 - val_loss: 0.0403 - val_mse: 0.4993\n",
      "Epoch 186/200\n",
      "42/43 [============================>.] - ETA: 0s - loss: 0.0019 - mse: 0.3127\n",
      "Epoch 00186: val_loss did not improve from 0.02790\n",
      "43/43 [==============================] - 0s 8ms/step - loss: 0.0019 - mse: 0.3154 - val_loss: 0.0362 - val_mse: 0.4873\n",
      "Epoch 187/200\n",
      "34/43 [======================>.......] - ETA: 0s - loss: 0.0022 - mse: 0.3047\n",
      "Epoch 00187: val_loss did not improve from 0.02790\n",
      "43/43 [==============================] - 0s 7ms/step - loss: 0.0021 - mse: 0.3123 - val_loss: 0.0434 - val_mse: 0.5233\n",
      "Epoch 188/200\n",
      "42/43 [============================>.] - ETA: 0s - loss: 0.0018 - mse: 0.3148\n",
      "Epoch 00188: val_loss did not improve from 0.02790\n",
      "43/43 [==============================] - 0s 7ms/step - loss: 0.0019 - mse: 0.3133 - val_loss: 0.0522 - val_mse: 0.5718\n",
      "Epoch 189/200\n",
      "41/43 [===========================>..] - ETA: 0s - loss: 0.0022 - mse: 0.3181\n",
      "Epoch 00189: val_loss did not improve from 0.02790\n",
      "43/43 [==============================] - 0s 8ms/step - loss: 0.0022 - mse: 0.3159 - val_loss: 0.0383 - val_mse: 0.4954\n",
      "Epoch 190/200\n",
      "41/43 [===========================>..] - ETA: 0s - loss: 0.0024 - mse: 0.3268\n",
      "Epoch 00190: val_loss did not improve from 0.02790\n",
      "43/43 [==============================] - 0s 6ms/step - loss: 0.0024 - mse: 0.3225 - val_loss: 0.0279 - val_mse: 0.4608\n",
      "Epoch 191/200\n",
      "35/43 [=======================>......] - ETA: 0s - loss: 0.0018 - mse: 0.3188 \n",
      "Epoch 00191: val_loss did not improve from 0.02790\n",
      "43/43 [==============================] - 0s 7ms/step - loss: 0.0019 - mse: 0.3117 - val_loss: 0.0292 - val_mse: 0.4607\n",
      "Epoch 192/200\n",
      "34/43 [======================>.......] - ETA: 0s - loss: 0.0018 - mse: 0.3100\n",
      "Epoch 00192: val_loss improved from 0.02790 to 0.02772, saving model to min_vl_model4.h5\n",
      "43/43 [==============================] - 0s 8ms/step - loss: 0.0019 - mse: 0.3136 - val_loss: 0.0277 - val_mse: 0.4587\n",
      "Epoch 193/200\n",
      "32/43 [=====================>........] - ETA: 0s - loss: 0.0023 - mse: 0.3195  \n",
      "Epoch 00193: val_loss improved from 0.02772 to 0.02767, saving model to min_vl_model4.h5\n",
      "43/43 [==============================] - 0s 8ms/step - loss: 0.0022 - mse: 0.3179 - val_loss: 0.0277 - val_mse: 0.4623\n",
      "Epoch 194/200\n",
      "37/43 [========================>.....] - ETA: 0s - loss: 0.0020 - mse: 0.3150\n",
      "Epoch 00194: val_loss did not improve from 0.02767\n",
      "43/43 [==============================] - 0s 7ms/step - loss: 0.0022 - mse: 0.3179 - val_loss: 0.0299 - val_mse: 0.4719\n",
      "Epoch 195/200\n",
      "43/43 [==============================] - ETA: 0s - loss: 0.0021 - mse: 0.3174 \n",
      "Epoch 00195: val_loss did not improve from 0.02767\n",
      "43/43 [==============================] - 0s 6ms/step - loss: 0.0021 - mse: 0.3174 - val_loss: 0.0295 - val_mse: 0.4641\n",
      "Epoch 196/200\n",
      "43/43 [==============================] - ETA: 0s - loss: 0.0022 - mse: 0.3142\n",
      "Epoch 00196: val_loss did not improve from 0.02767\n",
      "43/43 [==============================] - 0s 6ms/step - loss: 0.0022 - mse: 0.3142 - val_loss: 0.0309 - val_mse: 0.4792\n",
      "Epoch 197/200\n",
      "36/43 [========================>.....] - ETA: 0s - loss: 0.0022 - mse: 0.3152\n",
      "Epoch 00197: val_loss did not improve from 0.02767\n",
      "43/43 [==============================] - 0s 5ms/step - loss: 0.0021 - mse: 0.3154 - val_loss: 0.0299 - val_mse: 0.4752\n",
      "Epoch 198/200\n",
      "34/43 [======================>.......] - ETA: 0s - loss: 0.0024 - mse: 0.3122\n",
      "Epoch 00198: val_loss did not improve from 0.02767\n",
      "43/43 [==============================] - 0s 5ms/step - loss: 0.0025 - mse: 0.3211 - val_loss: 0.0344 - val_mse: 0.4799\n",
      "Epoch 199/200\n",
      "43/43 [==============================] - ETA: 0s - loss: 0.0025 - mse: 0.3236\n",
      "Epoch 00199: val_loss did not improve from 0.02767\n",
      "43/43 [==============================] - 0s 6ms/step - loss: 0.0025 - mse: 0.3236 - val_loss: 0.0483 - val_mse: 0.6334\n",
      "Epoch 200/200\n",
      "42/43 [============================>.] - ETA: 0s - loss: 0.0018 - mse: 0.3137\n",
      "Epoch 00200: val_loss did not improve from 0.02767\n",
      "43/43 [==============================] - 0s 6ms/step - loss: 0.0018 - mse: 0.3136 - val_loss: 0.0449 - val_mse: 0.5427\n"
     ]
    }
   ],
   "source": [
    "#Compiling the ANN\n",
    "opt = keras.optimizers.Adam(lr=0.0015, beta_1=0.9, beta_2=0.999, epsilon=None, decay=0.0, amsgrad=False)\n",
    "model.compile(optimizer=opt, loss='mean_squared_logarithmic_error', metrics=['mse'])\n",
    "#Fitting the ANN to the training set\n",
    "model_filepath = 'min_vl_model4.h5'\n",
    "checkpoint = ModelCheckpoint(model_filepath, monitor = 'val_loss', verbose=1, save_best_only = True, mode='min' )\n",
    "history=model.fit(X_train,y_train, validation_split=0.07, batch_size=32, epochs=200, callbacks=[checkpoint])\n",
    "model.load_weights(model_filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "rental-cooking",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_8\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_48 (Dense)             (None, 200)               62600     \n",
      "_________________________________________________________________\n",
      "dense_49 (Dense)             (None, 150)               30150     \n",
      "_________________________________________________________________\n",
      "dropout_15 (Dropout)         (None, 150)               0         \n",
      "_________________________________________________________________\n",
      "dense_50 (Dense)             (None, 100)               15100     \n",
      "_________________________________________________________________\n",
      "batch_normalization_18 (Batc (None, 100)               400       \n",
      "_________________________________________________________________\n",
      "dropout_16 (Dropout)         (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_51 (Dense)             (None, 80)                8080      \n",
      "_________________________________________________________________\n",
      "dense_52 (Dense)             (None, 50)                4050      \n",
      "_________________________________________________________________\n",
      "batch_normalization_19 (Batc (None, 50)                200       \n",
      "_________________________________________________________________\n",
      "dense_53 (Dense)             (None, 50)                2550      \n",
      "_________________________________________________________________\n",
      "batch_normalization_20 (Batc (None, 50)                200       \n",
      "_________________________________________________________________\n",
      "dense_54 (Dense)             (None, 30)                1530      \n",
      "_________________________________________________________________\n",
      "batch_normalization_21 (Batc (None, 30)                120       \n",
      "_________________________________________________________________\n",
      "dropout_17 (Dropout)         (None, 30)                0         \n",
      "_________________________________________________________________\n",
      "dense_55 (Dense)             (None, 1)                 31        \n",
      "=================================================================\n",
      "Total params: 125,011\n",
      "Trainable params: 124,551\n",
      "Non-trainable params: 460\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "from keras.initializers import he_normal\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "\n",
    "#Inıtialising the ANN\n",
    "model = Sequential()\n",
    "#Adding the input layer and first hidden layer\n",
    "model.add(Dense(units =200, kernel_initializer=he_normal(seed=None), activation= 'tanh', \n",
    "                input_dim=X_train.shape[1]))\n",
    "\n",
    "\n",
    "\n",
    "#Add the first hidden layer\n",
    "model.add(Dense(units =150, kernel_initializer=he_normal(seed=None), activation= 'tanh'))\n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "#Add the second hidden layer\n",
    "model.add(Dense(units =100, kernel_initializer=he_normal(seed=None), activation= 'tanh'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "#Add the third hidden layer\n",
    "model.add(Dense(units =80, kernel_initializer=he_normal(seed=None), activation= 'tanh'))\n",
    "\n",
    "\n",
    "#Add the fourth hidden layer\n",
    "model.add(Dense(units =50, kernel_initializer=he_normal(seed=None), activation= 'relu'))\n",
    "model.add(BatchNormalization())\n",
    "\n",
    "#Add the fifth hidden layer\n",
    "model.add(Dense(units =50, kernel_initializer=he_normal(seed=None), activation= 'relu'))\n",
    "model.add(BatchNormalization())\n",
    "\n",
    "#Add the sixthth hidden layer\n",
    "model.add(Dense(units =30, kernel_initializer=he_normal(seed=None), activation= 'relu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "#The output layer\n",
    "model.add(Dense(units =1, kernel_initializer=he_normal(seed=None), activation= 'relu'))\n",
    "\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "constant-bride",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "41/43 [===========================>..] - ETA: 0s - loss: 0.2270 - mse: 1.5248\n",
      "Epoch 00001: val_loss improved from inf to 0.10962, saving model to min_vl_model5.h5\n",
      "43/43 [==============================] - 1s 18ms/step - loss: 0.2233 - mse: 1.5090 - val_loss: 0.1096 - val_mse: 1.1143\n",
      "Epoch 2/200\n",
      "39/43 [==========================>...] - ETA: 0s - loss: 0.1284 - mse: 1.0404\n",
      "Epoch 00002: val_loss did not improve from 0.10962\n",
      "43/43 [==============================] - 0s 8ms/step - loss: 0.1294 - mse: 1.0241 - val_loss: 0.1340 - val_mse: 1.5720\n",
      "Epoch 3/200\n",
      "39/43 [==========================>...] - ETA: 0s - loss: 0.1057 - mse: 0.8134\n",
      "Epoch 00003: val_loss did not improve from 0.10962\n",
      "43/43 [==============================] - 0s 7ms/step - loss: 0.1052 - mse: 0.8121 - val_loss: 0.1683 - val_mse: 2.4631\n",
      "Epoch 4/200\n",
      "43/43 [==============================] - ETA: 0s - loss: 0.0803 - mse: 0.6796\n",
      "Epoch 00004: val_loss improved from 0.10962 to 0.08615, saving model to min_vl_model5.h5\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.0803 - mse: 0.6796 - val_loss: 0.0862 - val_mse: 1.0105\n",
      "Epoch 5/200\n",
      "43/43 [==============================] - ETA: 0s - loss: 0.0799 - mse: 0.7225\n",
      "Epoch 00005: val_loss did not improve from 0.08615\n",
      "43/43 [==============================] - 0s 7ms/step - loss: 0.0799 - mse: 0.7225 - val_loss: 0.0898 - val_mse: 0.9377\n",
      "Epoch 6/200\n",
      "36/43 [========================>.....] - ETA: 0s - loss: 0.0709 - mse: 0.6484\n",
      "Epoch 00006: val_loss improved from 0.08615 to 0.06718, saving model to min_vl_model5.h5\n",
      "43/43 [==============================] - 0s 9ms/step - loss: 0.0689 - mse: 0.6277 - val_loss: 0.0672 - val_mse: 0.6926\n",
      "Epoch 7/200\n",
      "34/43 [======================>.......] - ETA: 0s - loss: 0.0763 - mse: 0.6772\n",
      "Epoch 00007: val_loss did not improve from 0.06718\n",
      "43/43 [==============================] - 0s 7ms/step - loss: 0.0732 - mse: 0.6542 - val_loss: 0.0697 - val_mse: 0.7844\n",
      "Epoch 8/200\n",
      "38/43 [=========================>....] - ETA: 0s - loss: 0.0676 - mse: 0.6404\n",
      "Epoch 00008: val_loss improved from 0.06718 to 0.04748, saving model to min_vl_model5.h5\n",
      "43/43 [==============================] - 0s 11ms/step - loss: 0.0657 - mse: 0.6291 - val_loss: 0.0475 - val_mse: 0.5689\n",
      "Epoch 9/200\n",
      "41/43 [===========================>..] - ETA: 0s - loss: 0.0542 - mse: 0.5654\n",
      "Epoch 00009: val_loss improved from 0.04748 to 0.04571, saving model to min_vl_model5.h5\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.0542 - mse: 0.5631 - val_loss: 0.0457 - val_mse: 0.4905\n",
      "Epoch 10/200\n",
      "43/43 [==============================] - ETA: 0s - loss: 0.0550 - mse: 0.5499\n",
      "Epoch 00010: val_loss improved from 0.04571 to 0.03536, saving model to min_vl_model5.h5\n",
      "43/43 [==============================] - 0s 9ms/step - loss: 0.0550 - mse: 0.5499 - val_loss: 0.0354 - val_mse: 0.4428\n",
      "Epoch 11/200\n",
      "41/43 [===========================>..] - ETA: 0s - loss: 0.0460 - mse: 0.5150- ETA: 0s - loss: 0.0473 - mse: 0.5\n",
      "Epoch 00011: val_loss improved from 0.03536 to 0.03309, saving model to min_vl_model5.h5\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.0464 - mse: 0.5196 - val_loss: 0.0331 - val_mse: 0.4072\n",
      "Epoch 12/200\n",
      "42/43 [============================>.] - ETA: 0s - loss: 0.0458 - mse: 0.5211\n",
      "Epoch 00012: val_loss improved from 0.03309 to 0.02604, saving model to min_vl_model5.h5\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.0465 - mse: 0.5210 - val_loss: 0.0260 - val_mse: 0.4009\n",
      "Epoch 13/200\n",
      "42/43 [============================>.] - ETA: 0s - loss: 0.0483 - mse: 0.5537\n",
      "Epoch 00013: val_loss did not improve from 0.02604\n",
      "43/43 [==============================] - 0s 7ms/step - loss: 0.0482 - mse: 0.5513 - val_loss: 0.0480 - val_mse: 0.5283\n",
      "Epoch 14/200\n",
      "35/43 [=======================>......] - ETA: 0s - loss: 0.0438 - mse: 0.5315\n",
      "Epoch 00014: val_loss did not improve from 0.02604\n",
      "43/43 [==============================] - 0s 7ms/step - loss: 0.0439 - mse: 0.5358 - val_loss: 0.0371 - val_mse: 0.4358\n",
      "Epoch 15/200\n",
      "43/43 [==============================] - ETA: 0s - loss: 0.0367 - mse: 0.4929\n",
      "Epoch 00015: val_loss did not improve from 0.02604\n",
      "43/43 [==============================] - 0s 7ms/step - loss: 0.0367 - mse: 0.4929 - val_loss: 0.0348 - val_mse: 0.4454\n",
      "Epoch 16/200\n",
      "41/43 [===========================>..] - ETA: 0s - loss: 0.0379 - mse: 0.5117\n",
      "Epoch 00016: val_loss did not improve from 0.02604\n",
      "43/43 [==============================] - 0s 8ms/step - loss: 0.0381 - mse: 0.5110 - val_loss: 0.0284 - val_mse: 0.3921\n",
      "Epoch 17/200\n",
      "41/43 [===========================>..] - ETA: 0s - loss: 0.0311 - mse: 0.4787\n",
      "Epoch 00017: val_loss improved from 0.02604 to 0.02251, saving model to min_vl_model5.h5\n",
      "43/43 [==============================] - 0s 11ms/step - loss: 0.0313 - mse: 0.4791 - val_loss: 0.0225 - val_mse: 0.3675\n",
      "Epoch 18/200\n",
      "39/43 [==========================>...] - ETA: 0s - loss: 0.0334 - mse: 0.4674\n",
      "Epoch 00018: val_loss improved from 0.02251 to 0.01885, saving model to min_vl_model5.h5\n",
      "43/43 [==============================] - 1s 13ms/step - loss: 0.0323 - mse: 0.4672 - val_loss: 0.0189 - val_mse: 0.3661\n",
      "Epoch 19/200\n",
      "36/43 [========================>.....] - ETA: 0s - loss: 0.0362 - mse: 0.4915\n",
      "Epoch 00019: val_loss did not improve from 0.01885\n",
      "43/43 [==============================] - 0s 9ms/step - loss: 0.0344 - mse: 0.4816 - val_loss: 0.0197 - val_mse: 0.3667\n",
      "Epoch 20/200\n",
      "40/43 [==========================>...] - ETA: 0s - loss: 0.0263 - mse: 0.4671\n",
      "Epoch 00020: val_loss did not improve from 0.01885\n",
      "43/43 [==============================] - 0s 8ms/step - loss: 0.0262 - mse: 0.4665 - val_loss: 0.0218 - val_mse: 0.3774\n",
      "Epoch 21/200\n",
      "43/43 [==============================] - ETA: 0s - loss: 0.0298 - mse: 0.4793\n",
      "Epoch 00021: val_loss did not improve from 0.01885\n",
      "43/43 [==============================] - 0s 11ms/step - loss: 0.0298 - mse: 0.4793 - val_loss: 0.0190 - val_mse: 0.3545\n",
      "Epoch 22/200\n",
      "38/43 [=========================>....] - ETA: 0s - loss: 0.0297 - mse: 0.4750\n",
      "Epoch 00022: val_loss did not improve from 0.01885\n",
      "43/43 [==============================] - 0s 9ms/step - loss: 0.0328 - mse: 0.4827 - val_loss: 0.0199 - val_mse: 0.3595\n",
      "Epoch 23/200\n",
      "38/43 [=========================>....] - ETA: 0s - loss: 0.0259 - mse: 0.4478\n",
      "Epoch 00023: val_loss did not improve from 0.01885\n",
      "43/43 [==============================] - 0s 8ms/step - loss: 0.0270 - mse: 0.4491 - val_loss: 0.0193 - val_mse: 0.3536\n",
      "Epoch 24/200\n",
      "41/43 [===========================>..] - ETA: 0s - loss: 0.0255 - mse: 0.4598\n",
      "Epoch 00024: val_loss did not improve from 0.01885\n",
      "43/43 [==============================] - 0s 8ms/step - loss: 0.0264 - mse: 0.4554 - val_loss: 0.0219 - val_mse: 0.3644\n",
      "Epoch 25/200\n",
      "43/43 [==============================] - ETA: 0s - loss: 0.0274 - mse: 0.4576- ETA: 0s - loss: 0.0269 - mse: 0\n",
      "Epoch 00025: val_loss improved from 0.01885 to 0.01511, saving model to min_vl_model5.h5\n",
      "43/43 [==============================] - 1s 12ms/step - loss: 0.0274 - mse: 0.4576 - val_loss: 0.0151 - val_mse: 0.3420\n",
      "Epoch 26/200\n",
      "36/43 [========================>.....] - ETA: 0s - loss: 0.0230 - mse: 0.4406\n",
      "Epoch 00026: val_loss did not improve from 0.01511\n",
      "43/43 [==============================] - 0s 9ms/step - loss: 0.0225 - mse: 0.4374 - val_loss: 0.0202 - val_mse: 0.3578\n",
      "Epoch 27/200\n",
      "41/43 [===========================>..] - ETA: 0s - loss: 0.0229 - mse: 0.4325\n",
      "Epoch 00027: val_loss did not improve from 0.01511\n",
      "43/43 [==============================] - 0s 8ms/step - loss: 0.0241 - mse: 0.4477 - val_loss: 0.0176 - val_mse: 0.3515\n",
      "Epoch 28/200\n",
      "36/43 [========================>.....] - ETA: 0s - loss: 0.0254 - mse: 0.4311\n",
      "Epoch 00028: val_loss did not improve from 0.01511\n",
      "43/43 [==============================] - 0s 8ms/step - loss: 0.0249 - mse: 0.4362 - val_loss: 0.0256 - val_mse: 0.4074\n",
      "Epoch 29/200\n",
      "42/43 [============================>.] - ETA: 0s - loss: 0.0218 - mse: 0.4307\n",
      "Epoch 00029: val_loss did not improve from 0.01511\n",
      "43/43 [==============================] - 0s 7ms/step - loss: 0.0219 - mse: 0.4315 - val_loss: 0.0229 - val_mse: 0.3800\n",
      "Epoch 30/200\n",
      "43/43 [==============================] - ETA: 0s - loss: 0.0223 - mse: 0.4344\n",
      "Epoch 00030: val_loss did not improve from 0.01511\n",
      "43/43 [==============================] - 0s 7ms/step - loss: 0.0223 - mse: 0.4344 - val_loss: 0.0214 - val_mse: 0.3745\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 31/200\n",
      "43/43 [==============================] - ETA: 0s - loss: 0.0204 - mse: 0.4188\n",
      "Epoch 00031: val_loss did not improve from 0.01511\n",
      "43/43 [==============================] - 0s 7ms/step - loss: 0.0204 - mse: 0.4188 - val_loss: 0.0205 - val_mse: 0.3716\n",
      "Epoch 32/200\n",
      "41/43 [===========================>..] - ETA: 0s - loss: 0.0218 - mse: 0.4390\n",
      "Epoch 00032: val_loss did not improve from 0.01511\n",
      "43/43 [==============================] - 0s 8ms/step - loss: 0.0221 - mse: 0.4366 - val_loss: 0.0230 - val_mse: 0.3820\n",
      "Epoch 33/200\n",
      "38/43 [=========================>....] - ETA: 0s - loss: 0.0200 - mse: 0.4351\n",
      "Epoch 00033: val_loss did not improve from 0.01511\n",
      "43/43 [==============================] - 0s 8ms/step - loss: 0.0197 - mse: 0.4244 - val_loss: 0.0205 - val_mse: 0.3609\n",
      "Epoch 34/200\n",
      "42/43 [============================>.] - ETA: 0s - loss: 0.0199 - mse: 0.4178\n",
      "Epoch 00034: val_loss did not improve from 0.01511\n",
      "43/43 [==============================] - 0s 9ms/step - loss: 0.0198 - mse: 0.4185 - val_loss: 0.0195 - val_mse: 0.3636\n",
      "Epoch 35/200\n",
      "36/43 [========================>.....] - ETA: 0s - loss: 0.0184 - mse: 0.4117\n",
      "Epoch 00035: val_loss did not improve from 0.01511\n",
      "43/43 [==============================] - 0s 7ms/step - loss: 0.0184 - mse: 0.4156 - val_loss: 0.0160 - val_mse: 0.3535\n",
      "Epoch 36/200\n",
      "43/43 [==============================] - ETA: 0s - loss: 0.0149 - mse: 0.3966\n",
      "Epoch 00036: val_loss did not improve from 0.01511\n",
      "43/43 [==============================] - 0s 9ms/step - loss: 0.0149 - mse: 0.3966 - val_loss: 0.0178 - val_mse: 0.3562\n",
      "Epoch 37/200\n",
      "37/43 [========================>.....] - ETA: 0s - loss: 0.0158 - mse: 0.3851\n",
      "Epoch 00037: val_loss did not improve from 0.01511\n",
      "43/43 [==============================] - 0s 7ms/step - loss: 0.0166 - mse: 0.4059 - val_loss: 0.0160 - val_mse: 0.3473\n",
      "Epoch 38/200\n",
      "43/43 [==============================] - ETA: 0s - loss: 0.0168 - mse: 0.4019\n",
      "Epoch 00038: val_loss did not improve from 0.01511\n",
      "43/43 [==============================] - 0s 9ms/step - loss: 0.0168 - mse: 0.4019 - val_loss: 0.0152 - val_mse: 0.3441\n",
      "Epoch 39/200\n",
      "35/43 [=======================>......] - ETA: 0s - loss: 0.0164 - mse: 0.3970\n",
      "Epoch 00039: val_loss improved from 0.01511 to 0.01498, saving model to min_vl_model5.h5\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.0155 - mse: 0.3986 - val_loss: 0.0150 - val_mse: 0.3452\n",
      "Epoch 40/200\n",
      "35/43 [=======================>......] - ETA: 0s - loss: 0.0152 - mse: 0.4062\n",
      "Epoch 00040: val_loss did not improve from 0.01498\n",
      "43/43 [==============================] - 0s 8ms/step - loss: 0.0149 - mse: 0.3943 - val_loss: 0.0154 - val_mse: 0.3487\n",
      "Epoch 41/200\n",
      "38/43 [=========================>....] - ETA: 0s - loss: 0.0171 - mse: 0.4086\n",
      "Epoch 00041: val_loss did not improve from 0.01498\n",
      "43/43 [==============================] - 0s 8ms/step - loss: 0.0169 - mse: 0.4014 - val_loss: 0.0218 - val_mse: 0.4024\n",
      "Epoch 42/200\n",
      "35/43 [=======================>......] - ETA: 0s - loss: 0.0156 - mse: 0.4133\n",
      "Epoch 00042: val_loss did not improve from 0.01498\n",
      "43/43 [==============================] - 0s 7ms/step - loss: 0.0159 - mse: 0.4091 - val_loss: 0.0256 - val_mse: 0.4104\n",
      "Epoch 43/200\n",
      "42/43 [============================>.] - ETA: 0s - loss: 0.0159 - mse: 0.4101\n",
      "Epoch 00043: val_loss did not improve from 0.01498\n",
      "43/43 [==============================] - 0s 8ms/step - loss: 0.0160 - mse: 0.4086 - val_loss: 0.0190 - val_mse: 0.3688\n",
      "Epoch 44/200\n",
      "39/43 [==========================>...] - ETA: 0s - loss: 0.0153 - mse: 0.4039\n",
      "Epoch 00044: val_loss did not improve from 0.01498\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.0152 - mse: 0.4015 - val_loss: 0.0200 - val_mse: 0.3741\n",
      "Epoch 45/200\n",
      "41/43 [===========================>..] - ETA: 0s - loss: 0.0153 - mse: 0.4018\n",
      "Epoch 00045: val_loss did not improve from 0.01498\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.0151 - mse: 0.4040 - val_loss: 0.0207 - val_mse: 0.3941\n",
      "Epoch 46/200\n",
      "41/43 [===========================>..] - ETA: 0s - loss: 0.0134 - mse: 0.3907\n",
      "Epoch 00046: val_loss did not improve from 0.01498\n",
      "43/43 [==============================] - 0s 9ms/step - loss: 0.0135 - mse: 0.3933 - val_loss: 0.0214 - val_mse: 0.3998\n",
      "Epoch 47/200\n",
      "40/43 [==========================>...] - ETA: 0s - loss: 0.0140 - mse: 0.3894\n",
      "Epoch 00047: val_loss did not improve from 0.01498\n",
      "43/43 [==============================] - 0s 8ms/step - loss: 0.0145 - mse: 0.4061 - val_loss: 0.0230 - val_mse: 0.4041\n",
      "Epoch 48/200\n",
      "40/43 [==========================>...] - ETA: 0s - loss: 0.0122 - mse: 0.3881\n",
      "Epoch 00048: val_loss did not improve from 0.01498\n",
      "43/43 [==============================] - 0s 8ms/step - loss: 0.0122 - mse: 0.3900 - val_loss: 0.0184 - val_mse: 0.3598\n",
      "Epoch 49/200\n",
      "41/43 [===========================>..] - ETA: 0s - loss: 0.0135 - mse: 0.4010\n",
      "Epoch 00049: val_loss did not improve from 0.01498\n",
      "43/43 [==============================] - 0s 9ms/step - loss: 0.0136 - mse: 0.3970 - val_loss: 0.0246 - val_mse: 0.4057\n",
      "Epoch 50/200\n",
      "36/43 [========================>.....] - ETA: 0s - loss: 0.0117 - mse: 0.3924\n",
      "Epoch 00050: val_loss did not improve from 0.01498\n",
      "43/43 [==============================] - 0s 8ms/step - loss: 0.0118 - mse: 0.3818 - val_loss: 0.0186 - val_mse: 0.3653\n",
      "Epoch 51/200\n",
      "41/43 [===========================>..] - ETA: 0s - loss: 0.0140 - mse: 0.4020\n",
      "Epoch 00051: val_loss did not improve from 0.01498\n",
      "43/43 [==============================] - 0s 8ms/step - loss: 0.0139 - mse: 0.3992 - val_loss: 0.0180 - val_mse: 0.3828\n",
      "Epoch 52/200\n",
      "39/43 [==========================>...] - ETA: 0s - loss: 0.0116 - mse: 0.3806\n",
      "Epoch 00052: val_loss did not improve from 0.01498\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.0117 - mse: 0.3816 - val_loss: 0.0171 - val_mse: 0.3627\n",
      "Epoch 53/200\n",
      "40/43 [==========================>...] - ETA: 0s - loss: 0.0118 - mse: 0.3871\n",
      "Epoch 00053: val_loss did not improve from 0.01498\n",
      "43/43 [==============================] - 0s 9ms/step - loss: 0.0118 - mse: 0.3827 - val_loss: 0.0192 - val_mse: 0.3756\n",
      "Epoch 54/200\n",
      "40/43 [==========================>...] - ETA: 0s - loss: 0.0125 - mse: 0.3865\n",
      "Epoch 00054: val_loss did not improve from 0.01498\n",
      "43/43 [==============================] - 0s 8ms/step - loss: 0.0124 - mse: 0.3839 - val_loss: 0.0277 - val_mse: 0.4253\n",
      "Epoch 55/200\n",
      "37/43 [========================>.....] - ETA: 0s - loss: 0.0134 - mse: 0.3900\n",
      "Epoch 00055: val_loss did not improve from 0.01498\n",
      "43/43 [==============================] - 0s 9ms/step - loss: 0.0130 - mse: 0.3859 - val_loss: 0.0203 - val_mse: 0.3759\n",
      "Epoch 56/200\n",
      "41/43 [===========================>..] - ETA: 0s - loss: 0.0103 - mse: 0.3711\n",
      "Epoch 00056: val_loss did not improve from 0.01498\n",
      "43/43 [==============================] - 0s 8ms/step - loss: 0.0103 - mse: 0.3724 - val_loss: 0.0152 - val_mse: 0.3537\n",
      "Epoch 57/200\n",
      "43/43 [==============================] - ETA: 0s - loss: 0.0105 - mse: 0.3760\n",
      "Epoch 00057: val_loss did not improve from 0.01498\n",
      "43/43 [==============================] - 0s 8ms/step - loss: 0.0105 - mse: 0.3760 - val_loss: 0.0250 - val_mse: 0.4092\n",
      "Epoch 58/200\n",
      "37/43 [========================>.....] - ETA: 0s - loss: 0.0118 - mse: 0.4002\n",
      "Epoch 00058: val_loss did not improve from 0.01498\n",
      "43/43 [==============================] - 0s 9ms/step - loss: 0.0119 - mse: 0.3908 - val_loss: 0.0267 - val_mse: 0.3977\n",
      "Epoch 59/200\n",
      "40/43 [==========================>...] - ETA: 0s - loss: 0.0098 - mse: 0.3701\n",
      "Epoch 00059: val_loss did not improve from 0.01498\n",
      "43/43 [==============================] - 0s 8ms/step - loss: 0.0100 - mse: 0.3717 - val_loss: 0.0246 - val_mse: 0.4024\n",
      "Epoch 60/200\n",
      "41/43 [===========================>..] - ETA: 0s - loss: 0.0123 - mse: 0.3870\n",
      "Epoch 00060: val_loss did not improve from 0.01498\n",
      "43/43 [==============================] - 0s 9ms/step - loss: 0.0122 - mse: 0.3872 - val_loss: 0.0246 - val_mse: 0.3978\n",
      "Epoch 61/200\n",
      "37/43 [========================>.....] - ETA: 0s - loss: 0.0108 - mse: 0.3853\n",
      "Epoch 00061: val_loss did not improve from 0.01498\n",
      "43/43 [==============================] - 0s 9ms/step - loss: 0.0108 - mse: 0.3863 - val_loss: 0.0263 - val_mse: 0.4205\n",
      "Epoch 62/200\n",
      "39/43 [==========================>...] - ETA: 0s - loss: 0.0099 - mse: 0.3745\n",
      "Epoch 00062: val_loss did not improve from 0.01498\n",
      "43/43 [==============================] - 0s 8ms/step - loss: 0.0108 - mse: 0.3765 - val_loss: 0.0250 - val_mse: 0.4114\n",
      "Epoch 63/200\n",
      "41/43 [===========================>..] - ETA: 0s - loss: 0.0118 - mse: 0.3821\n",
      "Epoch 00063: val_loss did not improve from 0.01498\n",
      "43/43 [==============================] - 0s 8ms/step - loss: 0.0120 - mse: 0.3860 - val_loss: 0.0317 - val_mse: 0.4421\n",
      "Epoch 64/200\n",
      "39/43 [==========================>...] - ETA: 0s - loss: 0.0120 - mse: 0.3921\n",
      "Epoch 00064: val_loss did not improve from 0.01498\n",
      "43/43 [==============================] - 0s 8ms/step - loss: 0.0116 - mse: 0.3861 - val_loss: 0.0294 - val_mse: 0.4633\n",
      "Epoch 65/200\n",
      "40/43 [==========================>...] - ETA: 0s - loss: 0.0098 - mse: 0.3804\n",
      "Epoch 00065: val_loss did not improve from 0.01498\n",
      "43/43 [==============================] - 0s 8ms/step - loss: 0.0097 - mse: 0.3764 - val_loss: 0.0269 - val_mse: 0.4178\n",
      "Epoch 66/200\n",
      "40/43 [==========================>...] - ETA: 0s - loss: 0.0110 - mse: 0.3683\n",
      "Epoch 00066: val_loss did not improve from 0.01498\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.0112 - mse: 0.3767 - val_loss: 0.0275 - val_mse: 0.4238\n",
      "Epoch 67/200\n",
      "40/43 [==========================>...] - ETA: 0s - loss: 0.0103 - mse: 0.3834\n",
      "Epoch 00067: val_loss did not improve from 0.01498\n",
      "43/43 [==============================] - 0s 8ms/step - loss: 0.0107 - mse: 0.3817 - val_loss: 0.0298 - val_mse: 0.4288\n",
      "Epoch 68/200\n",
      "40/43 [==========================>...] - ETA: 0s - loss: 0.0104 - mse: 0.3803\n",
      "Epoch 00068: val_loss did not improve from 0.01498\n",
      "43/43 [==============================] - 0s 8ms/step - loss: 0.0105 - mse: 0.3773 - val_loss: 0.0284 - val_mse: 0.4143\n",
      "Epoch 69/200\n",
      "43/43 [==============================] - ETA: 0s - loss: 0.0103 - mse: 0.3712- ETA: 0s - loss: 0.0128 - mse: 0.\n",
      "Epoch 00069: val_loss did not improve from 0.01498\n",
      "43/43 [==============================] - 0s 9ms/step - loss: 0.0103 - mse: 0.3712 - val_loss: 0.0354 - val_mse: 0.4941\n",
      "Epoch 70/200\n",
      "40/43 [==========================>...] - ETA: 0s - loss: 0.0092 - mse: 0.3634\n",
      "Epoch 00070: val_loss did not improve from 0.01498\n",
      "43/43 [==============================] - 0s 8ms/step - loss: 0.0099 - mse: 0.3744 - val_loss: 0.0401 - val_mse: 0.5183\n",
      "Epoch 71/200\n",
      "42/43 [============================>.] - ETA: 0s - loss: 0.0101 - mse: 0.3779- ETA: 0s - loss: 0.0101 - mse: 0.385\n",
      "Epoch 00071: val_loss did not improve from 0.01498\n",
      "43/43 [==============================] - 0s 9ms/step - loss: 0.0101 - mse: 0.3778 - val_loss: 0.0315 - val_mse: 0.4570\n",
      "Epoch 72/200\n",
      "39/43 [==========================>...] - ETA: 0s - loss: 0.0090 - mse: 0.3761- ETA: 0s - loss: 0.0098 - mse: 0.39\n",
      "Epoch 00072: val_loss did not improve from 0.01498\n",
      "43/43 [==============================] - 0s 9ms/step - loss: 0.0090 - mse: 0.3720 - val_loss: 0.0263 - val_mse: 0.4142\n",
      "Epoch 73/200\n",
      "40/43 [==========================>...] - ETA: 0s - loss: 0.0097 - mse: 0.3728\n",
      "Epoch 00073: val_loss did not improve from 0.01498\n",
      "43/43 [==============================] - 0s 8ms/step - loss: 0.0095 - mse: 0.3668 - val_loss: 0.0258 - val_mse: 0.4076\n",
      "Epoch 74/200\n",
      "39/43 [==========================>...] - ETA: 0s - loss: 0.0096 - mse: 0.3636\n",
      "Epoch 00074: val_loss did not improve from 0.01498\n",
      "43/43 [==============================] - 0s 9ms/step - loss: 0.0097 - mse: 0.3708 - val_loss: 0.0294 - val_mse: 0.4444\n",
      "Epoch 75/200\n",
      "40/43 [==========================>...] - ETA: 0s - loss: 0.0085 - mse: 0.3617\n",
      "Epoch 00075: val_loss did not improve from 0.01498\n",
      "43/43 [==============================] - 0s 9ms/step - loss: 0.0086 - mse: 0.3624 - val_loss: 0.0268 - val_mse: 0.4193\n",
      "Epoch 76/200\n",
      "42/43 [============================>.] - ETA: 0s - loss: 0.0092 - mse: 0.3688\n",
      "Epoch 00076: val_loss did not improve from 0.01498\n",
      "43/43 [==============================] - 0s 7ms/step - loss: 0.0092 - mse: 0.3678 - val_loss: 0.0153 - val_mse: 0.3412\n",
      "Epoch 77/200\n",
      "38/43 [=========================>....] - ETA: 0s - loss: 0.0116 - mse: 0.3951- ETA: 0s - loss: 0.0124 - mse: 0.43\n",
      "Epoch 00077: val_loss did not improve from 0.01498\n",
      "43/43 [==============================] - 0s 9ms/step - loss: 0.0118 - mse: 0.3929 - val_loss: 0.0186 - val_mse: 0.3555\n",
      "Epoch 78/200\n",
      "39/43 [==========================>...] - ETA: 0s - loss: 0.0112 - mse: 0.3826\n",
      "Epoch 00078: val_loss did not improve from 0.01498\n",
      "43/43 [==============================] - 0s 8ms/step - loss: 0.0116 - mse: 0.3884 - val_loss: 0.0175 - val_mse: 0.3421\n",
      "Epoch 79/200\n",
      "42/43 [============================>.] - ETA: 0s - loss: 0.0091 - mse: 0.3620- ETA: 0s - loss: 0.0077 - mse: 0.\n",
      "Epoch 00079: val_loss did not improve from 0.01498\n",
      "43/43 [==============================] - 0s 9ms/step - loss: 0.0092 - mse: 0.3631 - val_loss: 0.0242 - val_mse: 0.3976\n",
      "Epoch 80/200\n",
      "43/43 [==============================] - ETA: 0s - loss: 0.0107 - mse: 0.3700\n",
      "Epoch 00080: val_loss did not improve from 0.01498\n",
      "43/43 [==============================] - 0s 8ms/step - loss: 0.0107 - mse: 0.3700 - val_loss: 0.0452 - val_mse: 0.5764\n",
      "Epoch 81/200\n",
      "42/43 [============================>.] - ETA: 0s - loss: 0.0102 - mse: 0.3606\n",
      "Epoch 00081: val_loss did not improve from 0.01498\n",
      "43/43 [==============================] - 0s 9ms/step - loss: 0.0101 - mse: 0.3605 - val_loss: 0.0384 - val_mse: 0.5263\n",
      "Epoch 82/200\n",
      "35/43 [=======================>......] - ETA: 0s - loss: 0.0106 - mse: 0.3885\n",
      "Epoch 00082: val_loss did not improve from 0.01498\n",
      "43/43 [==============================] - 0s 7ms/step - loss: 0.0099 - mse: 0.3805 - val_loss: 0.0350 - val_mse: 0.4769\n",
      "Epoch 83/200\n",
      "40/43 [==========================>...] - ETA: 0s - loss: 0.0092 - mse: 0.3674\n",
      "Epoch 00083: val_loss did not improve from 0.01498\n",
      "43/43 [==============================] - 0s 9ms/step - loss: 0.0096 - mse: 0.3693 - val_loss: 0.0408 - val_mse: 0.5697\n",
      "Epoch 84/200\n",
      "38/43 [=========================>....] - ETA: 0s - loss: 0.0091 - mse: 0.3723\n",
      "Epoch 00084: val_loss did not improve from 0.01498\n",
      "43/43 [==============================] - 0s 8ms/step - loss: 0.0095 - mse: 0.3658 - val_loss: 0.0364 - val_mse: 0.4923\n",
      "Epoch 85/200\n",
      "43/43 [==============================] - ETA: 0s - loss: 0.0094 - mse: 0.3772\n",
      "Epoch 00085: val_loss did not improve from 0.01498\n",
      "43/43 [==============================] - 0s 9ms/step - loss: 0.0094 - mse: 0.3772 - val_loss: 0.0329 - val_mse: 0.4525\n",
      "Epoch 86/200\n",
      "39/43 [==========================>...] - ETA: 0s - loss: 0.0078 - mse: 0.3619\n",
      "Epoch 00086: val_loss did not improve from 0.01498\n",
      "43/43 [==============================] - 0s 8ms/step - loss: 0.0077 - mse: 0.3637 - val_loss: 0.0335 - val_mse: 0.4520\n",
      "Epoch 87/200\n",
      "43/43 [==============================] - ETA: 0s - loss: 0.0088 - mse: 0.3697\n",
      "Epoch 00087: val_loss did not improve from 0.01498\n",
      "43/43 [==============================] - 0s 9ms/step - loss: 0.0088 - mse: 0.3697 - val_loss: 0.0326 - val_mse: 0.4624\n",
      "Epoch 88/200\n",
      "42/43 [============================>.] - ETA: 0s - loss: 0.0083 - mse: 0.3723- ETA: 0s - loss: 0.0082 - mse: 0.360\n",
      "Epoch 00088: val_loss did not improve from 0.01498\n",
      "43/43 [==============================] - 0s 9ms/step - loss: 0.0083 - mse: 0.3709 - val_loss: 0.0302 - val_mse: 0.4258\n",
      "Epoch 89/200\n",
      "39/43 [==========================>...] - ETA: 0s - loss: 0.0075 - mse: 0.3720\n",
      "Epoch 00089: val_loss did not improve from 0.01498\n",
      "43/43 [==============================] - 0s 8ms/step - loss: 0.0075 - mse: 0.3645 - val_loss: 0.0291 - val_mse: 0.4319\n",
      "Epoch 90/200\n",
      "38/43 [=========================>....] - ETA: 0s - loss: 0.0077 - mse: 0.3579\n",
      "Epoch 00090: val_loss did not improve from 0.01498\n",
      "43/43 [==============================] - 0s 8ms/step - loss: 0.0074 - mse: 0.3563 - val_loss: 0.0239 - val_mse: 0.3864\n",
      "Epoch 91/200\n",
      "41/43 [===========================>..] - ETA: 0s - loss: 0.0084 - mse: 0.3682\n",
      "Epoch 00091: val_loss did not improve from 0.01498\n",
      "43/43 [==============================] - 0s 7ms/step - loss: 0.0084 - mse: 0.3689 - val_loss: 0.0243 - val_mse: 0.4031\n",
      "Epoch 92/200\n",
      "37/43 [========================>.....] - ETA: 0s - loss: 0.0080 - mse: 0.3740\n",
      "Epoch 00092: val_loss did not improve from 0.01498\n",
      "43/43 [==============================] - 0s 8ms/step - loss: 0.0079 - mse: 0.3606 - val_loss: 0.0193 - val_mse: 0.3654\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 93/200\n",
      "35/43 [=======================>......] - ETA: 0s - loss: 0.0089 - mse: 0.3791\n",
      "Epoch 00093: val_loss did not improve from 0.01498\n",
      "43/43 [==============================] - 0s 7ms/step - loss: 0.0088 - mse: 0.3651 - val_loss: 0.0176 - val_mse: 0.3542\n",
      "Epoch 94/200\n",
      "41/43 [===========================>..] - ETA: 0s - loss: 0.0084 - mse: 0.3678\n",
      "Epoch 00094: val_loss did not improve from 0.01498\n",
      "43/43 [==============================] - 0s 8ms/step - loss: 0.0084 - mse: 0.3654 - val_loss: 0.0161 - val_mse: 0.3546\n",
      "Epoch 95/200\n",
      "41/43 [===========================>..] - ETA: 0s - loss: 0.0080 - mse: 0.3577- ETA: 0s - loss: 0.0074 - mse: 0.\n",
      "Epoch 00095: val_loss did not improve from 0.01498\n",
      "43/43 [==============================] - 0s 9ms/step - loss: 0.0079 - mse: 0.3594 - val_loss: 0.0249 - val_mse: 0.4042\n",
      "Epoch 96/200\n",
      "37/43 [========================>.....] - ETA: 0s - loss: 0.0074 - mse: 0.3524\n",
      "Epoch 00096: val_loss did not improve from 0.01498\n",
      "43/43 [==============================] - 0s 8ms/step - loss: 0.0074 - mse: 0.3528 - val_loss: 0.0202 - val_mse: 0.3773\n",
      "Epoch 97/200\n",
      "42/43 [============================>.] - ETA: 0s - loss: 0.0083 - mse: 0.3648\n",
      "Epoch 00097: val_loss improved from 0.01498 to 0.01430, saving model to min_vl_model5.h5\n",
      "43/43 [==============================] - 1s 12ms/step - loss: 0.0083 - mse: 0.3661 - val_loss: 0.0143 - val_mse: 0.3419\n",
      "Epoch 98/200\n",
      "38/43 [=========================>....] - ETA: 0s - loss: 0.0074 - mse: 0.3585\n",
      "Epoch 00098: val_loss improved from 0.01430 to 0.01351, saving model to min_vl_model5.h5\n",
      "43/43 [==============================] - 1s 13ms/step - loss: 0.0075 - mse: 0.3537 - val_loss: 0.0135 - val_mse: 0.3391\n",
      "Epoch 99/200\n",
      "43/43 [==============================] - ETA: 0s - loss: 0.0072 - mse: 0.3594\n",
      "Epoch 00099: val_loss did not improve from 0.01351\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.0072 - mse: 0.3594 - val_loss: 0.0141 - val_mse: 0.3488\n",
      "Epoch 100/200\n",
      "40/43 [==========================>...] - ETA: 0s - loss: 0.0079 - mse: 0.3719\n",
      "Epoch 00100: val_loss did not improve from 0.01351\n",
      "43/43 [==============================] - 0s 8ms/step - loss: 0.0081 - mse: 0.3685 - val_loss: 0.0196 - val_mse: 0.4170\n",
      "Epoch 101/200\n",
      "41/43 [===========================>..] - ETA: 0s - loss: 0.0079 - mse: 0.3576\n",
      "Epoch 00101: val_loss did not improve from 0.01351\n",
      "43/43 [==============================] - 1s 12ms/step - loss: 0.0081 - mse: 0.3646 - val_loss: 0.0210 - val_mse: 0.3705\n",
      "Epoch 102/200\n",
      "41/43 [===========================>..] - ETA: 0s - loss: 0.0071 - mse: 0.3567\n",
      "Epoch 00102: val_loss did not improve from 0.01351\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.0073 - mse: 0.3579 - val_loss: 0.0180 - val_mse: 0.3731\n",
      "Epoch 103/200\n",
      "40/43 [==========================>...] - ETA: 0s - loss: 0.0067 - mse: 0.3501\n",
      "Epoch 00103: val_loss did not improve from 0.01351\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.0067 - mse: 0.3464 - val_loss: 0.0171 - val_mse: 0.3553\n",
      "Epoch 104/200\n",
      "41/43 [===========================>..] - ETA: 0s - loss: 0.0072 - mse: 0.3592\n",
      "Epoch 00104: val_loss did not improve from 0.01351\n",
      "43/43 [==============================] - 0s 9ms/step - loss: 0.0071 - mse: 0.3559 - val_loss: 0.0177 - val_mse: 0.3682\n",
      "Epoch 105/200\n",
      "41/43 [===========================>..] - ETA: 0s - loss: 0.0070 - mse: 0.3725\n",
      "Epoch 00105: val_loss did not improve from 0.01351\n",
      "43/43 [==============================] - 0s 9ms/step - loss: 0.0070 - mse: 0.3705 - val_loss: 0.0155 - val_mse: 0.3423\n",
      "Epoch 106/200\n",
      "41/43 [===========================>..] - ETA: 0s - loss: 0.0070 - mse: 0.3560\n",
      "Epoch 00106: val_loss improved from 0.01351 to 0.01339, saving model to min_vl_model5.h5\n",
      "43/43 [==============================] - 1s 14ms/step - loss: 0.0069 - mse: 0.3537 - val_loss: 0.0134 - val_mse: 0.3335\n",
      "Epoch 107/200\n",
      "37/43 [========================>.....] - ETA: 0s - loss: 0.0072 - mse: 0.3607\n",
      "Epoch 00107: val_loss did not improve from 0.01339\n",
      "43/43 [==============================] - 1s 13ms/step - loss: 0.0070 - mse: 0.3589 - val_loss: 0.0137 - val_mse: 0.3380\n",
      "Epoch 108/200\n",
      "41/43 [===========================>..] - ETA: 0s - loss: 0.0068 - mse: 0.3738\n",
      "Epoch 00108: val_loss did not improve from 0.01339\n",
      "43/43 [==============================] - 0s 11ms/step - loss: 0.0068 - mse: 0.3689 - val_loss: 0.0171 - val_mse: 0.3632\n",
      "Epoch 109/200\n",
      "39/43 [==========================>...] - ETA: 0s - loss: 0.0078 - mse: 0.3646\n",
      "Epoch 00109: val_loss did not improve from 0.01339\n",
      "43/43 [==============================] - 0s 9ms/step - loss: 0.0075 - mse: 0.3659 - val_loss: 0.0162 - val_mse: 0.3462\n",
      "Epoch 110/200\n",
      "40/43 [==========================>...] - ETA: 0s - loss: 0.0075 - mse: 0.3581\n",
      "Epoch 00110: val_loss did not improve from 0.01339\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.0074 - mse: 0.3608 - val_loss: 0.0173 - val_mse: 0.3719\n",
      "Epoch 111/200\n",
      "40/43 [==========================>...] - ETA: 0s - loss: 0.0072 - mse: 0.3618\n",
      "Epoch 00111: val_loss did not improve from 0.01339\n",
      "43/43 [==============================] - 1s 12ms/step - loss: 0.0071 - mse: 0.3593 - val_loss: 0.0161 - val_mse: 0.3497\n",
      "Epoch 112/200\n",
      "43/43 [==============================] - ETA: 0s - loss: 0.0071 - mse: 0.3565\n",
      "Epoch 00112: val_loss did not improve from 0.01339\n",
      "43/43 [==============================] - 0s 9ms/step - loss: 0.0071 - mse: 0.3565 - val_loss: 0.0197 - val_mse: 0.3678\n",
      "Epoch 113/200\n",
      "43/43 [==============================] - ETA: 0s - loss: 0.0084 - mse: 0.3711\n",
      "Epoch 00113: val_loss did not improve from 0.01339\n",
      "43/43 [==============================] - 0s 9ms/step - loss: 0.0084 - mse: 0.3711 - val_loss: 0.0207 - val_mse: 0.3734\n",
      "Epoch 114/200\n",
      "37/43 [========================>.....] - ETA: 0s - loss: 0.0078 - mse: 0.3646\n",
      "Epoch 00114: val_loss did not improve from 0.01339\n",
      "43/43 [==============================] - 0s 8ms/step - loss: 0.0075 - mse: 0.3596 - val_loss: 0.0189 - val_mse: 0.3565\n",
      "Epoch 115/200\n",
      "43/43 [==============================] - ETA: 0s - loss: 0.0083 - mse: 0.3758\n",
      "Epoch 00115: val_loss did not improve from 0.01339\n",
      "43/43 [==============================] - 0s 9ms/step - loss: 0.0083 - mse: 0.3758 - val_loss: 0.0229 - val_mse: 0.3871\n",
      "Epoch 116/200\n",
      "38/43 [=========================>....] - ETA: 0s - loss: 0.0056 - mse: 0.3515\n",
      "Epoch 00116: val_loss did not improve from 0.01339\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.0061 - mse: 0.3485 - val_loss: 0.0258 - val_mse: 0.3999\n",
      "Epoch 117/200\n",
      "40/43 [==========================>...] - ETA: 0s - loss: 0.0066 - mse: 0.3441\n",
      "Epoch 00117: val_loss did not improve from 0.01339\n",
      "43/43 [==============================] - 0s 8ms/step - loss: 0.0072 - mse: 0.3579 - val_loss: 0.0216 - val_mse: 0.3863\n",
      "Epoch 118/200\n",
      "43/43 [==============================] - ETA: 0s - loss: 0.0054 - mse: 0.3451\n",
      "Epoch 00118: val_loss did not improve from 0.01339\n",
      "43/43 [==============================] - 0s 9ms/step - loss: 0.0054 - mse: 0.3451 - val_loss: 0.0247 - val_mse: 0.3912\n",
      "Epoch 119/200\n",
      "39/43 [==========================>...] - ETA: 0s - loss: 0.0058 - mse: 0.3542\n",
      "Epoch 00119: val_loss did not improve from 0.01339\n",
      "43/43 [==============================] - 0s 8ms/step - loss: 0.0060 - mse: 0.3566 - val_loss: 0.0326 - val_mse: 0.4469\n",
      "Epoch 120/200\n",
      "43/43 [==============================] - ETA: 0s - loss: 0.0071 - mse: 0.3554\n",
      "Epoch 00120: val_loss did not improve from 0.01339\n",
      "43/43 [==============================] - 0s 9ms/step - loss: 0.0071 - mse: 0.3554 - val_loss: 0.0354 - val_mse: 0.4606\n",
      "Epoch 121/200\n",
      "43/43 [==============================] - ETA: 0s - loss: 0.0098 - mse: 0.3754\n",
      "Epoch 00121: val_loss did not improve from 0.01339\n",
      "43/43 [==============================] - 0s 9ms/step - loss: 0.0098 - mse: 0.3754 - val_loss: 0.0260 - val_mse: 0.3887\n",
      "Epoch 122/200\n",
      "38/43 [=========================>....] - ETA: 0s - loss: 0.0076 - mse: 0.3564\n",
      "Epoch 00122: val_loss did not improve from 0.01339\n",
      "43/43 [==============================] - 0s 9ms/step - loss: 0.0074 - mse: 0.3541 - val_loss: 0.0326 - val_mse: 0.4270\n",
      "Epoch 123/200\n",
      "39/43 [==========================>...] - ETA: 0s - loss: 0.0079 - mse: 0.3560\n",
      "Epoch 00123: val_loss did not improve from 0.01339\n",
      "43/43 [==============================] - 0s 8ms/step - loss: 0.0077 - mse: 0.3580 - val_loss: 0.0337 - val_mse: 0.4519\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 124/200\n",
      "38/43 [=========================>....] - ETA: 0s - loss: 0.0080 - mse: 0.3539\n",
      "Epoch 00124: val_loss did not improve from 0.01339\n",
      "43/43 [==============================] - 0s 8ms/step - loss: 0.0085 - mse: 0.3622 - val_loss: 0.0344 - val_mse: 0.4609\n",
      "Epoch 125/200\n",
      "39/43 [==========================>...] - ETA: 0s - loss: 0.0075 - mse: 0.3585\n",
      "Epoch 00125: val_loss did not improve from 0.01339\n",
      "43/43 [==============================] - 0s 9ms/step - loss: 0.0075 - mse: 0.3600 - val_loss: 0.0329 - val_mse: 0.4545\n",
      "Epoch 126/200\n",
      "40/43 [==========================>...] - ETA: 0s - loss: 0.0069 - mse: 0.3434\n",
      "Epoch 00126: val_loss did not improve from 0.01339\n",
      "43/43 [==============================] - 0s 8ms/step - loss: 0.0067 - mse: 0.3467 - val_loss: 0.0361 - val_mse: 0.4822\n",
      "Epoch 127/200\n",
      "39/43 [==========================>...] - ETA: 0s - loss: 0.0067 - mse: 0.3455\n",
      "Epoch 00127: val_loss did not improve from 0.01339\n",
      "43/43 [==============================] - 0s 9ms/step - loss: 0.0069 - mse: 0.3557 - val_loss: 0.0389 - val_mse: 0.4985\n",
      "Epoch 128/200\n",
      "39/43 [==========================>...] - ETA: 0s - loss: 0.0063 - mse: 0.3455\n",
      "Epoch 00128: val_loss did not improve from 0.01339\n",
      "43/43 [==============================] - 0s 8ms/step - loss: 0.0066 - mse: 0.3457 - val_loss: 0.0384 - val_mse: 0.4990\n",
      "Epoch 129/200\n",
      "37/43 [========================>.....] - ETA: 0s - loss: 0.0064 - mse: 0.3501\n",
      "Epoch 00129: val_loss did not improve from 0.01339\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.0064 - mse: 0.3494 - val_loss: 0.0327 - val_mse: 0.4579\n",
      "Epoch 130/200\n",
      "43/43 [==============================] - ETA: 0s - loss: 0.0056 - mse: 0.3489\n",
      "Epoch 00130: val_loss did not improve from 0.01339\n",
      "43/43 [==============================] - 0s 9ms/step - loss: 0.0056 - mse: 0.3489 - val_loss: 0.0350 - val_mse: 0.4643\n",
      "Epoch 131/200\n",
      "43/43 [==============================] - ETA: 0s - loss: 0.0059 - mse: 0.3491\n",
      "Epoch 00131: val_loss did not improve from 0.01339\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.0059 - mse: 0.3491 - val_loss: 0.0316 - val_mse: 0.4386\n",
      "Epoch 132/200\n",
      "39/43 [==========================>...] - ETA: 0s - loss: 0.0051 - mse: 0.3476\n",
      "Epoch 00132: val_loss did not improve from 0.01339\n",
      "43/43 [==============================] - 0s 8ms/step - loss: 0.0052 - mse: 0.3438 - val_loss: 0.0341 - val_mse: 0.4501\n",
      "Epoch 133/200\n",
      "39/43 [==========================>...] - ETA: 0s - loss: 0.0061 - mse: 0.3570\n",
      "Epoch 00133: val_loss did not improve from 0.01339\n",
      "43/43 [==============================] - 0s 9ms/step - loss: 0.0059 - mse: 0.3525 - val_loss: 0.0374 - val_mse: 0.4717\n",
      "Epoch 134/200\n",
      "39/43 [==========================>...] - ETA: 0s - loss: 0.0046 - mse: 0.324 - ETA: 0s - loss: 0.0051 - mse: 0.3375\n",
      "Epoch 00134: val_loss did not improve from 0.01339\n",
      "43/43 [==============================] - 0s 9ms/step - loss: 0.0051 - mse: 0.3386 - val_loss: 0.0510 - val_mse: 0.6071\n",
      "Epoch 135/200\n",
      "40/43 [==========================>...] - ETA: 0s - loss: 0.0056 - mse: 0.3472\n",
      "Epoch 00135: val_loss did not improve from 0.01339\n",
      "43/43 [==============================] - 0s 9ms/step - loss: 0.0055 - mse: 0.3522 - val_loss: 0.0353 - val_mse: 0.4676\n",
      "Epoch 136/200\n",
      "40/43 [==========================>...] - ETA: 0s - loss: 0.0053 - mse: 0.3444\n",
      "Epoch 00136: val_loss did not improve from 0.01339\n",
      "43/43 [==============================] - 0s 8ms/step - loss: 0.0054 - mse: 0.3453 - val_loss: 0.0380 - val_mse: 0.4917\n",
      "Epoch 137/200\n",
      "41/43 [===========================>..] - ETA: 0s - loss: 0.0056 - mse: 0.3523\n",
      "Epoch 00137: val_loss did not improve from 0.01339\n",
      "43/43 [==============================] - 0s 9ms/step - loss: 0.0056 - mse: 0.3490 - val_loss: 0.0317 - val_mse: 0.4443\n",
      "Epoch 138/200\n",
      "41/43 [===========================>..] - ETA: 0s - loss: 0.0061 - mse: 0.3501\n",
      "Epoch 00138: val_loss did not improve from 0.01339\n",
      "43/43 [==============================] - 0s 8ms/step - loss: 0.0060 - mse: 0.3500 - val_loss: 0.0353 - val_mse: 0.4559\n",
      "Epoch 139/200\n",
      "40/43 [==========================>...] - ETA: 0s - loss: 0.0068 - mse: 0.3581\n",
      "Epoch 00139: val_loss did not improve from 0.01339\n",
      "43/43 [==============================] - 0s 9ms/step - loss: 0.0065 - mse: 0.3572 - val_loss: 0.0334 - val_mse: 0.4480\n",
      "Epoch 140/200\n",
      "38/43 [=========================>....] - ETA: 0s - loss: 0.0060 - mse: 0.3476\n",
      "Epoch 00140: val_loss did not improve from 0.01339\n",
      "43/43 [==============================] - 0s 8ms/step - loss: 0.0061 - mse: 0.3528 - val_loss: 0.0338 - val_mse: 0.4344\n",
      "Epoch 141/200\n",
      "39/43 [==========================>...] - ETA: 0s - loss: 0.0050 - mse: 0.3411\n",
      "Epoch 00141: val_loss did not improve from 0.01339\n",
      "43/43 [==============================] - 0s 8ms/step - loss: 0.0051 - mse: 0.3441 - val_loss: 0.0343 - val_mse: 0.4470\n",
      "Epoch 142/200\n",
      "38/43 [=========================>....] - ETA: 0s - loss: 0.0047 - mse: 0.3458\n",
      "Epoch 00142: val_loss did not improve from 0.01339\n",
      "43/43 [==============================] - 0s 11ms/step - loss: 0.0048 - mse: 0.3391 - val_loss: 0.0378 - val_mse: 0.4691\n",
      "Epoch 143/200\n",
      "42/43 [============================>.] - ETA: 0s - loss: 0.0052 - mse: 0.3363\n",
      "Epoch 00143: val_loss did not improve from 0.01339\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.0051 - mse: 0.3374 - val_loss: 0.0362 - val_mse: 0.4659\n",
      "Epoch 144/200\n",
      "40/43 [==========================>...] - ETA: 0s - loss: 0.0059 - mse: 0.3342\n",
      "Epoch 00144: val_loss did not improve from 0.01339\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.0073 - mse: 0.3506 - val_loss: 0.0315 - val_mse: 0.4095\n",
      "Epoch 145/200\n",
      "38/43 [=========================>....] - ETA: 0s - loss: 0.0076 - mse: 0.3692\n",
      "Epoch 00145: val_loss did not improve from 0.01339\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.0078 - mse: 0.3767 - val_loss: 0.0177 - val_mse: 0.3490\n",
      "Epoch 146/200\n",
      "38/43 [=========================>....] - ETA: 0s - loss: 0.0069 - mse: 0.3384\n",
      "Epoch 00146: val_loss did not improve from 0.01339\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.0077 - mse: 0.3641 - val_loss: 0.0223 - val_mse: 0.3716\n",
      "Epoch 147/200\n",
      "37/43 [========================>.....] - ETA: 0s - loss: 0.0059 - mse: 0.3408\n",
      "Epoch 00147: val_loss did not improve from 0.01339\n",
      "43/43 [==============================] - 0s 9ms/step - loss: 0.0062 - mse: 0.3544 - val_loss: 0.0230 - val_mse: 0.3717\n",
      "Epoch 148/200\n",
      "41/43 [===========================>..] - ETA: 0s - loss: 0.0050 - mse: 0.3399\n",
      "Epoch 00148: val_loss did not improve from 0.01339\n",
      "43/43 [==============================] - 0s 9ms/step - loss: 0.0051 - mse: 0.3391 - val_loss: 0.0206 - val_mse: 0.3618\n",
      "Epoch 149/200\n",
      "40/43 [==========================>...] - ETA: 0s - loss: 0.0051 - mse: 0.3340\n",
      "Epoch 00149: val_loss did not improve from 0.01339\n",
      "43/43 [==============================] - 0s 9ms/step - loss: 0.0050 - mse: 0.3375 - val_loss: 0.0234 - val_mse: 0.3779\n",
      "Epoch 150/200\n",
      "43/43 [==============================] - ETA: 0s - loss: 0.0069 - mse: 0.3460\n",
      "Epoch 00150: val_loss did not improve from 0.01339\n",
      "43/43 [==============================] - 0s 9ms/step - loss: 0.0069 - mse: 0.3460 - val_loss: 0.0249 - val_mse: 0.3748\n",
      "Epoch 151/200\n",
      "43/43 [==============================] - ETA: 0s - loss: 0.0058 - mse: 0.3478\n",
      "Epoch 00151: val_loss did not improve from 0.01339\n",
      "43/43 [==============================] - 0s 9ms/step - loss: 0.0058 - mse: 0.3478 - val_loss: 0.0315 - val_mse: 0.4232\n",
      "Epoch 152/200\n",
      "43/43 [==============================] - ETA: 0s - loss: 0.0053 - mse: 0.3448\n",
      "Epoch 00152: val_loss did not improve from 0.01339\n",
      "43/43 [==============================] - 0s 9ms/step - loss: 0.0053 - mse: 0.3448 - val_loss: 0.0193 - val_mse: 0.3571\n",
      "Epoch 153/200\n",
      "36/43 [========================>.....] - ETA: 0s - loss: 0.0055 - mse: 0.3417\n",
      "Epoch 00153: val_loss did not improve from 0.01339\n",
      "43/43 [==============================] - 0s 9ms/step - loss: 0.0057 - mse: 0.3473 - val_loss: 0.0277 - val_mse: 0.3944\n",
      "Epoch 154/200\n",
      "42/43 [============================>.] - ETA: 0s - loss: 0.0048 - mse: 0.3394\n",
      "Epoch 00154: val_loss did not improve from 0.01339\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.0048 - mse: 0.3375 - val_loss: 0.0169 - val_mse: 0.3479\n",
      "Epoch 155/200\n",
      "39/43 [==========================>...] - ETA: 0s - loss: 0.0045 - mse: 0.3341\n",
      "Epoch 00155: val_loss did not improve from 0.01339\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.0046 - mse: 0.3336 - val_loss: 0.0141 - val_mse: 0.3388\n",
      "Epoch 156/200\n",
      "37/43 [========================>.....] - ETA: 0s - loss: 0.0048 - mse: 0.3270\n",
      "Epoch 00156: val_loss did not improve from 0.01339\n",
      "43/43 [==============================] - 0s 9ms/step - loss: 0.0052 - mse: 0.3411 - val_loss: 0.0163 - val_mse: 0.3448\n",
      "Epoch 157/200\n",
      "41/43 [===========================>..] - ETA: 0s - loss: 0.0048 - mse: 0.3391\n",
      "Epoch 00157: val_loss did not improve from 0.01339\n",
      "43/43 [==============================] - 0s 8ms/step - loss: 0.0047 - mse: 0.3373 - val_loss: 0.0139 - val_mse: 0.3410\n",
      "Epoch 158/200\n",
      "39/43 [==========================>...] - ETA: 0s - loss: 0.0049 - mse: 0.3450\n",
      "Epoch 00158: val_loss did not improve from 0.01339\n",
      "43/43 [==============================] - 0s 9ms/step - loss: 0.0047 - mse: 0.3412 - val_loss: 0.0149 - val_mse: 0.3508\n",
      "Epoch 159/200\n",
      "37/43 [========================>.....] - ETA: 0s - loss: 0.0060 - mse: 0.3604\n",
      "Epoch 00159: val_loss did not improve from 0.01339\n",
      "43/43 [==============================] - 0s 8ms/step - loss: 0.0066 - mse: 0.3509 - val_loss: 0.0146 - val_mse: 0.3425\n",
      "Epoch 160/200\n",
      "41/43 [===========================>..] - ETA: 0s - loss: 0.0056 - mse: 0.3422\n",
      "Epoch 00160: val_loss did not improve from 0.01339\n",
      "43/43 [==============================] - 0s 8ms/step - loss: 0.0055 - mse: 0.3368 - val_loss: 0.0157 - val_mse: 0.3441\n",
      "Epoch 161/200\n",
      "41/43 [===========================>..] - ETA: 0s - loss: 0.0060 - mse: 0.3378\n",
      "Epoch 00161: val_loss did not improve from 0.01339\n",
      "43/43 [==============================] - 0s 8ms/step - loss: 0.0061 - mse: 0.3464 - val_loss: 0.0265 - val_mse: 0.3852\n",
      "Epoch 162/200\n",
      "41/43 [===========================>..] - ETA: 0s - loss: 0.0081 - mse: 0.3547\n",
      "Epoch 00162: val_loss did not improve from 0.01339\n",
      "43/43 [==============================] - 0s 7ms/step - loss: 0.0080 - mse: 0.3550 - val_loss: 0.0314 - val_mse: 0.4261\n",
      "Epoch 163/200\n",
      "40/43 [==========================>...] - ETA: 0s - loss: 0.0057 - mse: 0.3424\n",
      "Epoch 00163: val_loss did not improve from 0.01339\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.0060 - mse: 0.3385 - val_loss: 0.0242 - val_mse: 0.3813\n",
      "Epoch 164/200\n",
      "37/43 [========================>.....] - ETA: 0s - loss: 0.0045 - mse: 0.3331\n",
      "Epoch 00164: val_loss did not improve from 0.01339\n",
      "43/43 [==============================] - 0s 8ms/step - loss: 0.0045 - mse: 0.3338 - val_loss: 0.0231 - val_mse: 0.3770\n",
      "Epoch 165/200\n",
      "38/43 [=========================>....] - ETA: 0s - loss: 0.0048 - mse: 0.3352\n",
      "Epoch 00165: val_loss did not improve from 0.01339\n",
      "43/43 [==============================] - 0s 8ms/step - loss: 0.0049 - mse: 0.3330 - val_loss: 0.0221 - val_mse: 0.3743\n",
      "Epoch 166/200\n",
      "40/43 [==========================>...] - ETA: 0s - loss: 0.0050 - mse: 0.3370\n",
      "Epoch 00166: val_loss did not improve from 0.01339\n",
      "43/43 [==============================] - 0s 9ms/step - loss: 0.0055 - mse: 0.3494 - val_loss: 0.0256 - val_mse: 0.3972\n",
      "Epoch 167/200\n",
      "41/43 [===========================>..] - ETA: 0s - loss: 0.0048 - mse: 0.3370\n",
      "Epoch 00167: val_loss did not improve from 0.01339\n",
      "43/43 [==============================] - 0s 9ms/step - loss: 0.0047 - mse: 0.3371 - val_loss: 0.0191 - val_mse: 0.3572\n",
      "Epoch 168/200\n",
      "41/43 [===========================>..] - ETA: 0s - loss: 0.0045 - mse: 0.3313\n",
      "Epoch 00168: val_loss did not improve from 0.01339\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.0045 - mse: 0.3305 - val_loss: 0.0205 - val_mse: 0.3652\n",
      "Epoch 169/200\n",
      "42/43 [============================>.] - ETA: 0s - loss: 0.0067 - mse: 0.3414\n",
      "Epoch 00169: val_loss did not improve from 0.01339\n",
      "43/43 [==============================] - 0s 9ms/step - loss: 0.0067 - mse: 0.3413 - val_loss: 0.0143 - val_mse: 0.3471\n",
      "Epoch 170/200\n",
      "38/43 [=========================>....] - ETA: 0s - loss: 0.0064 - mse: 0.3474\n",
      "Epoch 00170: val_loss did not improve from 0.01339\n",
      "43/43 [==============================] - 0s 9ms/step - loss: 0.0063 - mse: 0.3457 - val_loss: 0.0444 - val_mse: 0.5867\n",
      "Epoch 171/200\n",
      "39/43 [==========================>...] - ETA: 0s - loss: 0.0068 - mse: 0.3438\n",
      "Epoch 00171: val_loss did not improve from 0.01339\n",
      "43/43 [==============================] - 0s 8ms/step - loss: 0.0069 - mse: 0.3380 - val_loss: 0.0307 - val_mse: 0.4420\n",
      "Epoch 172/200\n",
      "37/43 [========================>.....] - ETA: 0s - loss: 0.0064 - mse: 0.3360\n",
      "Epoch 00172: val_loss did not improve from 0.01339\n",
      "43/43 [==============================] - 0s 8ms/step - loss: 0.0065 - mse: 0.3493 - val_loss: 0.0348 - val_mse: 0.4639\n",
      "Epoch 173/200\n",
      "42/43 [============================>.] - ETA: 0s - loss: 0.0046 - mse: 0.3312\n",
      "Epoch 00173: val_loss did not improve from 0.01339\n",
      "43/43 [==============================] - 0s 9ms/step - loss: 0.0046 - mse: 0.3304 - val_loss: 0.0347 - val_mse: 0.4682\n",
      "Epoch 174/200\n",
      "41/43 [===========================>..] - ETA: 0s - loss: 0.0043 - mse: 0.3326\n",
      "Epoch 00174: val_loss did not improve from 0.01339\n",
      "43/43 [==============================] - 0s 8ms/step - loss: 0.0044 - mse: 0.3349 - val_loss: 0.0311 - val_mse: 0.4455\n",
      "Epoch 175/200\n",
      "42/43 [============================>.] - ETA: 0s - loss: 0.0042 - mse: 0.3317- ETA: 0s - loss: 0.0040 - mse: 0.322\n",
      "Epoch 00175: val_loss did not improve from 0.01339\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.0043 - mse: 0.3318 - val_loss: 0.0337 - val_mse: 0.4686\n",
      "Epoch 176/200\n",
      "38/43 [=========================>....] - ETA: 0s - loss: 0.0049 - mse: 0.3557\n",
      "Epoch 00176: val_loss did not improve from 0.01339\n",
      "43/43 [==============================] - 0s 8ms/step - loss: 0.0049 - mse: 0.3452 - val_loss: 0.0233 - val_mse: 0.3873\n",
      "Epoch 177/200\n",
      "39/43 [==========================>...] - ETA: 0s - loss: 0.0049 - mse: 0.3534\n",
      "Epoch 00177: val_loss did not improve from 0.01339\n",
      "43/43 [==============================] - 0s 8ms/step - loss: 0.0049 - mse: 0.3432 - val_loss: 0.0295 - val_mse: 0.4404\n",
      "Epoch 178/200\n",
      "38/43 [=========================>....] - ETA: 0s - loss: 0.0057 - mse: 0.3529\n",
      "Epoch 00178: val_loss did not improve from 0.01339\n",
      "43/43 [==============================] - 0s 8ms/step - loss: 0.0058 - mse: 0.3479 - val_loss: 0.0275 - val_mse: 0.4160\n",
      "Epoch 179/200\n",
      "41/43 [===========================>..] - ETA: 0s - loss: 0.0058 - mse: 0.3555- ETA: 0s - loss: 0.0045 - mse: 0.\n",
      "Epoch 00179: val_loss did not improve from 0.01339\n",
      "43/43 [==============================] - 0s 8ms/step - loss: 0.0057 - mse: 0.3516 - val_loss: 0.0203 - val_mse: 0.3800\n",
      "Epoch 180/200\n",
      "40/43 [==========================>...] - ETA: 0s - loss: 0.0045 - mse: 0.3374\n",
      "Epoch 00180: val_loss did not improve from 0.01339\n",
      "43/43 [==============================] - 0s 8ms/step - loss: 0.0046 - mse: 0.3384 - val_loss: 0.0208 - val_mse: 0.3958\n",
      "Epoch 181/200\n",
      "43/43 [==============================] - ETA: 0s - loss: 0.0051 - mse: 0.3418\n",
      "Epoch 00181: val_loss did not improve from 0.01339\n",
      "43/43 [==============================] - 0s 9ms/step - loss: 0.0051 - mse: 0.3418 - val_loss: 0.0259 - val_mse: 0.4779\n",
      "Epoch 182/200\n",
      "39/43 [==========================>...] - ETA: 0s - loss: 0.0049 - mse: 0.3496\n",
      "Epoch 00182: val_loss did not improve from 0.01339\n",
      "43/43 [==============================] - 0s 8ms/step - loss: 0.0049 - mse: 0.3475 - val_loss: 0.0180 - val_mse: 0.3662\n",
      "Epoch 183/200\n",
      "39/43 [==========================>...] - ETA: 0s - loss: 0.0040 - mse: 0.3198\n",
      "Epoch 00183: val_loss did not improve from 0.01339\n",
      "43/43 [==============================] - 0s 8ms/step - loss: 0.0040 - mse: 0.3248 - val_loss: 0.0205 - val_mse: 0.3834\n",
      "Epoch 184/200\n",
      "39/43 [==========================>...] - ETA: 0s - loss: 0.0042 - mse: 0.3311\n",
      "Epoch 00184: val_loss did not improve from 0.01339\n",
      "43/43 [==============================] - 0s 9ms/step - loss: 0.0044 - mse: 0.3339 - val_loss: 0.0214 - val_mse: 0.4042\n",
      "Epoch 185/200\n",
      "39/43 [==========================>...] - ETA: 0s - loss: 0.0042 - mse: 0.3371\n",
      "Epoch 00185: val_loss did not improve from 0.01339\n",
      "43/43 [==============================] - 0s 9ms/step - loss: 0.0041 - mse: 0.3371 - val_loss: 0.0250 - val_mse: 0.4339\n",
      "Epoch 186/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "40/43 [==========================>...] - ETA: 0s - loss: 0.0044 - mse: 0.3370\n",
      "Epoch 00186: val_loss did not improve from 0.01339\n",
      "43/43 [==============================] - 0s 8ms/step - loss: 0.0043 - mse: 0.3326 - val_loss: 0.0220 - val_mse: 0.4015\n",
      "Epoch 187/200\n",
      "39/43 [==========================>...] - ETA: 0s - loss: 0.0036 - mse: 0.3231\n",
      "Epoch 00187: val_loss did not improve from 0.01339\n",
      "43/43 [==============================] - 0s 8ms/step - loss: 0.0036 - mse: 0.3256 - val_loss: 0.0222 - val_mse: 0.3979\n",
      "Epoch 188/200\n",
      "38/43 [=========================>....] - ETA: 0s - loss: 0.0041 - mse: 0.3302\n",
      "Epoch 00188: val_loss did not improve from 0.01339\n",
      "43/43 [==============================] - 0s 8ms/step - loss: 0.0041 - mse: 0.3299 - val_loss: 0.0287 - val_mse: 0.4552\n",
      "Epoch 189/200\n",
      "37/43 [========================>.....] - ETA: 0s - loss: 0.0045 - mse: 0.3394\n",
      "Epoch 00189: val_loss did not improve from 0.01339\n",
      "43/43 [==============================] - 0s 8ms/step - loss: 0.0044 - mse: 0.3375 - val_loss: 0.0291 - val_mse: 0.4741\n",
      "Epoch 190/200\n",
      "38/43 [=========================>....] - ETA: 0s - loss: 0.0041 - mse: 0.3435\n",
      "Epoch 00190: val_loss did not improve from 0.01339\n",
      "43/43 [==============================] - 0s 8ms/step - loss: 0.0040 - mse: 0.3376 - val_loss: 0.0520 - val_mse: 0.6706\n",
      "Epoch 191/200\n",
      "40/43 [==========================>...] - ETA: 0s - loss: 0.0048 - mse: 0.3494\n",
      "Epoch 00191: val_loss did not improve from 0.01339\n",
      "43/43 [==============================] - 0s 8ms/step - loss: 0.0049 - mse: 0.3435 - val_loss: 0.0345 - val_mse: 0.5458\n",
      "Epoch 192/200\n",
      "42/43 [============================>.] - ETA: 0s - loss: 0.0041 - mse: 0.3403\n",
      "Epoch 00192: val_loss did not improve from 0.01339\n",
      "43/43 [==============================] - 0s 8ms/step - loss: 0.0041 - mse: 0.3384 - val_loss: 0.0254 - val_mse: 0.4381\n",
      "Epoch 193/200\n",
      "38/43 [=========================>....] - ETA: 0s - loss: 0.0042 - mse: 0.3376\n",
      "Epoch 00193: val_loss did not improve from 0.01339\n",
      "43/43 [==============================] - 0s 8ms/step - loss: 0.0040 - mse: 0.3365 - val_loss: 0.0236 - val_mse: 0.4091\n",
      "Epoch 194/200\n",
      "36/43 [========================>.....] - ETA: 0s - loss: 0.0041 - mse: 0.3322\n",
      "Epoch 00194: val_loss did not improve from 0.01339\n",
      "43/43 [==============================] - 0s 7ms/step - loss: 0.0041 - mse: 0.3352 - val_loss: 0.0171 - val_mse: 0.3625\n",
      "Epoch 195/200\n",
      "42/43 [============================>.] - ETA: 0s - loss: 0.0037 - mse: 0.3271\n",
      "Epoch 00195: val_loss did not improve from 0.01339\n",
      "43/43 [==============================] - 0s 7ms/step - loss: 0.0040 - mse: 0.3276 - val_loss: 0.0183 - val_mse: 0.3884\n",
      "Epoch 196/200\n",
      "38/43 [=========================>....] - ETA: 0s - loss: 0.0037 - mse: 0.3133\n",
      "Epoch 00196: val_loss did not improve from 0.01339\n",
      "43/43 [==============================] - 0s 9ms/step - loss: 0.0038 - mse: 0.3232 - val_loss: 0.0221 - val_mse: 0.3975\n",
      "Epoch 197/200\n",
      "41/43 [===========================>..] - ETA: 0s - loss: 0.0038 - mse: 0.3326\n",
      "Epoch 00197: val_loss did not improve from 0.01339\n",
      "43/43 [==============================] - 0s 9ms/step - loss: 0.0038 - mse: 0.3350 - val_loss: 0.0230 - val_mse: 0.4413\n",
      "Epoch 198/200\n",
      "39/43 [==========================>...] - ETA: 0s - loss: 0.0035 - mse: 0.3255\n",
      "Epoch 00198: val_loss did not improve from 0.01339\n",
      "43/43 [==============================] - 0s 9ms/step - loss: 0.0034 - mse: 0.3257 - val_loss: 0.0241 - val_mse: 0.4695\n",
      "Epoch 199/200\n",
      "41/43 [===========================>..] - ETA: 0s - loss: 0.0039 - mse: 0.3373\n",
      "Epoch 00199: val_loss did not improve from 0.01339\n",
      "43/43 [==============================] - 0s 7ms/step - loss: 0.0040 - mse: 0.3350 - val_loss: 0.0150 - val_mse: 0.3501\n",
      "Epoch 200/200\n",
      "39/43 [==========================>...] - ETA: 0s - loss: 0.0036 - mse: 0.3312\n",
      "Epoch 00200: val_loss did not improve from 0.01339\n",
      "43/43 [==============================] - 0s 8ms/step - loss: 0.0036 - mse: 0.3317 - val_loss: 0.0159 - val_mse: 0.3441\n"
     ]
    }
   ],
   "source": [
    "#Compiling the ANN\n",
    "opt = keras.optimizers.Adam(lr=0.0015, beta_1=0.9, beta_2=0.999, epsilon=None, decay=0.0, amsgrad=False)\n",
    "model.compile(optimizer=opt, loss='mean_squared_logarithmic_error', metrics=['mse'])\n",
    "#Fitting the ANN to the training set\n",
    "model_filepath = 'min_vl_model5.h5'\n",
    "checkpoint = ModelCheckpoint(model_filepath, monitor = 'val_loss', verbose=1, save_best_only = True, mode='min' )\n",
    "history=model.fit(X_train,y_train, validation_split=0.07, batch_size=32, epochs=200, callbacks=[checkpoint])\n",
    "model.load_weights(model_filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "bizarre-playing",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Inıtialising the ANN\n",
    "model = Sequential()\n",
    "#Adding the input layer and first hidden layer\n",
    "model.add(Dense(units =200, kernel_initializer='random_uniform', activation= 'tanh', \n",
    "                input_dim=X_train.shape[1]))\n",
    "#Add the first hidden layer\n",
    "model.add(Dense(units =50, kernel_initializer='random_uniform', activation= 'tanh'))\n",
    "\n",
    "model.add(Dense(units =1, kernel_initializer='random_uniform', activation= 'relu'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "technological-insured",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "36/43 [========================>.....] - ETA: 0s - loss: 0.0543 - mse: 0.6016\n",
      "Epoch 00001: val_loss improved from inf to 0.02556, saving model to min_vl_model1.h5\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.0516 - mse: 0.5862 - val_loss: 0.0256 - val_mse: 0.4250\n",
      "Epoch 2/200\n",
      "29/43 [===================>..........] - ETA: 0s - loss: 0.0244 - mse: 0.4801\n",
      "Epoch 00002: val_loss improved from 0.02556 to 0.02233, saving model to min_vl_model1.h5\n",
      "43/43 [==============================] - 0s 5ms/step - loss: 0.0242 - mse: 0.4817 - val_loss: 0.0223 - val_mse: 0.4204\n",
      "Epoch 3/200\n",
      "39/43 [==========================>...] - ETA: 0s - loss: 0.0183 - mse: 0.4439\n",
      "Epoch 00003: val_loss did not improve from 0.02233\n",
      "43/43 [==============================] - 0s 4ms/step - loss: 0.0179 - mse: 0.4451 - val_loss: 0.0305 - val_mse: 0.4446\n",
      "Epoch 4/200\n",
      "36/43 [========================>.....] - ETA: 0s - loss: 0.0141 - mse: 0.4128\n",
      "Epoch 00004: val_loss did not improve from 0.02233\n",
      "43/43 [==============================] - 0s 4ms/step - loss: 0.0147 - mse: 0.4229 - val_loss: 0.0319 - val_mse: 0.4534\n",
      "Epoch 5/200\n",
      "38/43 [=========================>....] - ETA: 0s - loss: 0.0138 - mse: 0.4044\n",
      "Epoch 00005: val_loss did not improve from 0.02233\n",
      "43/43 [==============================] - 0s 4ms/step - loss: 0.0132 - mse: 0.4047 - val_loss: 0.0288 - val_mse: 0.4233\n",
      "Epoch 6/200\n",
      "37/43 [========================>.....] - ETA: 0s - loss: 0.0138 - mse: 0.4133\n",
      "Epoch 00006: val_loss did not improve from 0.02233\n",
      "43/43 [==============================] - 0s 3ms/step - loss: 0.0134 - mse: 0.4049 - val_loss: 0.0322 - val_mse: 0.4455\n",
      "Epoch 7/200\n",
      "34/43 [======================>.......] - ETA: 0s - loss: 0.0114 - mse: 0.4032\n",
      "Epoch 00007: val_loss did not improve from 0.02233\n",
      "43/43 [==============================] - 0s 4ms/step - loss: 0.0108 - mse: 0.3918 - val_loss: 0.0329 - val_mse: 0.4342\n",
      "Epoch 8/200\n",
      "40/43 [==========================>...] - ETA: 0s - loss: 0.0096 - mse: 0.3861\n",
      "Epoch 00008: val_loss did not improve from 0.02233\n",
      "43/43 [==============================] - 0s 4ms/step - loss: 0.0093 - mse: 0.3841 - val_loss: 0.0418 - val_mse: 0.4868\n",
      "Epoch 9/200\n",
      "36/43 [========================>.....] - ETA: 0s - loss: 0.0096 - mse: 0.3773\n",
      "Epoch 00009: val_loss did not improve from 0.02233\n",
      "43/43 [==============================] - 0s 4ms/step - loss: 0.0092 - mse: 0.3772 - val_loss: 0.0351 - val_mse: 0.4459\n",
      "Epoch 10/200\n",
      "37/43 [========================>.....] - ETA: 0s - loss: 0.0092 - mse: 0.3849\n",
      "Epoch 00010: val_loss did not improve from 0.02233\n",
      "43/43 [==============================] - 0s 4ms/step - loss: 0.0092 - mse: 0.3712 - val_loss: 0.0373 - val_mse: 0.4579\n",
      "Epoch 11/200\n",
      "38/43 [=========================>....] - ETA: 0s - loss: 0.0084 - mse: 0.3786\n",
      "Epoch 00011: val_loss did not improve from 0.02233\n",
      "43/43 [==============================] - 0s 4ms/step - loss: 0.0080 - mse: 0.3678 - val_loss: 0.0421 - val_mse: 0.5055\n",
      "Epoch 12/200\n",
      "40/43 [==========================>...] - ETA: 0s - loss: 0.0072 - mse: 0.3602\n",
      "Epoch 00012: val_loss did not improve from 0.02233\n",
      "43/43 [==============================] - 0s 3ms/step - loss: 0.0072 - mse: 0.3578 - val_loss: 0.0352 - val_mse: 0.4464\n",
      "Epoch 13/200\n",
      "39/43 [==========================>...] - ETA: 0s - loss: 0.0061 - mse: 0.3525\n",
      "Epoch 00013: val_loss did not improve from 0.02233\n",
      "43/43 [==============================] - 0s 4ms/step - loss: 0.0060 - mse: 0.3535 - val_loss: 0.0346 - val_mse: 0.4445\n",
      "Epoch 14/200\n",
      "38/43 [=========================>....] - ETA: 0s - loss: 0.0052 - mse: 0.3502\n",
      "Epoch 00014: val_loss did not improve from 0.02233\n",
      "43/43 [==============================] - 0s 3ms/step - loss: 0.0051 - mse: 0.3478 - val_loss: 0.0331 - val_mse: 0.4320\n",
      "Epoch 15/200\n",
      "39/43 [==========================>...] - ETA: 0s - loss: 0.0049 - mse: 0.3458\n",
      "Epoch 00015: val_loss did not improve from 0.02233\n",
      "43/43 [==============================] - 0s 4ms/step - loss: 0.0049 - mse: 0.3439 - val_loss: 0.0417 - val_mse: 0.4874\n",
      "Epoch 16/200\n",
      "37/43 [========================>.....] - ETA: 0s - loss: 0.0039 - mse: 0.3357\n",
      "Epoch 00016: val_loss did not improve from 0.02233\n",
      "43/43 [==============================] - 0s 4ms/step - loss: 0.0043 - mse: 0.3406 - val_loss: 0.0428 - val_mse: 0.5022\n",
      "Epoch 17/200\n",
      "38/43 [=========================>....] - ETA: 0s - loss: 0.0039 - mse: 0.3397\n",
      "Epoch 00017: val_loss did not improve from 0.02233\n",
      "43/43 [==============================] - 0s 4ms/step - loss: 0.0039 - mse: 0.3354 - val_loss: 0.0453 - val_mse: 0.5234\n",
      "Epoch 18/200\n",
      "39/43 [==========================>...] - ETA: 0s - loss: 0.0042 - mse: 0.3409\n",
      "Epoch 00018: val_loss did not improve from 0.02233\n",
      "43/43 [==============================] - 0s 4ms/step - loss: 0.0041 - mse: 0.3356 - val_loss: 0.0409 - val_mse: 0.5004\n",
      "Epoch 19/200\n",
      "35/43 [=======================>......] - ETA: 0s - loss: 0.0032 - mse: 0.3321\n",
      "Epoch 00019: val_loss did not improve from 0.02233\n",
      "43/43 [==============================] - 0s 4ms/step - loss: 0.0032 - mse: 0.3308 - val_loss: 0.0453 - val_mse: 0.5305\n",
      "Epoch 20/200\n",
      "32/43 [=====================>........] - ETA: 0s - loss: 0.0031 - mse: 0.3302- ETA: 0s - loss: 0.0035 - mse: 0.348\n",
      "Epoch 00020: val_loss did not improve from 0.02233\n",
      "43/43 [==============================] - 0s 4ms/step - loss: 0.0030 - mse: 0.3272 - val_loss: 0.0483 - val_mse: 0.5668\n",
      "Epoch 21/200\n",
      "33/43 [======================>.......] - ETA: 0s - loss: 0.0028 - mse: 0.3206\n",
      "Epoch 00021: val_loss did not improve from 0.02233\n",
      "43/43 [==============================] - 0s 4ms/step - loss: 0.0027 - mse: 0.3253 - val_loss: 0.0469 - val_mse: 0.5532\n",
      "Epoch 22/200\n",
      "40/43 [==========================>...] - ETA: 0s - loss: 0.0034 - mse: 0.3246\n",
      "Epoch 00022: val_loss did not improve from 0.02233\n",
      "43/43 [==============================] - 0s 3ms/step - loss: 0.0034 - mse: 0.3286 - val_loss: 0.0473 - val_mse: 0.5444\n",
      "Epoch 23/200\n",
      "40/43 [==========================>...] - ETA: 0s - loss: 0.0027 - mse: 0.3223\n",
      "Epoch 00023: val_loss did not improve from 0.02233\n",
      "43/43 [==============================] - 0s 3ms/step - loss: 0.0027 - mse: 0.3227 - val_loss: 0.0463 - val_mse: 0.5438\n",
      "Epoch 24/200\n",
      "37/43 [========================>.....] - ETA: 0s - loss: 0.0030 - mse: 0.3254   \n",
      "Epoch 00024: val_loss did not improve from 0.02233\n",
      "43/43 [==============================] - 0s 4ms/step - loss: 0.0028 - mse: 0.3229 - val_loss: 0.0462 - val_mse: 0.5472\n",
      "Epoch 25/200\n",
      "34/43 [======================>.......] - ETA: 0s - loss: 0.0028 - mse: 0.3296\n",
      "Epoch 00025: val_loss did not improve from 0.02233\n",
      "43/43 [==============================] - 0s 4ms/step - loss: 0.0029 - mse: 0.3241 - val_loss: 0.0423 - val_mse: 0.5026\n",
      "Epoch 26/200\n",
      "35/43 [=======================>......] - ETA: 0s - loss: 0.0025 - mse: 0.3174   \n",
      "Epoch 00026: val_loss did not improve from 0.02233\n",
      "43/43 [==============================] - 0s 4ms/step - loss: 0.0023 - mse: 0.3209 - val_loss: 0.0484 - val_mse: 0.5620\n",
      "Epoch 27/200\n",
      "42/43 [============================>.] - ETA: 0s - loss: 0.0019 - mse: 0.3155   \n",
      "Epoch 00027: val_loss did not improve from 0.02233\n",
      "43/43 [==============================] - 0s 3ms/step - loss: 0.0019 - mse: 0.3181 - val_loss: 0.0501 - val_mse: 0.5758\n",
      "Epoch 28/200\n",
      "34/43 [======================>.......] - ETA: 0s - loss: 0.0024 - mse: 0.3308   \n",
      "Epoch 00028: val_loss did not improve from 0.02233\n",
      "43/43 [==============================] - 0s 4ms/step - loss: 0.0024 - mse: 0.3204 - val_loss: 0.0472 - val_mse: 0.5474\n",
      "Epoch 29/200\n",
      "38/43 [=========================>....] - ETA: 0s - loss: 0.0023 - mse: 0.3101\n",
      "Epoch 00029: val_loss did not improve from 0.02233\n",
      "43/43 [==============================] - 0s 3ms/step - loss: 0.0024 - mse: 0.3189 - val_loss: 0.0514 - val_mse: 0.5797\n",
      "Epoch 30/200\n",
      "36/43 [========================>.....] - ETA: 0s - loss: 0.0024 - mse: 0.3226\n",
      "Epoch 00030: val_loss did not improve from 0.02233\n",
      "43/43 [==============================] - 0s 3ms/step - loss: 0.0023 - mse: 0.3185 - val_loss: 0.0521 - val_mse: 0.5955\n",
      "Epoch 31/200\n",
      "36/43 [========================>.....] - ETA: 0s - loss: 0.0015 - mse: 0.3099\n",
      "Epoch 00031: val_loss did not improve from 0.02233\n",
      "43/43 [==============================] - 0s 4ms/step - loss: 0.0016 - mse: 0.3147 - val_loss: 0.0527 - val_mse: 0.5985\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 32/200\n",
      "39/43 [==========================>...] - ETA: 0s - loss: 0.0016 - mse: 0.3162   \n",
      "Epoch 00032: val_loss did not improve from 0.02233\n",
      "43/43 [==============================] - 0s 3ms/step - loss: 0.0016 - mse: 0.3141 - val_loss: 0.0457 - val_mse: 0.5398\n",
      "Epoch 33/200\n",
      "33/43 [======================>.......] - ETA: 0s - loss: 0.0018 - mse: 0.3178\n",
      "Epoch 00033: val_loss did not improve from 0.02233\n",
      "43/43 [==============================] - 0s 4ms/step - loss: 0.0017 - mse: 0.3136 - val_loss: 0.0496 - val_mse: 0.5687\n",
      "Epoch 34/200\n",
      "42/43 [============================>.] - ETA: 0s - loss: 0.0016 - mse: 0.3136  \n",
      "Epoch 00034: val_loss did not improve from 0.02233\n",
      "43/43 [==============================] - 0s 5ms/step - loss: 0.0016 - mse: 0.3131 - val_loss: 0.0463 - val_mse: 0.5432\n",
      "Epoch 35/200\n",
      "36/43 [========================>.....] - ETA: 0s - loss: 0.0016 - mse: 0.3108   \n",
      "Epoch 00035: val_loss did not improve from 0.02233\n",
      "43/43 [==============================] - 0s 4ms/step - loss: 0.0015 - mse: 0.3120 - val_loss: 0.0496 - val_mse: 0.5824\n",
      "Epoch 36/200\n",
      "33/43 [======================>.......] - ETA: 0s - loss: 0.0014 - mse: 0.3208   \n",
      "Epoch 00036: val_loss did not improve from 0.02233\n",
      "43/43 [==============================] - 0s 4ms/step - loss: 0.0014 - mse: 0.3111 - val_loss: 0.0523 - val_mse: 0.6112\n",
      "Epoch 37/200\n",
      "39/43 [==========================>...] - ETA: 0s - loss: 0.0013 - mse: 0.3119\n",
      "Epoch 00037: val_loss did not improve from 0.02233\n",
      "43/43 [==============================] - 0s 5ms/step - loss: 0.0013 - mse: 0.3106 - val_loss: 0.0492 - val_mse: 0.5751\n",
      "Epoch 38/200\n",
      "36/43 [========================>.....] - ETA: 0s - loss: 0.0012 - mse: 0.2999   \n",
      "Epoch 00038: val_loss did not improve from 0.02233\n",
      "43/43 [==============================] - 0s 4ms/step - loss: 0.0011 - mse: 0.3092 - val_loss: 0.0499 - val_mse: 0.5809\n",
      "Epoch 39/200\n",
      "39/43 [==========================>...] - ETA: 0s - loss: 0.0012 - mse: 0.3128  \n",
      "Epoch 00039: val_loss did not improve from 0.02233\n",
      "43/43 [==============================] - 0s 5ms/step - loss: 0.0011 - mse: 0.3083 - val_loss: 0.0517 - val_mse: 0.5949\n",
      "Epoch 40/200\n",
      "31/43 [====================>.........] - ETA: 0s - loss: 0.0010 - mse: 0.2919\n",
      "Epoch 00040: val_loss did not improve from 0.02233\n",
      "43/43 [==============================] - 0s 4ms/step - loss: 0.0010 - mse: 0.3076 - val_loss: 0.0540 - val_mse: 0.6244\n",
      "Epoch 41/200\n",
      "43/43 [==============================] - ETA: 0s - loss: 0.0011 - mse: 0.3075  \n",
      "Epoch 00041: val_loss did not improve from 0.02233\n",
      "43/43 [==============================] - 0s 5ms/step - loss: 0.0011 - mse: 0.3075 - val_loss: 0.0515 - val_mse: 0.5983\n",
      "Epoch 42/200\n",
      "42/43 [============================>.] - ETA: 0s - loss: 0.0010 - mse: 0.3072   \n",
      "Epoch 00042: val_loss did not improve from 0.02233\n",
      "43/43 [==============================] - 0s 5ms/step - loss: 9.9941e-04 - mse: 0.3066 - val_loss: 0.0551 - val_mse: 0.6368\n",
      "Epoch 43/200\n",
      "34/43 [======================>.......] - ETA: 0s - loss: 0.0012 - mse: 0.3185    \n",
      "Epoch 00043: val_loss did not improve from 0.02233\n",
      "43/43 [==============================] - 0s 4ms/step - loss: 0.0012 - mse: 0.3075 - val_loss: 0.0512 - val_mse: 0.5934\n",
      "Epoch 44/200\n",
      "34/43 [======================>.......] - ETA: 0s - loss: 0.0010 - mse: 0.3119    \n",
      "Epoch 00044: val_loss did not improve from 0.02233\n",
      "43/43 [==============================] - 0s 4ms/step - loss: 0.0010 - mse: 0.3074 - val_loss: 0.0522 - val_mse: 0.6115\n",
      "Epoch 45/200\n",
      "29/43 [===================>..........] - ETA: 0s - loss: 9.5080e-04 - mse: 0.3112\n",
      "Epoch 00045: val_loss did not improve from 0.02233\n",
      "43/43 [==============================] - 0s 5ms/step - loss: 0.0010 - mse: 0.3067 - val_loss: 0.0532 - val_mse: 0.6146\n",
      "Epoch 46/200\n",
      "33/43 [======================>.......] - ETA: 0s - loss: 0.0010 - mse: 0.3090    \n",
      "Epoch 00046: val_loss did not improve from 0.02233\n",
      "43/43 [==============================] - 0s 4ms/step - loss: 9.5115e-04 - mse: 0.3056 - val_loss: 0.0504 - val_mse: 0.5900\n",
      "Epoch 47/200\n",
      "31/43 [====================>.........] - ETA: 0s - loss: 9.1489e-04 - mse: 0.3029\n",
      "Epoch 00047: val_loss did not improve from 0.02233\n",
      "43/43 [==============================] - 0s 5ms/step - loss: 9.5058e-04 - mse: 0.3055 - val_loss: 0.0566 - val_mse: 0.6603\n",
      "Epoch 48/200\n",
      "43/43 [==============================] - ETA: 0s - loss: 8.6322e-04 - mse: 0.3046\n",
      "Epoch 00048: val_loss did not improve from 0.02233\n",
      "43/43 [==============================] - 0s 5ms/step - loss: 8.6322e-04 - mse: 0.3046 - val_loss: 0.0517 - val_mse: 0.6081\n",
      "Epoch 49/200\n",
      "37/43 [========================>.....] - ETA: 0s - loss: 9.7383e-04 - mse: 0.2984\n",
      "Epoch 00049: val_loss did not improve from 0.02233\n",
      "43/43 [==============================] - 0s 4ms/step - loss: 9.2157e-04 - mse: 0.3046 - val_loss: 0.0504 - val_mse: 0.5915\n",
      "Epoch 50/200\n",
      "34/43 [======================>.......] - ETA: 0s - loss: 0.0010 - mse: 0.3116    \n",
      "Epoch 00050: val_loss did not improve from 0.02233\n",
      "43/43 [==============================] - 0s 4ms/step - loss: 0.0010 - mse: 0.3052 - val_loss: 0.0566 - val_mse: 0.6691\n",
      "Epoch 51/200\n",
      "32/43 [=====================>........] - ETA: 0s - loss: 0.0011 - mse: 0.3068\n",
      "Epoch 00051: val_loss did not improve from 0.02233\n",
      "43/43 [==============================] - 0s 4ms/step - loss: 9.9670e-04 - mse: 0.3048 - val_loss: 0.0501 - val_mse: 0.5854\n",
      "Epoch 52/200\n",
      "42/43 [============================>.] - ETA: 0s - loss: 9.0697e-04 - mse: 0.3038\n",
      "Epoch 00052: val_loss did not improve from 0.02233\n",
      "43/43 [==============================] - 0s 3ms/step - loss: 8.9986e-04 - mse: 0.3043 - val_loss: 0.0516 - val_mse: 0.5982\n",
      "Epoch 53/200\n",
      "34/43 [======================>.......] - ETA: 0s - loss: 8.0406e-04 - mse: 0.2955\n",
      "Epoch 00053: val_loss did not improve from 0.02233\n",
      "43/43 [==============================] - 0s 4ms/step - loss: 0.0012 - mse: 0.3049 - val_loss: 0.0474 - val_mse: 0.5671\n",
      "Epoch 54/200\n",
      "33/43 [======================>.......] - ETA: 0s - loss: 0.0017 - mse: 0.3058\n",
      "Epoch 00054: val_loss did not improve from 0.02233\n",
      "43/43 [==============================] - 0s 4ms/step - loss: 0.0016 - mse: 0.3070 - val_loss: 0.0541 - val_mse: 0.6291\n",
      "Epoch 55/200\n",
      "41/43 [===========================>..] - ETA: 0s - loss: 0.0012 - mse: 0.3057\n",
      "Epoch 00055: val_loss did not improve from 0.02233\n",
      "43/43 [==============================] - 0s 3ms/step - loss: 0.0012 - mse: 0.3055 - val_loss: 0.0590 - val_mse: 0.6929\n",
      "Epoch 56/200\n",
      "32/43 [=====================>........] - ETA: 0s - loss: 0.0010 - mse: 0.3061    \n",
      "Epoch 00056: val_loss did not improve from 0.02233\n",
      "43/43 [==============================] - 0s 4ms/step - loss: 0.0011 - mse: 0.3049 - val_loss: 0.0598 - val_mse: 0.6997\n",
      "Epoch 57/200\n",
      "33/43 [======================>.......] - ETA: 0s - loss: 0.0011 - mse: 0.2985   \n",
      "Epoch 00057: val_loss did not improve from 0.02233\n",
      "43/43 [==============================] - 0s 4ms/step - loss: 0.0011 - mse: 0.3047 - val_loss: 0.0586 - val_mse: 0.6958\n",
      "Epoch 58/200\n",
      "35/43 [=======================>......] - ETA: 0s - loss: 7.0471e-04 - mse: 0.3070\n",
      "Epoch 00058: val_loss did not improve from 0.02233\n",
      "43/43 [==============================] - 0s 4ms/step - loss: 7.8145e-04 - mse: 0.3033 - val_loss: 0.0611 - val_mse: 0.7198\n",
      "Epoch 59/200\n",
      "32/43 [=====================>........] - ETA: 0s - loss: 5.8241e-04 - mse: 0.3100\n",
      "Epoch 00059: val_loss did not improve from 0.02233\n",
      "43/43 [==============================] - 0s 4ms/step - loss: 6.9970e-04 - mse: 0.3031 - val_loss: 0.0585 - val_mse: 0.6913\n",
      "Epoch 60/200\n",
      "34/43 [======================>.......] - ETA: 0s - loss: 4.9721e-04 - mse: 0.2945\n",
      "Epoch 00060: val_loss did not improve from 0.02233\n",
      "43/43 [==============================] - 0s 4ms/step - loss: 5.7878e-04 - mse: 0.3021 - val_loss: 0.0624 - val_mse: 0.7417\n",
      "Epoch 61/200\n",
      "39/43 [==========================>...] - ETA: 0s - loss: 4.9617e-04 - mse: 0.3035\n",
      "Epoch 00061: val_loss did not improve from 0.02233\n",
      "43/43 [==============================] - 0s 3ms/step - loss: 5.7742e-04 - mse: 0.3017 - val_loss: 0.0608 - val_mse: 0.7277\n",
      "Epoch 62/200\n",
      "35/43 [=======================>......] - ETA: 0s - loss: 7.4306e-04 - mse: 0.3074\n",
      "Epoch 00062: val_loss did not improve from 0.02233\n",
      "43/43 [==============================] - 0s 4ms/step - loss: 7.2216e-04 - mse: 0.3017 - val_loss: 0.0580 - val_mse: 0.6889\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 63/200\n",
      "33/43 [======================>.......] - ETA: 0s - loss: 5.6777e-04 - mse: 0.3043\n",
      "Epoch 00063: val_loss did not improve from 0.02233\n",
      "43/43 [==============================] - 0s 4ms/step - loss: 5.3845e-04 - mse: 0.3012 - val_loss: 0.0583 - val_mse: 0.7025\n",
      "Epoch 64/200\n",
      "35/43 [=======================>......] - ETA: 0s - loss: 5.4746e-04 - mse: 0.2935\n",
      "Epoch 00064: val_loss did not improve from 0.02233\n",
      "43/43 [==============================] - 0s 4ms/step - loss: 5.1161e-04 - mse: 0.3009 - val_loss: 0.0582 - val_mse: 0.6958\n",
      "Epoch 65/200\n",
      "35/43 [=======================>......] - ETA: 0s - loss: 5.5940e-04 - mse: 0.3016\n",
      "Epoch 00065: val_loss did not improve from 0.02233\n",
      "43/43 [==============================] - 0s 4ms/step - loss: 5.3345e-04 - mse: 0.3010 - val_loss: 0.0612 - val_mse: 0.7334\n",
      "Epoch 66/200\n",
      "34/43 [======================>.......] - ETA: 0s - loss: 6.5394e-04 - mse: 0.3145\n",
      "Epoch 00066: val_loss did not improve from 0.02233\n",
      "43/43 [==============================] - 0s 6ms/step - loss: 7.1238e-04 - mse: 0.3016 - val_loss: 0.0559 - val_mse: 0.6601\n",
      "Epoch 67/200\n",
      "33/43 [======================>.......] - ETA: 0s - loss: 5.3141e-04 - mse: 0.3094\n",
      "Epoch 00067: val_loss did not improve from 0.02233\n",
      "43/43 [==============================] - 0s 4ms/step - loss: 5.8194e-04 - mse: 0.3012 - val_loss: 0.0599 - val_mse: 0.7153\n",
      "Epoch 68/200\n",
      "30/43 [===================>..........] - ETA: 0s - loss: 5.4509e-04 - mse: 0.2949- ETA: 0s - loss: 4.5876e-04 - mse: 0.290\n",
      "Epoch 00068: val_loss did not improve from 0.02233\n",
      "43/43 [==============================] - 0s 5ms/step - loss: 6.0513e-04 - mse: 0.3009 - val_loss: 0.0594 - val_mse: 0.7072\n",
      "Epoch 69/200\n",
      "40/43 [==========================>...] - ETA: 0s - loss: 5.3231e-04 - mse: 0.3003\n",
      "Epoch 00069: val_loss did not improve from 0.02233\n",
      "43/43 [==============================] - 0s 5ms/step - loss: 5.6285e-04 - mse: 0.3005 - val_loss: 0.0619 - val_mse: 0.7402\n",
      "Epoch 70/200\n",
      "38/43 [=========================>....] - ETA: 0s - loss: 5.6974e-04 - mse: 0.3005\n",
      "Epoch 00070: val_loss did not improve from 0.02233\n",
      "43/43 [==============================] - 0s 4ms/step - loss: 5.6719e-04 - mse: 0.3004 - val_loss: 0.0582 - val_mse: 0.7028\n",
      "Epoch 71/200\n",
      "33/43 [======================>.......] - ETA: 0s - loss: 4.6827e-04 - mse: 0.3045\n",
      "Epoch 00071: val_loss did not improve from 0.02233\n",
      "43/43 [==============================] - 0s 4ms/step - loss: 4.9196e-04 - mse: 0.3003 - val_loss: 0.0624 - val_mse: 0.7495\n",
      "Epoch 72/200\n",
      "35/43 [=======================>......] - ETA: 0s - loss: 4.9865e-04 - mse: 0.3017\n",
      "Epoch 00072: val_loss did not improve from 0.02233\n",
      "43/43 [==============================] - 0s 4ms/step - loss: 5.0576e-04 - mse: 0.3003 - val_loss: 0.0577 - val_mse: 0.6895\n",
      "Epoch 73/200\n",
      "38/43 [=========================>....] - ETA: 0s - loss: 5.6819e-04 - mse: 0.3025\n",
      "Epoch 00073: val_loss did not improve from 0.02233\n",
      "43/43 [==============================] - 0s 4ms/step - loss: 5.6885e-04 - mse: 0.3005 - val_loss: 0.0616 - val_mse: 0.7454\n",
      "Epoch 74/200\n",
      "30/43 [===================>..........] - ETA: 0s - loss: 5.4296e-04 - mse: 0.3084\n",
      "Epoch 00074: val_loss did not improve from 0.02233\n",
      "43/43 [==============================] - 0s 5ms/step - loss: 5.8862e-04 - mse: 0.3011 - val_loss: 0.0567 - val_mse: 0.6791\n",
      "Epoch 75/200\n",
      "29/43 [===================>..........] - ETA: 0s - loss: 7.9857e-04 - mse: 0.3100\n",
      "Epoch 00075: val_loss did not improve from 0.02233\n",
      "43/43 [==============================] - 0s 4ms/step - loss: 7.1353e-04 - mse: 0.3023 - val_loss: 0.0619 - val_mse: 0.7431\n",
      "Epoch 76/200\n",
      "33/43 [======================>.......] - ETA: 0s - loss: 5.4316e-04 - mse: 0.2913\n",
      "Epoch 00076: val_loss did not improve from 0.02233\n",
      "43/43 [==============================] - 0s 4ms/step - loss: 7.0847e-04 - mse: 0.3017 - val_loss: 0.0596 - val_mse: 0.7122\n",
      "Epoch 77/200\n",
      "34/43 [======================>.......] - ETA: 0s - loss: 9.3838e-04 - mse: 0.2967\n",
      "Epoch 00077: val_loss did not improve from 0.02233\n",
      "43/43 [==============================] - 0s 4ms/step - loss: 0.0010 - mse: 0.3025 - val_loss: 0.0646 - val_mse: 0.7668\n",
      "Epoch 78/200\n",
      "32/43 [=====================>........] - ETA: 0s - loss: 0.0011 - mse: 0.2960\n",
      "Epoch 00078: val_loss did not improve from 0.02233\n",
      "43/43 [==============================] - 0s 4ms/step - loss: 0.0010 - mse: 0.3023 - val_loss: 0.0622 - val_mse: 0.7475\n",
      "Epoch 79/200\n",
      "32/43 [=====================>........] - ETA: 0s - loss: 7.2580e-04 - mse: 0.2976\n",
      "Epoch 00079: val_loss did not improve from 0.02233\n",
      "43/43 [==============================] - 0s 4ms/step - loss: 6.4169e-04 - mse: 0.3010 - val_loss: 0.0639 - val_mse: 0.7745\n",
      "Epoch 80/200\n",
      "41/43 [===========================>..] - ETA: 0s - loss: 5.8843e-04 - mse: 0.2964\n",
      "Epoch 00080: val_loss did not improve from 0.02233\n",
      "43/43 [==============================] - 0s 3ms/step - loss: 5.7945e-04 - mse: 0.3005 - val_loss: 0.0628 - val_mse: 0.7555\n",
      "Epoch 81/200\n",
      "38/43 [=========================>....] - ETA: 0s - loss: 5.2096e-04 - mse: 0.3045\n",
      "Epoch 00081: val_loss did not improve from 0.02233\n",
      "43/43 [==============================] - 0s 4ms/step - loss: 5.0669e-04 - mse: 0.3000 - val_loss: 0.0628 - val_mse: 0.7682\n",
      "Epoch 82/200\n",
      "29/43 [===================>..........] - ETA: 0s - loss: 3.6762e-04 - mse: 0.3005\n",
      "Epoch 00082: val_loss did not improve from 0.02233\n",
      "43/43 [==============================] - 0s 4ms/step - loss: 4.9315e-04 - mse: 0.3000 - val_loss: 0.0598 - val_mse: 0.7157\n",
      "Epoch 83/200\n",
      "35/43 [=======================>......] - ETA: 0s - loss: 5.7336e-04 - mse: 0.2952\n",
      "Epoch 00083: val_loss did not improve from 0.02233\n",
      "43/43 [==============================] - 0s 4ms/step - loss: 5.3489e-04 - mse: 0.2998 - val_loss: 0.0602 - val_mse: 0.7335\n",
      "Epoch 84/200\n",
      "34/43 [======================>.......] - ETA: 0s - loss: 7.9006e-04 - mse: 0.3147\n",
      "Epoch 00084: val_loss did not improve from 0.02233\n",
      "43/43 [==============================] - 0s 4ms/step - loss: 7.5695e-04 - mse: 0.3004 - val_loss: 0.0637 - val_mse: 0.7703\n",
      "Epoch 85/200\n",
      "33/43 [======================>.......] - ETA: 0s - loss: 5.8561e-04 - mse: 0.2932\n",
      "Epoch 00085: val_loss did not improve from 0.02233\n",
      "43/43 [==============================] - 0s 4ms/step - loss: 6.3748e-04 - mse: 0.3002 - val_loss: 0.0603 - val_mse: 0.7292\n",
      "Epoch 86/200\n",
      "27/43 [=================>............] - ETA: 0s - loss: 6.3946e-04 - mse: 0.3146\n",
      "Epoch 00086: val_loss did not improve from 0.02233\n",
      "43/43 [==============================] - 0s 4ms/step - loss: 5.8894e-04 - mse: 0.3003 - val_loss: 0.0593 - val_mse: 0.7133\n",
      "Epoch 87/200\n",
      "38/43 [=========================>....] - ETA: 0s - loss: 4.6903e-04 - mse: 0.3057\n",
      "Epoch 00087: val_loss did not improve from 0.02233\n",
      "43/43 [==============================] - 0s 4ms/step - loss: 4.8483e-04 - mse: 0.3005 - val_loss: 0.0603 - val_mse: 0.7296\n",
      "Epoch 88/200\n",
      "33/43 [======================>.......] - ETA: 0s - loss: 5.5939e-04 - mse: 0.2891\n",
      "Epoch 00088: val_loss did not improve from 0.02233\n",
      "43/43 [==============================] - 0s 4ms/step - loss: 5.1656e-04 - mse: 0.2998 - val_loss: 0.0632 - val_mse: 0.7630\n",
      "Epoch 89/200\n",
      "32/43 [=====================>........] - ETA: 0s - loss: 8.4583e-04 - mse: 0.2969\n",
      "Epoch 00089: val_loss did not improve from 0.02233\n",
      "43/43 [==============================] - 0s 4ms/step - loss: 8.4026e-04 - mse: 0.3006 - val_loss: 0.0636 - val_mse: 0.7634\n",
      "Epoch 90/200\n",
      "31/43 [====================>.........] - ETA: 0s - loss: 7.8354e-04 - mse: 0.2837\n",
      "Epoch 00090: val_loss did not improve from 0.02233\n",
      "43/43 [==============================] - 0s 4ms/step - loss: 6.5889e-04 - mse: 0.3000 - val_loss: 0.0616 - val_mse: 0.7404\n",
      "Epoch 91/200\n",
      "41/43 [===========================>..] - ETA: 0s - loss: 5.6338e-04 - mse: 0.2972\n",
      "Epoch 00091: val_loss did not improve from 0.02233\n",
      "43/43 [==============================] - 0s 5ms/step - loss: 5.9141e-04 - mse: 0.3007 - val_loss: 0.0625 - val_mse: 0.7418\n",
      "Epoch 92/200\n",
      "37/43 [========================>.....] - ETA: 0s - loss: 4.2791e-04 - mse: 0.2977\n",
      "Epoch 00092: val_loss did not improve from 0.02233\n",
      "43/43 [==============================] - 0s 5ms/step - loss: 4.6448e-04 - mse: 0.2998 - val_loss: 0.0637 - val_mse: 0.7776\n",
      "Epoch 93/200\n",
      "31/43 [====================>.........] - ETA: 0s - loss: 3.3963e-04 - mse: 0.2891\n",
      "Epoch 00093: val_loss did not improve from 0.02233\n",
      "43/43 [==============================] - 0s 4ms/step - loss: 4.0308e-04 - mse: 0.2991 - val_loss: 0.0586 - val_mse: 0.7012\n",
      "Epoch 94/200\n",
      "38/43 [=========================>....] - ETA: 0s - loss: 3.5671e-04 - mse: 0.3035\n",
      "Epoch 00094: val_loss did not improve from 0.02233\n",
      "43/43 [==============================] - 0s 5ms/step - loss: 3.5460e-04 - mse: 0.2989 - val_loss: 0.0591 - val_mse: 0.7147\n",
      "Epoch 95/200\n",
      "43/43 [==============================] - ETA: 0s - loss: 3.6136e-04 - mse: 0.2987\n",
      "Epoch 00095: val_loss did not improve from 0.02233\n",
      "43/43 [==============================] - 0s 4ms/step - loss: 3.6136e-04 - mse: 0.2987 - val_loss: 0.0607 - val_mse: 0.7303\n",
      "Epoch 96/200\n",
      "31/43 [====================>.........] - ETA: 0s - loss: 3.3914e-04 - mse: 0.3123\n",
      "Epoch 00096: val_loss did not improve from 0.02233\n",
      "43/43 [==============================] - 0s 4ms/step - loss: 3.4087e-04 - mse: 0.2984 - val_loss: 0.0601 - val_mse: 0.7276\n",
      "Epoch 97/200\n",
      "42/43 [============================>.] - ETA: 0s - loss: 3.0690e-04 - mse: 0.2980\n",
      "Epoch 00097: val_loss did not improve from 0.02233\n",
      "43/43 [==============================] - 0s 6ms/step - loss: 3.0605e-04 - mse: 0.2982 - val_loss: 0.0601 - val_mse: 0.7215\n",
      "Epoch 98/200\n",
      "32/43 [=====================>........] - ETA: 0s - loss: 3.7341e-04 - mse: 0.2979\n",
      "Epoch 00098: val_loss did not improve from 0.02233\n",
      "43/43 [==============================] - 0s 4ms/step - loss: 3.4297e-04 - mse: 0.2993 - val_loss: 0.0654 - val_mse: 0.7967\n",
      "Epoch 99/200\n",
      "34/43 [======================>.......] - ETA: 0s - loss: 3.7456e-04 - mse: 0.2993\n",
      "Epoch 00099: val_loss did not improve from 0.02233\n",
      "43/43 [==============================] - 0s 4ms/step - loss: 3.5676e-04 - mse: 0.2987 - val_loss: 0.0599 - val_mse: 0.7135\n",
      "Epoch 100/200\n",
      "29/43 [===================>..........] - ETA: 0s - loss: 3.5977e-04 - mse: 0.3011\n",
      "Epoch 00100: val_loss did not improve from 0.02233\n",
      "43/43 [==============================] - 0s 4ms/step - loss: 3.8883e-04 - mse: 0.2989 - val_loss: 0.0614 - val_mse: 0.7386\n",
      "Epoch 101/200\n",
      "28/43 [==================>...........] - ETA: 0s - loss: 3.4991e-04 - mse: 0.3163\n",
      "Epoch 00101: val_loss did not improve from 0.02233\n",
      "43/43 [==============================] - 0s 4ms/step - loss: 4.1477e-04 - mse: 0.2990 - val_loss: 0.0613 - val_mse: 0.7407\n",
      "Epoch 102/200\n",
      "39/43 [==========================>...] - ETA: 0s - loss: 3.5289e-04 - mse: 0.3016\n",
      "Epoch 00102: val_loss did not improve from 0.02233\n",
      "43/43 [==============================] - 0s 4ms/step - loss: 3.8755e-04 - mse: 0.2985 - val_loss: 0.0616 - val_mse: 0.7501\n",
      "Epoch 103/200\n",
      "43/43 [==============================] - ETA: 0s - loss: 4.3607e-04 - mse: 0.2987\n",
      "Epoch 00103: val_loss did not improve from 0.02233\n",
      "43/43 [==============================] - 0s 5ms/step - loss: 4.3607e-04 - mse: 0.2987 - val_loss: 0.0643 - val_mse: 0.7793\n",
      "Epoch 104/200\n",
      "37/43 [========================>.....] - ETA: 0s - loss: 5.3706e-04 - mse: 0.2945\n",
      "Epoch 00104: val_loss did not improve from 0.02233\n",
      "43/43 [==============================] - 0s 3ms/step - loss: 9.9017e-04 - mse: 0.3005 - val_loss: 0.0686 - val_mse: 0.8140\n",
      "Epoch 105/200\n",
      "42/43 [============================>.] - ETA: 0s - loss: 0.0124 - mse: 0.3491\n",
      "Epoch 00105: val_loss did not improve from 0.02233\n",
      "43/43 [==============================] - 0s 3ms/step - loss: 0.0123 - mse: 0.3498 - val_loss: 0.0560 - val_mse: 0.6785\n",
      "Epoch 106/200\n",
      "37/43 [========================>.....] - ETA: 0s - loss: 0.0091 - mse: 0.3536\n",
      "Epoch 00106: val_loss did not improve from 0.02233\n",
      "43/43 [==============================] - 0s 4ms/step - loss: 0.0095 - mse: 0.3579 - val_loss: 0.0823 - val_mse: 0.9063\n",
      "Epoch 107/200\n",
      "32/43 [=====================>........] - ETA: 0s - loss: 0.0057 - mse: 0.3327\n",
      "Epoch 00107: val_loss did not improve from 0.02233\n",
      "43/43 [==============================] - 0s 4ms/step - loss: 0.0058 - mse: 0.3305 - val_loss: 0.0820 - val_mse: 0.9433\n",
      "Epoch 108/200\n",
      "33/43 [======================>.......] - ETA: 0s - loss: 0.0036 - mse: 0.3230\n",
      "Epoch 00108: val_loss did not improve from 0.02233\n",
      "43/43 [==============================] - 0s 4ms/step - loss: 0.0035 - mse: 0.3151 - val_loss: 0.0807 - val_mse: 0.9437\n",
      "Epoch 109/200\n",
      "40/43 [==========================>...] - ETA: 0s - loss: 0.0019 - mse: 0.3102\n",
      "Epoch 00109: val_loss did not improve from 0.02233\n",
      "43/43 [==============================] - 0s 3ms/step - loss: 0.0019 - mse: 0.3073 - val_loss: 0.0836 - val_mse: 0.9928\n",
      "Epoch 110/200\n",
      "35/43 [=======================>......] - ETA: 0s - loss: 0.0014 - mse: 0.3037   \n",
      "Epoch 00110: val_loss did not improve from 0.02233\n",
      "43/43 [==============================] - 0s 4ms/step - loss: 0.0013 - mse: 0.3045 - val_loss: 0.0848 - val_mse: 1.0107\n",
      "Epoch 111/200\n",
      "35/43 [=======================>......] - ETA: 0s - loss: 0.0012 - mse: 0.2977   \n",
      "Epoch 00111: val_loss did not improve from 0.02233\n",
      "43/43 [==============================] - 0s 4ms/step - loss: 0.0011 - mse: 0.3032 - val_loss: 0.0825 - val_mse: 0.9996\n",
      "Epoch 112/200\n",
      "35/43 [=======================>......] - ETA: 0s - loss: 9.0813e-04 - mse: 0.2984\n",
      "Epoch 00112: val_loss did not improve from 0.02233\n",
      "43/43 [==============================] - 0s 4ms/step - loss: 8.7492e-04 - mse: 0.3019 - val_loss: 0.0831 - val_mse: 1.0142\n",
      "Epoch 113/200\n",
      "37/43 [========================>.....] - ETA: 0s - loss: 5.2971e-04 - mse: 0.2997\n",
      "Epoch 00113: val_loss did not improve from 0.02233\n",
      "43/43 [==============================] - 0s 4ms/step - loss: 8.1545e-04 - mse: 0.3016 - val_loss: 0.0845 - val_mse: 1.0386\n",
      "Epoch 114/200\n",
      "33/43 [======================>.......] - ETA: 0s - loss: 7.7726e-04 - mse: 0.3035\n",
      "Epoch 00114: val_loss did not improve from 0.02233\n",
      "43/43 [==============================] - 0s 4ms/step - loss: 7.2249e-04 - mse: 0.3010 - val_loss: 0.0841 - val_mse: 1.0362\n",
      "Epoch 115/200\n",
      "36/43 [========================>.....] - ETA: 0s - loss: 9.2451e-04 - mse: 0.2985\n",
      "Epoch 00115: val_loss did not improve from 0.02233\n",
      "43/43 [==============================] - 0s 4ms/step - loss: 8.7715e-04 - mse: 0.3013 - val_loss: 0.0856 - val_mse: 1.0599\n",
      "Epoch 116/200\n",
      "40/43 [==========================>...] - ETA: 0s - loss: 6.8621e-04 - mse: 0.3018\n",
      "Epoch 00116: val_loss did not improve from 0.02233\n",
      "43/43 [==============================] - 0s 3ms/step - loss: 7.1098e-04 - mse: 0.3007 - val_loss: 0.0861 - val_mse: 1.0682\n",
      "Epoch 117/200\n",
      "43/43 [==============================] - ETA: 0s - loss: 6.7293e-04 - mse: 0.3004\n",
      "Epoch 00117: val_loss did not improve from 0.02233\n",
      "43/43 [==============================] - 0s 5ms/step - loss: 6.7293e-04 - mse: 0.3004 - val_loss: 0.0866 - val_mse: 1.0784\n",
      "Epoch 118/200\n",
      "35/43 [=======================>......] - ETA: 0s - loss: 3.5513e-04 - mse: 0.2998\n",
      "Epoch 00118: val_loss did not improve from 0.02233\n",
      "43/43 [==============================] - 0s 4ms/step - loss: 6.3906e-04 - mse: 0.3000 - val_loss: 0.0869 - val_mse: 1.0877\n",
      "Epoch 119/200\n",
      "43/43 [==============================] - ETA: 0s - loss: 6.1548e-04 - mse: 0.2999\n",
      "Epoch 00119: val_loss did not improve from 0.02233\n",
      "43/43 [==============================] - 0s 3ms/step - loss: 6.1548e-04 - mse: 0.2999 - val_loss: 0.0874 - val_mse: 1.0963\n",
      "Epoch 120/200\n",
      "42/43 [============================>.] - ETA: 0s - loss: 6.0188e-04 - mse: 0.2966\n",
      "Epoch 00120: val_loss did not improve from 0.02233\n",
      "43/43 [==============================] - 0s 5ms/step - loss: 5.9789e-04 - mse: 0.2997 - val_loss: 0.0878 - val_mse: 1.1046\n",
      "Epoch 121/200\n",
      "30/43 [===================>..........] - ETA: 0s - loss: 6.1388e-04 - mse: 0.3040\n",
      "Epoch 00121: val_loss did not improve from 0.02233\n",
      "43/43 [==============================] - 0s 4ms/step - loss: 5.9515e-04 - mse: 0.2996 - val_loss: 0.0878 - val_mse: 1.1080\n",
      "Epoch 122/200\n",
      "33/43 [======================>.......] - ETA: 0s - loss: 6.8325e-04 - mse: 0.2973\n",
      "Epoch 00122: val_loss did not improve from 0.02233\n",
      "43/43 [==============================] - 0s 4ms/step - loss: 5.9882e-04 - mse: 0.2995 - val_loss: 0.0883 - val_mse: 1.1150\n",
      "Epoch 123/200\n",
      "32/43 [=====================>........] - ETA: 0s - loss: 6.8450e-04 - mse: 0.2975\n",
      "Epoch 00123: val_loss did not improve from 0.02233\n",
      "43/43 [==============================] - 0s 5ms/step - loss: 5.8032e-04 - mse: 0.2994 - val_loss: 0.0888 - val_mse: 1.1246\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 124/200\n",
      "33/43 [======================>.......] - ETA: 0s - loss: 2.8827e-04 - mse: 0.3063\n",
      "Epoch 00124: val_loss did not improve from 0.02233\n",
      "43/43 [==============================] - 0s 4ms/step - loss: 5.8163e-04 - mse: 0.2993 - val_loss: 0.0882 - val_mse: 1.1203\n",
      "Epoch 125/200\n",
      "33/43 [======================>.......] - ETA: 0s - loss: 2.4187e-04 - mse: 0.2944\n",
      "Epoch 00125: val_loss did not improve from 0.02233\n",
      "43/43 [==============================] - 0s 4ms/step - loss: 5.7880e-04 - mse: 0.2993 - val_loss: 0.0897 - val_mse: 1.1393\n",
      "Epoch 126/200\n",
      "38/43 [=========================>....] - ETA: 0s - loss: 2.6713e-04 - mse: 0.3066\n",
      "Epoch 00126: val_loss did not improve from 0.02233\n",
      "43/43 [==============================] - 0s 5ms/step - loss: 5.7954e-04 - mse: 0.2992 - val_loss: 0.0893 - val_mse: 1.1377\n",
      "Epoch 127/200\n",
      "33/43 [======================>.......] - ETA: 0s - loss: 3.4214e-04 - mse: 0.2961\n",
      "Epoch 00127: val_loss did not improve from 0.02233\n",
      "43/43 [==============================] - 0s 4ms/step - loss: 5.7923e-04 - mse: 0.2992 - val_loss: 0.0899 - val_mse: 1.1463\n",
      "Epoch 128/200\n",
      "34/43 [======================>.......] - ETA: 0s - loss: 3.3870e-04 - mse: 0.2950\n",
      "Epoch 00128: val_loss did not improve from 0.02233\n",
      "43/43 [==============================] - 0s 4ms/step - loss: 5.7632e-04 - mse: 0.2991 - val_loss: 0.0898 - val_mse: 1.1483\n",
      "Epoch 129/200\n",
      "29/43 [===================>..........] - ETA: 0s - loss: 7.2483e-04 - mse: 0.2950\n",
      "Epoch 00129: val_loss did not improve from 0.02233\n",
      "43/43 [==============================] - 0s 4ms/step - loss: 6.0382e-04 - mse: 0.2991 - val_loss: 0.0897 - val_mse: 1.1489\n",
      "Epoch 130/200\n",
      "40/43 [==========================>...] - ETA: 0s - loss: 5.8865e-04 - mse: 0.2965\n",
      "Epoch 00130: val_loss did not improve from 0.02233\n",
      "43/43 [==============================] - 0s 5ms/step - loss: 5.9351e-04 - mse: 0.2990 - val_loss: 0.0904 - val_mse: 1.1600\n",
      "Epoch 131/200\n",
      "38/43 [=========================>....] - ETA: 0s - loss: 6.4808e-04 - mse: 0.3007\n",
      "Epoch 00131: val_loss did not improve from 0.02233\n",
      "43/43 [==============================] - 0s 4ms/step - loss: 5.8941e-04 - mse: 0.2989 - val_loss: 0.0904 - val_mse: 1.1604\n",
      "Epoch 132/200\n",
      "43/43 [==============================] - ETA: 0s - loss: 5.7133e-04 - mse: 0.2988\n",
      "Epoch 00132: val_loss did not improve from 0.02233\n",
      "43/43 [==============================] - 0s 5ms/step - loss: 5.7133e-04 - mse: 0.2988 - val_loss: 0.0901 - val_mse: 1.1621\n",
      "Epoch 133/200\n",
      "30/43 [===================>..........] - ETA: 0s - loss: 3.1849e-04 - mse: 0.2966\n",
      "Epoch 00133: val_loss did not improve from 0.02233\n",
      "43/43 [==============================] - 0s 4ms/step - loss: 5.5594e-04 - mse: 0.2987 - val_loss: 0.0905 - val_mse: 1.1678\n",
      "Epoch 134/200\n",
      "39/43 [==========================>...] - ETA: 0s - loss: 5.6435e-04 - mse: 0.2949\n",
      "Epoch 00134: val_loss did not improve from 0.02233\n",
      "43/43 [==============================] - 0s 4ms/step - loss: 5.5549e-04 - mse: 0.2987 - val_loss: 0.0906 - val_mse: 1.1708\n",
      "Epoch 135/200\n",
      "30/43 [===================>..........] - ETA: 0s - loss: 2.2605e-04 - mse: 0.2833\n",
      "Epoch 00135: val_loss did not improve from 0.02233\n",
      "43/43 [==============================] - 0s 5ms/step - loss: 5.5007e-04 - mse: 0.2987 - val_loss: 0.0906 - val_mse: 1.1750\n",
      "Epoch 136/200\n",
      "43/43 [==============================] - ETA: 0s - loss: 5.4623e-04 - mse: 0.2985\n",
      "Epoch 00136: val_loss did not improve from 0.02233\n",
      "43/43 [==============================] - 0s 5ms/step - loss: 5.4623e-04 - mse: 0.2985 - val_loss: 0.0914 - val_mse: 1.1836\n",
      "Epoch 137/200\n",
      "38/43 [=========================>....] - ETA: 0s - loss: 5.6012e-04 - mse: 0.2934\n",
      "Epoch 00137: val_loss did not improve from 0.02233\n",
      "43/43 [==============================] - 0s 3ms/step - loss: 5.5361e-04 - mse: 0.2985 - val_loss: 0.0910 - val_mse: 1.1812\n",
      "Epoch 138/200\n",
      "39/43 [==========================>...] - ETA: 0s - loss: 6.0043e-04 - mse: 0.3013\n",
      "Epoch 00138: val_loss did not improve from 0.02233\n",
      "43/43 [==============================] - 0s 4ms/step - loss: 5.6190e-04 - mse: 0.2984 - val_loss: 0.0922 - val_mse: 1.1982\n",
      "Epoch 139/200\n",
      "33/43 [======================>.......] - ETA: 0s - loss: 6.3339e-04 - mse: 0.2941\n",
      "Epoch 00139: val_loss did not improve from 0.02233\n",
      "43/43 [==============================] - 0s 4ms/step - loss: 5.5201e-04 - mse: 0.2983 - val_loss: 0.0905 - val_mse: 1.1748\n",
      "Epoch 140/200\n",
      "34/43 [======================>.......] - ETA: 0s - loss: 6.4022e-04 - mse: 0.2866\n",
      "Epoch 00140: val_loss did not improve from 0.02233\n",
      "43/43 [==============================] - 0s 4ms/step - loss: 5.7396e-04 - mse: 0.2985 - val_loss: 0.0922 - val_mse: 1.2019\n",
      "Epoch 141/200\n",
      "40/43 [==========================>...] - ETA: 0s - loss: 5.5354e-04 - mse: 0.3058\n",
      "Epoch 00141: val_loss did not improve from 0.02233\n",
      "43/43 [==============================] - 0s 3ms/step - loss: 5.6276e-04 - mse: 0.2983 - val_loss: 0.0916 - val_mse: 1.1905\n",
      "Epoch 142/200\n",
      "33/43 [======================>.......] - ETA: 0s - loss: 6.8000e-04 - mse: 0.2974\n",
      "Epoch 00142: val_loss did not improve from 0.02233\n",
      "43/43 [==============================] - 0s 4ms/step - loss: 5.5798e-04 - mse: 0.2982 - val_loss: 0.0932 - val_mse: 1.2134\n",
      "Epoch 143/200\n",
      "35/43 [=======================>......] - ETA: 0s - loss: 5.8518e-04 - mse: 0.3054\n",
      "Epoch 00143: val_loss did not improve from 0.02233\n",
      "43/43 [==============================] - 0s 4ms/step - loss: 5.8017e-04 - mse: 0.2982 - val_loss: 0.0916 - val_mse: 1.1915\n",
      "Epoch 144/200\n",
      "36/43 [========================>.....] - ETA: 0s - loss: 7.5226e-04 - mse: 0.3032\n",
      "Epoch 00144: val_loss did not improve from 0.02233\n",
      "43/43 [==============================] - 0s 4ms/step - loss: 6.8474e-04 - mse: 0.2986 - val_loss: 0.0928 - val_mse: 1.2095\n",
      "Epoch 145/200\n",
      "43/43 [==============================] - ETA: 0s - loss: 7.4839e-04 - mse: 0.2990\n",
      "Epoch 00145: val_loss did not improve from 0.02233\n",
      "43/43 [==============================] - 0s 5ms/step - loss: 7.4839e-04 - mse: 0.2990 - val_loss: 0.0930 - val_mse: 1.2200\n",
      "Epoch 146/200\n",
      "30/43 [===================>..........] - ETA: 0s - loss: 8.0567e-04 - mse: 0.2882\n",
      "Epoch 00146: val_loss did not improve from 0.02233\n",
      "43/43 [==============================] - 0s 4ms/step - loss: 7.4221e-04 - mse: 0.2990 - val_loss: 0.0926 - val_mse: 1.2086\n",
      "Epoch 147/200\n",
      "31/43 [====================>.........] - ETA: 0s - loss: 5.3444e-04 - mse: 0.2936\n",
      "Epoch 00147: val_loss did not improve from 0.02233\n",
      "43/43 [==============================] - 0s 5ms/step - loss: 7.6654e-04 - mse: 0.2988 - val_loss: 0.0923 - val_mse: 1.2011\n",
      "Epoch 148/200\n",
      "43/43 [==============================] - ETA: 0s - loss: 7.3277e-04 - mse: 0.2985\n",
      "Epoch 00148: val_loss did not improve from 0.02233\n",
      "43/43 [==============================] - 0s 5ms/step - loss: 7.3277e-04 - mse: 0.2985 - val_loss: 0.0930 - val_mse: 1.2165\n",
      "Epoch 149/200\n",
      "31/43 [====================>.........] - ETA: 0s - loss: 8.3194e-04 - mse: 0.3070\n",
      "Epoch 00149: val_loss did not improve from 0.02233\n",
      "43/43 [==============================] - 0s 4ms/step - loss: 6.8609e-04 - mse: 0.2984 - val_loss: 0.0923 - val_mse: 1.2050\n",
      "Epoch 150/200\n",
      "34/43 [======================>.......] - ETA: 0s - loss: 7.6032e-04 - mse: 0.3077\n",
      "Epoch 00150: val_loss did not improve from 0.02233\n",
      "43/43 [==============================] - 0s 5ms/step - loss: 6.7448e-04 - mse: 0.2986 - val_loss: 0.0933 - val_mse: 1.2321\n",
      "Epoch 151/200\n",
      "41/43 [===========================>..] - ETA: 0s - loss: 6.8220e-04 - mse: 0.2949\n",
      "Epoch 00151: val_loss did not improve from 0.02233\n",
      "43/43 [==============================] - 0s 5ms/step - loss: 6.6189e-04 - mse: 0.2985 - val_loss: 0.0930 - val_mse: 1.2253\n",
      "Epoch 152/200\n",
      "34/43 [======================>.......] - ETA: 0s - loss: 7.3600e-04 - mse: 0.2926\n",
      "Epoch 00152: val_loss did not improve from 0.02233\n",
      "43/43 [==============================] - 0s 4ms/step - loss: 6.2275e-04 - mse: 0.2982 - val_loss: 0.0929 - val_mse: 1.2190\n",
      "Epoch 153/200\n",
      "38/43 [=========================>....] - ETA: 0s - loss: 6.4946e-04 - mse: 0.2973\n",
      "Epoch 00153: val_loss did not improve from 0.02233\n",
      "43/43 [==============================] - 0s 3ms/step - loss: 6.1152e-04 - mse: 0.2986 - val_loss: 0.0933 - val_mse: 1.2294\n",
      "Epoch 154/200\n",
      "37/43 [========================>.....] - ETA: 0s - loss: 5.9484e-04 - mse: 0.2948\n",
      "Epoch 00154: val_loss did not improve from 0.02233\n",
      "43/43 [==============================] - 0s 4ms/step - loss: 6.0336e-04 - mse: 0.2984 - val_loss: 0.0932 - val_mse: 1.2208\n",
      "Epoch 155/200\n",
      "31/43 [====================>.........] - ETA: 0s - loss: 6.9513e-04 - mse: 0.2958\n",
      "Epoch 00155: val_loss did not improve from 0.02233\n",
      "43/43 [==============================] - 0s 4ms/step - loss: 6.0717e-04 - mse: 0.2984 - val_loss: 0.0937 - val_mse: 1.2444\n",
      "Epoch 156/200\n",
      "43/43 [==============================] - ETA: 0s - loss: 5.8243e-04 - mse: 0.2979\n",
      "Epoch 00156: val_loss did not improve from 0.02233\n",
      "43/43 [==============================] - 0s 3ms/step - loss: 5.8243e-04 - mse: 0.2979 - val_loss: 0.0936 - val_mse: 1.2312\n",
      "Epoch 157/200\n",
      "31/43 [====================>.........] - ETA: 0s - loss: 6.0527e-04 - mse: 0.2831\n",
      "Epoch 00157: val_loss did not improve from 0.02233\n",
      "43/43 [==============================] - 0s 4ms/step - loss: 5.6208e-04 - mse: 0.2977 - val_loss: 0.0944 - val_mse: 1.2514\n",
      "Epoch 158/200\n",
      "33/43 [======================>.......] - ETA: 0s - loss: 5.7325e-04 - mse: 0.2997\n",
      "Epoch 00158: val_loss did not improve from 0.02233\n",
      "43/43 [==============================] - 0s 4ms/step - loss: 5.5670e-04 - mse: 0.2977 - val_loss: 0.0937 - val_mse: 1.2404\n",
      "Epoch 159/200\n",
      "43/43 [==============================] - ETA: 0s - loss: 5.6881e-04 - mse: 0.2977\n",
      "Epoch 00159: val_loss did not improve from 0.02233\n",
      "43/43 [==============================] - 0s 3ms/step - loss: 5.6881e-04 - mse: 0.2977 - val_loss: 0.0930 - val_mse: 1.2312\n",
      "Epoch 160/200\n",
      "38/43 [=========================>....] - ETA: 0s - loss: 6.1047e-04 - mse: 0.2915\n",
      "Epoch 00160: val_loss did not improve from 0.02233\n",
      "43/43 [==============================] - 0s 4ms/step - loss: 5.6419e-04 - mse: 0.2977 - val_loss: 0.0952 - val_mse: 1.2658\n",
      "Epoch 161/200\n",
      "32/43 [=====================>........] - ETA: 0s - loss: 2.4803e-04 - mse: 0.2976\n",
      "Epoch 00161: val_loss did not improve from 0.02233\n",
      "43/43 [==============================] - 0s 4ms/step - loss: 5.4441e-04 - mse: 0.2976 - val_loss: 0.0950 - val_mse: 1.2658\n",
      "Epoch 162/200\n",
      "34/43 [======================>.......] - ETA: 0s - loss: 6.5280e-04 - mse: 0.3087\n",
      "Epoch 00162: val_loss did not improve from 0.02233\n",
      "43/43 [==============================] - 0s 4ms/step - loss: 5.4172e-04 - mse: 0.2975 - val_loss: 0.0945 - val_mse: 1.2597\n",
      "Epoch 163/200\n",
      "37/43 [========================>.....] - ETA: 0s - loss: 6.1945e-04 - mse: 0.2987\n",
      "Epoch 00163: val_loss did not improve from 0.02233\n",
      "43/43 [==============================] - 0s 3ms/step - loss: 5.6314e-04 - mse: 0.2976 - val_loss: 0.0957 - val_mse: 1.2787\n",
      "Epoch 164/200\n",
      "31/43 [====================>.........] - ETA: 0s - loss: 2.9261e-04 - mse: 0.2943\n",
      "Epoch 00164: val_loss did not improve from 0.02233\n",
      "43/43 [==============================] - 0s 4ms/step - loss: 5.8183e-04 - mse: 0.2976 - val_loss: 0.0954 - val_mse: 1.2745\n",
      "Epoch 165/200\n",
      "35/43 [=======================>......] - ETA: 0s - loss: 2.5118e-04 - mse: 0.2974\n",
      "Epoch 00165: val_loss did not improve from 0.02233\n",
      "43/43 [==============================] - 0s 4ms/step - loss: 6.1613e-04 - mse: 0.2976 - val_loss: 0.0960 - val_mse: 1.2878\n",
      "Epoch 166/200\n",
      "40/43 [==========================>...] - ETA: 0s - loss: 6.0903e-04 - mse: 0.2970\n",
      "Epoch 00166: val_loss did not improve from 0.02233\n",
      "43/43 [==============================] - 0s 3ms/step - loss: 5.8049e-04 - mse: 0.2974 - val_loss: 0.0953 - val_mse: 1.2789\n",
      "Epoch 167/200\n",
      "39/43 [==========================>...] - ETA: 0s - loss: 6.4763e-04 - mse: 0.2944\n",
      "Epoch 00167: val_loss did not improve from 0.02233\n",
      "43/43 [==============================] - 0s 4ms/step - loss: 6.0703e-04 - mse: 0.2975 - val_loss: 0.0974 - val_mse: 1.3051\n",
      "Epoch 168/200\n",
      "33/43 [======================>.......] - ETA: 0s - loss: 8.2410e-04 - mse: 0.2940\n",
      "Epoch 00168: val_loss did not improve from 0.02233\n",
      "43/43 [==============================] - 0s 4ms/step - loss: 6.8314e-04 - mse: 0.2978 - val_loss: 0.0962 - val_mse: 1.2870\n",
      "Epoch 169/200\n",
      "32/43 [=====================>........] - ETA: 0s - loss: 7.8991e-04 - mse: 0.2992\n",
      "Epoch 00169: val_loss did not improve from 0.02233\n",
      "43/43 [==============================] - 0s 4ms/step - loss: 6.8731e-04 - mse: 0.2980 - val_loss: 0.0971 - val_mse: 1.2991\n",
      "Epoch 170/200\n",
      "39/43 [==========================>...] - ETA: 0s - loss: 3.9426e-04 - mse: 0.2983\n",
      "Epoch 00170: val_loss did not improve from 0.02233\n",
      "43/43 [==============================] - 0s 4ms/step - loss: 7.0556e-04 - mse: 0.2981 - val_loss: 0.0957 - val_mse: 1.2829\n",
      "Epoch 171/200\n",
      "31/43 [====================>.........] - ETA: 0s - loss: 8.4228e-04 - mse: 0.2993\n",
      "Epoch 00171: val_loss did not improve from 0.02233\n",
      "43/43 [==============================] - 0s 4ms/step - loss: 7.0204e-04 - mse: 0.2980 - val_loss: 0.0985 - val_mse: 1.3258\n",
      "Epoch 172/200\n",
      "32/43 [=====================>........] - ETA: 0s - loss: 8.3675e-04 - mse: 0.2932\n",
      "Epoch 00172: val_loss did not improve from 0.02233\n",
      "43/43 [==============================] - 0s 4ms/step - loss: 7.0870e-04 - mse: 0.2982 - val_loss: 0.0959 - val_mse: 1.2797\n",
      "Epoch 173/200\n",
      "38/43 [=========================>....] - ETA: 0s - loss: 7.5538e-04 - mse: 0.2973\n",
      "Epoch 00173: val_loss did not improve from 0.02233\n",
      "43/43 [==============================] - 0s 3ms/step - loss: 7.0629e-04 - mse: 0.2981 - val_loss: 0.0970 - val_mse: 1.2961\n",
      "Epoch 174/200\n",
      "31/43 [====================>.........] - ETA: 0s - loss: 4.4425e-04 - mse: 0.3007\n",
      "Epoch 00174: val_loss did not improve from 0.02233\n",
      "43/43 [==============================] - 0s 4ms/step - loss: 7.2669e-04 - mse: 0.2986 - val_loss: 0.0976 - val_mse: 1.2996\n",
      "Epoch 175/200\n",
      "33/43 [======================>.......] - ETA: 0s - loss: 9.0604e-04 - mse: 0.2928\n",
      "Epoch 00175: val_loss did not improve from 0.02233\n",
      "43/43 [==============================] - 0s 4ms/step - loss: 7.6745e-04 - mse: 0.2988 - val_loss: 0.0989 - val_mse: 1.3341\n",
      "Epoch 176/200\n",
      "41/43 [===========================>..] - ETA: 0s - loss: 7.1502e-04 - mse: 0.2986\n",
      "Epoch 00176: val_loss did not improve from 0.02233\n",
      "43/43 [==============================] - 0s 3ms/step - loss: 7.0907e-04 - mse: 0.2983 - val_loss: 0.0972 - val_mse: 1.2960\n",
      "Epoch 177/200\n",
      "39/43 [==========================>...] - ETA: 0s - loss: 6.9632e-04 - mse: 0.2985\n",
      "Epoch 00177: val_loss did not improve from 0.02233\n",
      "43/43 [==============================] - 0s 4ms/step - loss: 6.6762e-04 - mse: 0.2981 - val_loss: 0.0978 - val_mse: 1.3098\n",
      "Epoch 178/200\n",
      "32/43 [=====================>........] - ETA: 0s - loss: 7.2907e-04 - mse: 0.3043\n",
      "Epoch 00178: val_loss did not improve from 0.02233\n",
      "43/43 [==============================] - 0s 4ms/step - loss: 6.3571e-04 - mse: 0.2977 - val_loss: 0.0967 - val_mse: 1.2893\n",
      "Epoch 179/200\n",
      "33/43 [======================>.......] - ETA: 0s - loss: 6.8719e-04 - mse: 0.2923\n",
      "Epoch 00179: val_loss did not improve from 0.02233\n",
      "43/43 [==============================] - 0s 4ms/step - loss: 6.4771e-04 - mse: 0.2978 - val_loss: 0.0980 - val_mse: 1.3102\n",
      "Epoch 180/200\n",
      "35/43 [=======================>......] - ETA: 0s - loss: 3.3344e-04 - mse: 0.3011\n",
      "Epoch 00180: val_loss did not improve from 0.02233\n",
      "43/43 [==============================] - 0s 4ms/step - loss: 5.8876e-04 - mse: 0.2975 - val_loss: 0.0975 - val_mse: 1.3024\n",
      "Epoch 181/200\n",
      "31/43 [====================>.........] - ETA: 0s - loss: 6.7596e-04 - mse: 0.2934\n",
      "Epoch 00181: val_loss did not improve from 0.02233\n",
      "43/43 [==============================] - 0s 4ms/step - loss: 5.8524e-04 - mse: 0.2975 - val_loss: 0.0981 - val_mse: 1.3177\n",
      "Epoch 182/200\n",
      "40/43 [==========================>...] - ETA: 0s - loss: 6.1516e-04 - mse: 0.2938\n",
      "Epoch 00182: val_loss did not improve from 0.02233\n",
      "43/43 [==============================] - 0s 4ms/step - loss: 5.9235e-04 - mse: 0.2974 - val_loss: 0.0975 - val_mse: 1.3116\n",
      "Epoch 183/200\n",
      "38/43 [=========================>....] - ETA: 0s - loss: 6.1107e-04 - mse: 0.2985\n",
      "Epoch 00183: val_loss did not improve from 0.02233\n",
      "43/43 [==============================] - 0s 4ms/step - loss: 5.6160e-04 - mse: 0.2973 - val_loss: 0.0984 - val_mse: 1.3251\n",
      "Epoch 184/200\n",
      "29/43 [===================>..........] - ETA: 0s - loss: 7.1103e-04 - mse: 0.3000\n",
      "Epoch 00184: val_loss did not improve from 0.02233\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "43/43 [==============================] - 0s 4ms/step - loss: 5.7414e-04 - mse: 0.2972 - val_loss: 0.0982 - val_mse: 1.3217\n",
      "Epoch 185/200\n",
      "33/43 [======================>.......] - ETA: 0s - loss: 7.0597e-04 - mse: 0.3071\n",
      "Epoch 00185: val_loss did not improve from 0.02233\n",
      "43/43 [==============================] - 0s 4ms/step - loss: 6.0976e-04 - mse: 0.2974 - val_loss: 0.0975 - val_mse: 1.3089\n",
      "Epoch 186/200\n",
      "33/43 [======================>.......] - ETA: 0s - loss: 6.0194e-04 - mse: 0.2942\n",
      "Epoch 00186: val_loss did not improve from 0.02233\n",
      "43/43 [==============================] - 0s 4ms/step - loss: 5.8291e-04 - mse: 0.2974 - val_loss: 0.0996 - val_mse: 1.3533\n",
      "Epoch 187/200\n",
      "34/43 [======================>.......] - ETA: 0s - loss: 6.1183e-04 - mse: 0.2939\n",
      "Epoch 00187: val_loss did not improve from 0.02233\n",
      "43/43 [==============================] - 0s 4ms/step - loss: 5.6845e-04 - mse: 0.2972 - val_loss: 0.0986 - val_mse: 1.3364\n",
      "Epoch 188/200\n",
      "37/43 [========================>.....] - ETA: 0s - loss: 6.6881e-04 - mse: 0.3056\n",
      "Epoch 00188: val_loss did not improve from 0.02233\n",
      "43/43 [==============================] - 0s 4ms/step - loss: 6.5298e-04 - mse: 0.2977 - val_loss: 0.0991 - val_mse: 1.3292\n",
      "Epoch 189/200\n",
      "32/43 [=====================>........] - ETA: 0s - loss: 4.0214e-04 - mse: 0.2904\n",
      "Epoch 00189: val_loss did not improve from 0.02233\n",
      "43/43 [==============================] - 0s 4ms/step - loss: 6.5744e-04 - mse: 0.2975 - val_loss: 0.0997 - val_mse: 1.3488\n",
      "Epoch 190/200\n",
      "32/43 [=====================>........] - ETA: 0s - loss: 3.7183e-04 - mse: 0.3020\n",
      "Epoch 00190: val_loss did not improve from 0.02233\n",
      "43/43 [==============================] - 0s 4ms/step - loss: 6.6543e-04 - mse: 0.2975 - val_loss: 0.0997 - val_mse: 1.3380\n",
      "Epoch 191/200\n",
      "37/43 [========================>.....] - ETA: 0s - loss: 6.9235e-04 - mse: 0.2922\n",
      "Epoch 00191: val_loss did not improve from 0.02233\n",
      "43/43 [==============================] - 0s 4ms/step - loss: 6.3368e-04 - mse: 0.2974 - val_loss: 0.0993 - val_mse: 1.3396\n",
      "Epoch 192/200\n",
      "38/43 [=========================>....] - ETA: 0s - loss: 6.5743e-04 - mse: 0.2980\n",
      "Epoch 00192: val_loss did not improve from 0.02233\n",
      "43/43 [==============================] - 0s 3ms/step - loss: 6.3076e-04 - mse: 0.2973 - val_loss: 0.1007 - val_mse: 1.3600\n",
      "Epoch 193/200\n",
      "30/43 [===================>..........] - ETA: 0s - loss: 9.8304e-04 - mse: 0.3074\n",
      "Epoch 00193: val_loss did not improve from 0.02233\n",
      "43/43 [==============================] - 0s 4ms/step - loss: 0.0012 - mse: 0.2994 - val_loss: 0.0887 - val_mse: 1.1636\n",
      "Epoch 194/200\n",
      "37/43 [========================>.....] - ETA: 0s - loss: 0.0037 - mse: 0.3221\n",
      "Epoch 00194: val_loss did not improve from 0.02233\n",
      "43/43 [==============================] - 0s 4ms/step - loss: 0.0040 - mse: 0.3156 - val_loss: 0.1012 - val_mse: 1.2538\n",
      "Epoch 195/200\n",
      "37/43 [========================>.....] - ETA: 0s - loss: 0.0048 - mse: 0.3278\n",
      "Epoch 00195: val_loss did not improve from 0.02233\n",
      "43/43 [==============================] - 0s 4ms/step - loss: 0.0045 - mse: 0.3144 - val_loss: 0.0966 - val_mse: 1.1962\n",
      "Epoch 196/200\n",
      "43/43 [==============================] - ETA: 0s - loss: 0.0026 - mse: 0.3084  \n",
      "Epoch 00196: val_loss did not improve from 0.02233\n",
      "43/43 [==============================] - 0s 5ms/step - loss: 0.0026 - mse: 0.3084 - val_loss: 0.0816 - val_mse: 1.0455\n",
      "Epoch 197/200\n",
      "32/43 [=====================>........] - ETA: 0s - loss: 0.0024 - mse: 0.3199\n",
      "Epoch 00197: val_loss did not improve from 0.02233\n",
      "43/43 [==============================] - 0s 4ms/step - loss: 0.0028 - mse: 0.3112 - val_loss: 0.0722 - val_mse: 0.8664\n",
      "Epoch 198/200\n",
      "38/43 [=========================>....] - ETA: 0s - loss: 0.0020 - mse: 0.3139\n",
      "Epoch 00198: val_loss did not improve from 0.02233\n",
      "43/43 [==============================] - 0s 3ms/step - loss: 0.0019 - mse: 0.3070 - val_loss: 0.0689 - val_mse: 0.8219\n",
      "Epoch 199/200\n",
      "33/43 [======================>.......] - ETA: 0s - loss: 0.0013 - mse: 0.3066    \n",
      "Epoch 00199: val_loss did not improve from 0.02233\n",
      "43/43 [==============================] - 0s 4ms/step - loss: 0.0025 - mse: 0.3153 - val_loss: 0.0900 - val_mse: 1.0578\n",
      "Epoch 200/200\n",
      "33/43 [======================>.......] - ETA: 0s - loss: 0.0022 - mse: 0.3097\n",
      "Epoch 00200: val_loss did not improve from 0.02233\n",
      "43/43 [==============================] - 0s 4ms/step - loss: 0.0021 - mse: 0.3073 - val_loss: 0.0758 - val_mse: 0.9126\n"
     ]
    }
   ],
   "source": [
    "#Compiling the ANN\n",
    "opt = keras.optimizers.Adam(lr=0.0015, beta_1=0.9, beta_2=0.999, epsilon=None, decay=0.0, amsgrad=False)\n",
    "model.compile(optimizer=opt, loss='mean_squared_logarithmic_error', metrics=['mse'])\n",
    "#Fitting the ANN to the training set\n",
    "model_filepath = 'min_vl_model1.h5'\n",
    "checkpoint = ModelCheckpoint(model_filepath, monitor = 'val_loss', verbose=1, save_best_only = True, mode='min' )\n",
    "history=model.fit(X_train,y_train, validation_split=0.07, batch_size=32, epochs=200, callbacks=[checkpoint])\n",
    "model.load_weights(model_filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "reliable-vampire",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "assisted-spoke",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = sc_y.inverse_transform(y_pred)\n",
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "intermediate-mouse",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = pd.DataFrame(y_pred)\n",
    "y_pred[\"Id\"] = ids\n",
    "y_pred = y_pred.rename(columns={0: \"SalePrice\"})\n",
    "y_pred = y_pred[[\"Id\",\"SalePrice\"]]\n",
    "y_pred.to_csv(\"Submission1.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
